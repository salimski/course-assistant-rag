{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673e70de",
   "metadata": {},
   "source": [
    "# Agentic RAG System - Information Retrieval Course\n",
    "## Assignment: Track 3 - Agentic RAG\n",
    "\n",
    "**Students:**\n",
    "- 314777475 Salim B\n",
    "- 213017049 Habib N\n",
    "\n",
    "**System Overview:**\n",
    "This project implements an agentic RAG system that combines:\n",
    "- Course materials retrieval (RAG with ChromaDB + Ollama)\n",
    "- Weather API (Open-Meteo)\n",
    "- Holiday API (Hebcal)\n",
    "- Calendar management (JSON-based)\n",
    "\n",
    "**Architecture:** LangGraph-based workflow with conditional tool routing and auto-chaining capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf16a81",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1752251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agentic RAG System...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
    "os.environ[\"CHROMA_TELEMETRY\"] = \"False\"\n",
    "\n",
    "print(\"Initializing Agentic RAG System...\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09ed089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import all dependencies\n",
    "import json\n",
    "import re\n",
    "from datetime import date\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from tools.rag_tool import create_rag_tool\n",
    "from tools.weather_tool import create_weather_tool\n",
    "from tools.calendar_tool import create_calendar_tool\n",
    "from tools.holiday_tool import create_holiday_tool\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b2f24",
   "metadata": {},
   "source": [
    "## 2. Initialize Tools\n",
    "\n",
    "We initialize four tools:\n",
    "1. **RAG Tool** - ChromaDB vector store with course materials\n",
    "2. **Weather Tool** - Open-Meteo API (external)\n",
    "3. **Calendar Tool** - JSON-backed event management\n",
    "4. **Holiday Tool** - Hebcal API (external)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964aab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tools...\n",
      "Initializing embeddings model...\n",
      "Loading existing vector store from chroma_db...\n",
      "Loaded 1547 documents from vector store.\n",
      "âœ“ All tools loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tools...\")\n",
    "rag_tool_instance = create_rag_tool()\n",
    "weather_tool_instance = create_weather_tool()\n",
    "calendar_tool_instance = create_calendar_tool()\n",
    "holiday_tool_instance = create_holiday_tool()\n",
    "print(\"âœ“ All tools loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f077a61",
   "metadata": {},
   "source": [
    "## 3. Define LangChain Tool Wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d26da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Registered 6 tools\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_course_materials(query: str) -> str:\n",
    "    \"\"\"Search Information Retrieval course materials (RAG).\"\"\"\n",
    "    return rag_tool_instance.search(query)\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, on_date: str = None) -> str:\n",
    "    \"\"\"Weather (external API). If on_date provided (YYYY-MM-DD), returns forecast if available.\"\"\"\n",
    "    return weather_tool_instance.get_weather(location, on_date=on_date)\n",
    "\n",
    "@tool\n",
    "def check_calendar(query: str = \"upcoming\") -> str:\n",
    "    \"\"\"\n",
    "    Calendar (internal tool). If query contains 'next exam', returns JSON for chaining.\n",
    "    \"\"\"\n",
    "    q = (query or \"\").lower().strip()\n",
    "    if \"next exam\" in q:\n",
    "        return calendar_tool_instance.get_next_exam_json()\n",
    "    \n",
    "    if \"deadline\" in q:\n",
    "        return calendar_tool_instance.get_next_deadline()\n",
    "    elif \"exam\" in q or \"test\" in q:\n",
    "        return calendar_tool_instance.check_specific_event(\"exam\")\n",
    "    else:\n",
    "        return calendar_tool_instance.get_upcoming_events()\n",
    "\n",
    "@tool\n",
    "def get_next_holiday(country_code: str = \"IL\") -> str:\n",
    "    \"\"\"External API: next upcoming holiday after today's date.\"\"\"\n",
    "    return holiday_tool_instance.get_next_holiday(country_code=country_code, today_iso=date.today().isoformat())\n",
    "\n",
    "@tool\n",
    "def get_public_holidays(year: int, country_code: str = \"IL\") -> str:\n",
    "    \"\"\"External API: list holidays in a given year.\"\"\"\n",
    "    return holiday_tool_instance.get_holidays(year=year, country_code=country_code)\n",
    "\n",
    "@tool\n",
    "def is_public_holiday(date_str: str, country_code: str = \"IL\") -> str:\n",
    "    \"\"\"External API: check if a specific date is a holiday.\"\"\"\n",
    "    return holiday_tool_instance.is_holiday(date_str=date_str, country_code=country_code)\n",
    "\n",
    "tools = [\n",
    "    search_course_materials,\n",
    "    get_weather,\n",
    "    check_calendar,\n",
    "    get_next_holiday,\n",
    "    get_public_holidays,\n",
    "    is_public_holiday,\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Registered {len(tools)} tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9a7db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tool dictionary created with 6 tools\n",
      "Initializing Llama 3.1 model...\n",
      "âœ“ Model initialized with tool capabilities\n"
     ]
    }
   ],
   "source": [
    "# Create tool dictionary for lookup\n",
    "tools = [\n",
    "    search_course_materials,\n",
    "    get_weather,\n",
    "    check_calendar,\n",
    "    get_next_holiday,\n",
    "    get_public_holidays,\n",
    "    is_public_holiday,\n",
    "]\n",
    "\n",
    "tool_dict = {t.name: t for t in tools}\n",
    "\n",
    "print(f\"âœ“ Tool dictionary created with {len(tool_dict)} tools\")\n",
    "\n",
    "# Initialize LLM with tools\n",
    "print(\"Initializing Llama 3.1 model...\")\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0, base_url=\"http://localhost:11434\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(\"âœ“ Model initialized with tool capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6de93",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM with Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e788310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Llama 3.1 model...\n",
      "âœ“ Model initialized with tool capabilities\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Llama 3.1 model...\")\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0, base_url=\"http://localhost:11434\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(\"âœ“ Model initialized with tool capabilities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8717b",
   "metadata": {},
   "source": [
    "## 5. Build LangGraph Workflow\n",
    "\n",
    "The workflow includes:\n",
    "- **Agent Node**: Makes decisions and calls tools\n",
    "- **Tool Node**: Executes tool calls and auto-chaining\n",
    "- **Conditional Routing**: Decides whether to continue with tools or end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694059d4",
   "metadata": {},
   "source": [
    "## 5. Define Helper Functions and Agent Logic\n",
    "\n",
    "These helper functions support:\n",
    "- Message parsing and extraction\n",
    "- Auto-chaining logic detection\n",
    "- Response formatting\n",
    "- Tool output processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8e2098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "class AgentState(MessagesState):\n",
    "    pass\n",
    "\n",
    "def _last_user_text(messages) -> str:\n",
    "    \"\"\"Extract the last user message from conversation history.\"\"\"\n",
    "    for m in reversed(messages):\n",
    "        if isinstance(m, HumanMessage):\n",
    "            return m.content or \"\"\n",
    "    return \"\"\n",
    "\n",
    "def _user_requested_weather_on_exam_day(user_text: str) -> bool:\n",
    "    \"\"\"Detect if user is asking for weather on exam day (for auto-chaining).\"\"\"\n",
    "    t = user_text.lower()\n",
    "    return (\"exam\" in t) and (\"weather\" in t) and ((\"that day\" in t) or (\"specific day\" in t) or (\"exam day\" in t))\n",
    "\n",
    "def _extract_date_from_calendar_json(tool_text: str) -> Optional[str]:\n",
    "    \"\"\"Extract date from calendar tool JSON response.\"\"\"\n",
    "    try:\n",
    "        obj = json.loads(tool_text)\n",
    "        if obj.get(\"found\") and isinstance(obj.get(\"date\"), str):\n",
    "            if re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", obj[\"date\"]):\n",
    "                return obj[\"date\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _is_social_or_useless(text: str) -> bool:\n",
    "    \"\"\"Check if response is just social pleasantries without real content.\"\"\"\n",
    "    if not text:\n",
    "        return True\n",
    "    t = text.strip().lower()\n",
    "    social = {\n",
    "        \"you're welcome!\", \"you're welcome!\", \"welcome!\", \"no problem\", \"np\",\n",
    "        \"ok\", \"okay\", \"sure\", \"thanks\", \"thank you\", \"great\", \"cool\"\n",
    "    }\n",
    "    if t in social:\n",
    "        return True\n",
    "    if len(t) <= 4 and t in {\"ok\", \"k\", \"sure\", \"yes\", \"no\"}:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _unique_preserve_order(items):\n",
    "    \"\"\"Remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in items:\n",
    "        if x not in seen:\n",
    "            out.append(x)\n",
    "            seen.add(x)\n",
    "    return out\n",
    "\n",
    "def _format_exam_json(calendar_out: str) -> Optional[str]:\n",
    "    \"\"\"Format calendar JSON response nicely for display.\"\"\"\n",
    "    try:\n",
    "        obj = json.loads(calendar_out)\n",
    "        if isinstance(obj, dict) and obj.get(\"found\") is True and obj.get(\"type\") == \"exam\":\n",
    "            title = obj.get(\"title\", \"Exam\")\n",
    "            d = obj.get(\"date\", \"\")\n",
    "            tm = obj.get(\"time\", \"\")\n",
    "            return f\"Next exam: {title}\\nDate: {d} {tm}\".strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _extract_tool_outputs(messages) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract outputs from different tool types.\n",
    "    Returns (weather_out, calendar_out, holiday_out, rag_out)\n",
    "    \"\"\"\n",
    "    weather_out = None\n",
    "    calendar_out = None\n",
    "    holiday_out = None\n",
    "    rag_out = None\n",
    "\n",
    "    for m in messages:\n",
    "        if isinstance(m, ToolMessage):\n",
    "            name = getattr(m, \"name\", \"\") or \"\"\n",
    "            content = (m.content or \"\").strip()\n",
    "            if name == \"get_weather\":\n",
    "                weather_out = content\n",
    "            elif name == \"check_calendar\":\n",
    "                calendar_out = content\n",
    "            elif name == \"search_course_materials\":\n",
    "                rag_out = content\n",
    "            elif name in {\"get_next_holiday\", \"get_public_holidays\", \"is_public_holiday\"}:\n",
    "                holiday_out = content\n",
    "\n",
    "    return weather_out, calendar_out, holiday_out, rag_out\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2e872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Response generation functions defined\n"
     ]
    }
   ],
   "source": [
    "def _llm_course_explanation(user_input: str, rag_evidence: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a clean course explanation using LLM, grounded on retrieved evidence.\n",
    "    This is used especially in multi-tool queries so we don't dump raw chunks as the explanation.\n",
    "    \"\"\"\n",
    "    sys = SystemMessage(content=(\n",
    "        \"You are an expert Information Retrieval tutor.\\n\"\n",
    "        \"Task: Answer ONLY the course/theory part of the user's request.\\n\"\n",
    "        \"Use the provided evidence as grounding. If evidence is weak, answer with best effort but stay general.\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"1) Main explanation (2â€“4 sentences)\\n\"\n",
    "        \"2) Example / formula if relevant (1â€“3 lines)\\n\"\n",
    "        \"3) Practical connection (1â€“2 sentences)\\n\"\n",
    "        \"Do NOT mention tools. Do NOT paste the evidence.\"\n",
    "    ))\n",
    "    human = HumanMessage(content=f\"USER QUESTION:\\n{user_input}\\n\\nEVIDENCE:\\n{rag_evidence}\")\n",
    "    try:\n",
    "        resp = llm.invoke([sys, human])\n",
    "        text = (getattr(resp, \"content\", \"\") or \"\").strip()\n",
    "        return text if text else \"I couldn't generate a course explanation from the evidence.\"\n",
    "    except Exception:\n",
    "        return \"I couldn't generate a course explanation from the evidence.\"\n",
    "\n",
    "def build_final_answer_multi(\n",
    "    user_input: str,\n",
    "    weather_out: Optional[str],\n",
    "    calendar_out: Optional[str],\n",
    "    holiday_out: Optional[str],\n",
    "    rag_out: Optional[str],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Multi-tool answer:\n",
    "    - Non-RAG sections come directly from tool outputs (deterministic)\n",
    "    - Course explanation is an LLM-generated summary grounded in RAG evidence\n",
    "    - Evidence section includes the raw RAG chunks\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    if weather_out:\n",
    "        parts.append(\"**Weather**\\n\" + weather_out)\n",
    "\n",
    "    if calendar_out:\n",
    "        pretty = _format_exam_json(calendar_out)\n",
    "        parts.append(\"**Schedule / Exams**\\n\" + (pretty if pretty else calendar_out))\n",
    "\n",
    "    if holiday_out:\n",
    "        parts.append(\"**Holidays**\\n\" + holiday_out)\n",
    "\n",
    "    if rag_out:\n",
    "        course_expl = _llm_course_explanation(user_input=user_input, rag_evidence=rag_out)\n",
    "        parts.append(\"**Course explanation**\\n\" + course_expl)\n",
    "        parts.append(\"**Evidence (retrieved chunks)**\\n\" + rag_out)\n",
    "\n",
    "    return \"\\n\\n\".join([p.strip() for p in parts if p and p.strip()]).strip()\n",
    "\n",
    "print(\"âœ“ Response generation functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ae46f",
   "metadata": {},
   "source": [
    "## 6. Define LangGraph Nodes\n",
    "\n",
    "**Agent Node**: Makes decisions and calls appropriate tools based on user query.\n",
    "\n",
    "**Tool Node**: Executes tool calls and implements auto-chaining logic (e.g., fetching weather for exam dates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d4f278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent node defined\n"
     ]
    }
   ],
   "source": [
    "def agent_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    Agent node: Analyzes user input and decides which tools to call.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    today = date.today().isoformat()\n",
    "\n",
    "    system_prompt = SystemMessage(content=f\"\"\"\n",
    "You are a helpful assistant for a student. You can use tools.\n",
    "\n",
    "CONTEXT:\n",
    "- Today's date is {today}.\n",
    "- Default location is Haifa, Israel.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- If the user asks multiple things, answer ALL of them in one response.\n",
    "- After tool call(s), restate results clearly in your answer.\n",
    "- Do NOT respond with generic filler when the user asked a question.\n",
    "\n",
    "LOCATION:\n",
    "- If user asks weather without specifying a city, assume Haifa, Israel.\n",
    "\n",
    "HOLIDAYS:\n",
    "- \"next holiday\" -> get_next_holiday\n",
    "- list holidays -> get_public_holidays(year)\n",
    "- check specific date -> is_public_holiday(date)\n",
    "\n",
    "RAG:\n",
    "- Use search_course_materials for IR concepts.\n",
    "- Summarize: 2â€“4 sentences + example/formula if relevant + practical connection.\n",
    "\n",
    "HONESTY:\n",
    "- Never invent dates, weather, holidays, or events.\n",
    "- If forecast not available for that date, say so.\n",
    "\"\"\".strip())\n",
    "\n",
    "    response = llm_with_tools.invoke([system_prompt] + messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"âœ“ Agent node defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b25aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tool node with auto-chaining defined\n"
     ]
    }
   ],
   "source": [
    "def tool_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    Tool node: Executes tool calls and implements auto-chaining logic.\n",
    "    \n",
    "    Auto-chaining example: If user asks \"weather on exam day\", this node:\n",
    "    1. Calls calendar tool to get exam date\n",
    "    2. Automatically calls weather tool with that specific date\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    user_text = _last_user_text(messages)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    exam_date: Optional[str] = None\n",
    "    saw_weather_call_missing_date = False\n",
    "    weather_location_used: Optional[str] = None\n",
    "\n",
    "    tool_calls = getattr(last_message, \"tool_calls\", None) or []\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "\n",
    "        print(f\"\\nðŸ”§ Calling tool: {tool_name}\")\n",
    "        print(f\"   Arguments: {tool_args}\")\n",
    "\n",
    "        # Guard: reject placeholder locations for weather\n",
    "        if tool_name == \"get_weather\":\n",
    "            loc = str(tool_args.get(\"location\", \"\")).strip()\n",
    "            on_date = tool_args.get(\"on_date\", None)\n",
    "\n",
    "            bad_locs = {\"current location\", \"my location\", \"here\", \"now\", \"today\", \"local\", \"location\"}\n",
    "            if loc.lower() in bad_locs or len(loc) < 2:\n",
    "                tool_result = \"Error: No valid city provided. Please provide a real city (e.g., 'Haifa, Israel').\"\n",
    "            else:\n",
    "                weather_location_used = loc\n",
    "                if not on_date:\n",
    "                    saw_weather_call_missing_date = True\n",
    "                tool_result = tool_dict[tool_name].invoke(tool_args)\n",
    "        else:\n",
    "            tool_result = tool_dict[tool_name].invoke(tool_args)\n",
    "\n",
    "        preview = str(tool_result)[:160].replace(\"\\n\", \" \")\n",
    "        print(f\"   Result preview: {preview}...\")\n",
    "\n",
    "        if tool_name == \"check_calendar\":\n",
    "            maybe_date = _extract_date_from_calendar_json(str(tool_result))\n",
    "            if maybe_date:\n",
    "                exam_date = maybe_date\n",
    "\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=str(tool_result),\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ðŸ” AUTO-CHAINING: weather on exam day if model forgot on_date\n",
    "    if _user_requested_weather_on_exam_day(user_text) and exam_date and weather_location_used and saw_weather_call_missing_date:\n",
    "        print(f\"\\nðŸ” Auto-chaining: Fetching forecast for exam date {exam_date}...\")\n",
    "        chained = get_weather.invoke({\"location\": weather_location_used, \"on_date\": exam_date})\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=str(chained),\n",
    "                name=\"get_weather\",\n",
    "                tool_call_id=\"auto_chained_exam_weather\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "print(\"âœ“ Tool node with auto-chaining defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0f4c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Conditional routing function defined\n"
     ]
    }
   ],
   "source": [
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"\n",
    "    Decides whether to continue with tool execution or end the conversation.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last_message, \"tool_calls\", None) or []\n",
    "    return \"tools\" if tool_calls else \"end\"\n",
    "\n",
    "print(\"âœ“ Conditional routing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d1642",
   "metadata": {},
   "source": [
    "## 7. Build and Compile LangGraph Workflow\n",
    "\n",
    "This creates the state machine that orchestrates the agentic behavior:\n",
    "- Entry point: Agent node\n",
    "- Conditional edge: If tools needed â†’ Tool node, else â†’ End\n",
    "- Loop back: After tools execute â†’ Agent node (to process results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ceb32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LangGraph workflow...\n",
      "âœ“ LangGraph workflow compiled successfully\n",
      "============================================================\n",
      "\n",
      "System ready! ðŸš€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building LangGraph workflow...\")\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"âœ“ LangGraph workflow compiled successfully\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSystem ready! ðŸš€\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e037a3",
   "metadata": {},
   "source": [
    "## 8. Define Chat Function\n",
    "\n",
    "The chat function implements a hybrid response policy:\n",
    "- **Single RAG query**: LLM-generated summary\n",
    "- **Multi-tool with RAG**: Deterministic tool outputs + LLM course explanation + evidence\n",
    "- **Non-RAG only**: Deterministic aggregation from tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0839f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Chat function defined\n",
      "\n",
      "ðŸŽ¯ System is now ready for demonstrations!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat(user_input: str):\n",
    "    \"\"\"\n",
    "    Main chat interface that processes user queries through the agentic workflow.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"USER: {user_input}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "    result = app.invoke(state)\n",
    "\n",
    "    messages = result[\"messages\"]\n",
    "    final_message = messages[-1]\n",
    "    llm_response = (getattr(final_message, \"content\", \"\") or \"\").strip()\n",
    "\n",
    "    # Detect tool usage and which tools were used\n",
    "    tool_names = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, ToolMessage):\n",
    "            name = getattr(m, \"name\", \"\") or \"\"\n",
    "            if name:\n",
    "                tool_names.append(name)\n",
    "    unique_tools = _unique_preserve_order(tool_names)\n",
    "    used_tools = len(unique_tools) > 0\n",
    "\n",
    "    weather_out, calendar_out, holiday_out, rag_out = _extract_tool_outputs(messages)\n",
    "\n",
    "    # HYBRID POLICY:\n",
    "    # - If only RAG used: show LLM answer (nice), optionally add evidence if you want.\n",
    "    # - If multiple tools used and RAG used: show deterministic non-RAG + LLM course explanation + evidence.\n",
    "    # - If non-RAG only: show deterministic tool output.\n",
    "    if not used_tools:\n",
    "        response = llm_response\n",
    "\n",
    "    elif len(unique_tools) == 1 and unique_tools[0] == \"search_course_materials\":\n",
    "        # Single RAG question -> prefer LLM summary\n",
    "        response = llm_response if not _is_social_or_useless(llm_response) else (rag_out or llm_response)\n",
    "\n",
    "    elif rag_out:\n",
    "        # Multi-tool with RAG -> deterministic sections + LLM course explanation + evidence\n",
    "        response = build_final_answer_multi(\n",
    "            user_input=user_input,\n",
    "            weather_out=weather_out,\n",
    "            calendar_out=calendar_out,\n",
    "            holiday_out=holiday_out,\n",
    "            rag_out=rag_out,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Multi-tool without RAG -> deterministic aggregation from tools\n",
    "        parts = []\n",
    "        if weather_out:\n",
    "            parts.append(\"**Weather**\\n\" + weather_out)\n",
    "        if calendar_out:\n",
    "            pretty = _format_exam_json(calendar_out)\n",
    "            parts.append(\"**Schedule / Exams**\\n\" + (pretty if pretty else calendar_out))\n",
    "        if holiday_out:\n",
    "            parts.append(\"**Holidays**\\n\" + holiday_out)\n",
    "        response = \"\\n\\n\".join([p.strip() for p in parts if p and p.strip()]).strip()\n",
    "        if not response:\n",
    "            response = llm_response\n",
    "\n",
    "    # Final fallback: if model was useless and deterministic is empty, show last tool output\n",
    "    if _is_social_or_useless(response):\n",
    "        for m in reversed(messages):\n",
    "            if isinstance(m, ToolMessage) and (m.content or \"\").strip():\n",
    "                response = m.content.strip()\n",
    "                break\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"ASSISTANT: {response}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"âœ“ Chat function defined\")\n",
    "print(\"\\nðŸŽ¯ System is now ready for demonstrations!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762d358",
   "metadata": {},
   "source": [
    "## 6. Demo: Example Queries\n",
    "\n",
    "Let's demonstrate the system with various query types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e9addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: RAG Query - Course Material\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "USER: What is TF-IDF?\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Calling tool: search_course_materials\n",
      "   Arguments: {'query': 'TF-IDF'}\n",
      "\n",
      "Searching for: 'TF-IDF'\n",
      "Found 5 relevant chunks.\n",
      "   Result preview:   ==================================================  ðŸ“„ Chunk 1: Information Retrieval - Word Similarity  - WordNet - Word Vectors Development: Moshe Friedman C...\n",
      "\n",
      "============================================================\n",
      "ASSISTANT: TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used in Information Retrieval to calculate the importance of words in a document. It takes into account both the frequency of a word in a document (term frequency) and its rarity across all documents in the corpus (inverse document frequency).\n",
      "\n",
      "The goal of TF-IDF is to assign higher weights to words that are highly relevant to the document, while downplaying words that are common or irrelevant.\n",
      "\n",
      "For example, if we have two documents:\n",
      "\n",
      "Doc 0: \"Fresh Red Apple\"\n",
      "Doc 1: \"Green Granny Apple\"\n",
      "\n",
      "TF-IDF would calculate the importance of each word in these documents. For instance, the word \"Apple\" would be highly weighted in both documents because it's a key term for both texts.\n",
      "\n",
      "In practical terms, TF-IDF is used in search engines to rank documents based on their relevance to a query. It helps to filter out irrelevant results and bring more accurate matches to the top of the search results.\n",
      "\n",
      "Note that the provided output from the tool call shows some example scores calculated using TF-IDF, but these are not directly relevant to explaining the concept itself.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: RAG-only query\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE 1: RAG Query - Course Material\")\n",
    "print(\"=\" * 70)\n",
    "result1 = chat(\"What is TF-IDF?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba130f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: External API - Weather\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "USER: What's the weather in Haifa today?\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Calling tool: get_weather\n",
      "   Arguments: {'on_date': '2026-02-05', 'location': 'Haifa, Israel'}\n",
      "   Result preview: Forecast for Haifa, Israel on 2026-02-05: - Temperature: 10.6Â°C to 22.5Â°C - Max precipitation probability: 0% - Max wind: 23.7 km/h...\n",
      "\n",
      "============================================================\n",
      "ASSISTANT: **Weather**\n",
      "Forecast for Haifa, Israel on 2026-02-05:\n",
      "- Temperature: 10.6Â°C to 22.5Â°C\n",
      "- Max precipitation probability: 0%\n",
      "- Max wind: 23.7 km/h\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: External API - Weather\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: External API - Weather\")\n",
    "print(\"=\" * 70)\n",
    "result2 = chat(\"What's the weather in Haifa today?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Multi-Tool Query\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "USER: When is my next exam? And for that exam date, tell me what the weather is at that date, not for today, but for the exam date for Haifa Israel.\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Calling tool: check_calendar\n",
      "   Arguments: {'query': 'next exam'}\n",
      "   Result preview: {\"found\": true, \"title\": \"Machine Learning Exam\", \"date\": \"2026-02-10\", \"time\": \"09:00\", \"type\": \"exam\"}...\n",
      "\n",
      "ðŸ”§ Calling tool: get_weather\n",
      "   Arguments: {'location': 'Haifa, Israel', 'on_date': '2026-02-05'}\n",
      "   Result preview: Forecast for Haifa, Israel on 2026-02-05: - Temperature: 10.6Â°C to 22.5Â°C - Max precipitation probability: 0% - Max wind: 23.7 km/h...\n",
      "\n",
      "============================================================\n",
      "ASSISTANT: **Weather**\n",
      "Forecast for Haifa, Israel on 2026-02-05:\n",
      "- Temperature: 10.6Â°C to 22.5Â°C\n",
      "- Max precipitation probability: 0%\n",
      "- Max wind: 23.7 km/h\n",
      "\n",
      "**Schedule / Exams**\n",
      "Next exam: Machine Learning Exam\n",
      "Date: 2026-02-10 09:00\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Multi-tool query\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 3: Multi-Tool Query\")\n",
    "print(\"=\" * 70)\n",
    "result3 = chat(\"When is my next exam? And for that exam date, tell me what the weather is at that date, not for today, but for the exam date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b2e550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: External API - Holidays\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "USER: When is the next holiday in Israel?\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Calling tool: get_next_holiday\n",
      "   Arguments: {'country_code': 'IL'}\n",
      "\n",
      "Finding next holiday in IL after 2026-02-05...\n",
      "   Result preview: Next holiday in IL: 2026-03-02 â€” Erev Purim....\n",
      "\n",
      "============================================================\n",
      "ASSISTANT: **Holidays**\n",
      "Next holiday in IL: 2026-03-02 â€” Erev Purim.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Holiday API\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 4: External API - Holidays\")\n",
    "print(\"=\" * 70)\n",
    "result4 = chat(\"When is the next holiday in Israel?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c49e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 5: RAG - Complex Course Question\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "USER: Explain PageRank algorithm and how it relates to link analysis\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Calling tool: search_course_materials\n",
      "   Arguments: {'query': 'PageRank algorithm link analysis'}\n",
      "\n",
      "Searching for: 'PageRank algorithm link analysis'\n",
      "Found 5 relevant chunks.\n",
      "   Result preview:   ==================================================  ðŸ“„ Chunk 1: Information Retrieval Course - Lecture Notes  PageRank Algorithm: PageRank is a link analysis a...\n",
      "\n",
      "============================================================\n",
      "ASSISTANT: The PageRank algorithm is a link analysis algorithm used by Google Search to assign a numerical weight to each element of a hyperlinked set of documents. The basic idea behind PageRank is that important pages receive more links from other pages. The algorithm can be calculated iteratively using the formula: PR(A) = (1-d) + d * (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn)), where PR(A) is the PageRank of page A, d is a damping factor, and C(Ti) is the number of links pointing to page Ti.\n",
      "\n",
      "In terms of link analysis, PageRank can be seen as a way to measure the importance or authority of a web page based on the number and quality of links pointing to it. The more high-quality links a page has, the higher its PageRank will be. This means that if a page is linked to by many other important pages, it is likely to have a high PageRank itself.\n",
      "\n",
      "The Vector Space Model (VSM) and Term Frequency-Inverse Document Frequency (TF-IDF) are also related concepts in information retrieval, but they are not directly related to link analysis or the PageRank algorithm. VSM represents text documents as vectors of identifiers, while TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection.\n",
      "\n",
      "In summary, the PageRank algorithm is a key component of Google's search engine and is used to measure the importance of web pages based on their link structure.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Complex RAG query\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 5: RAG - Complex Course Question\")\n",
    "print(\"=\" * 70)\n",
    "result5 = chat(\"Explain PageRank algorithm and how it relates to link analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244cccf",
   "metadata": {},
   "source": [
    "## 7. System Architecture Summary\n",
    "\n",
    "**Agentic Behavior Demonstrated:**\n",
    "1. âœ… Conditional tool routing based on query intent\n",
    "2. âœ… Auto-chaining: Weather forecast for exam dates\n",
    "3. âœ… Multi-tool orchestration\n",
    "4. âœ… RAG + External API integration\n",
    "\n",
    "**Tools Used:**\n",
    "- **RAG**: ChromaDB + Ollama embeddings (nomic-embed-text)\n",
    "- **LLM**: Ollama llama3.1:8b (local)\n",
    "- **External API 1**: Open-Meteo (Weather)\n",
    "- **External API 2**: Hebcal (Holidays)\n",
    "- **Internal Tool**: Calendar (JSON-based)\n",
    "\n",
    "**Assignment Track**: Agentic RAG (Track 3) âœ“\n",
    "**Bonus APIs**: 2 external APIs beyond RAG âœ“\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
