טוב, כיוון שהפעם לא היותה קלות, אני אנסה עוד במהלך השבוע לעשות איזו הקלטה שמן הסנטי יותר קצרה ממה שהיה פעם קודמת, אני מסכיר את הלוח, תגידו לי? לא? אוקיי. אז המצגת המוסע, שוב, אני לא חושב שאנחנו נעבור יותר, ואם שמתם לב, במודל גם שיניתי קצת את הכותרת, זה משהו שהוא ongoing ל-expanding classic IR, מה הרעיון? הרעיון הוא שזה אמור להיות איזושהי יחידה שהיא מעבר בין ה-classic IR לבין שימוש ב-LLMs ובראגים בהמשך, זה הרעיון, אז כאילו יש נושאים שהם בטפר, ככה כמו מה שהתחלנו בשיעור שעבר, שאנחנו נמשיך אותו היום. אוקיי, אז אנחנו בעצם עושים איזה ריקה בהתחלה של שבוע שעבר, אז התחלנו את כל הנושא הזה מתוך הנושא של query expansion.

אוקיי, יש לנו בעצם איזושהי שאילתה, שאילתה טקסטואלית, אנחנו רוצים להשיג מסמכים שקשורים לאותה שאילתה, אגב, כל אחד מהרכיבים שאני אומר שאפשר לפרק ולהערער עליו, נעשה קצת מזה היום, אבל כאילו מהו ה-input ומהו ה-output למשל, אז היום קצת נתחיל עם זה. אז אחת הבעיות הגדולות שיש לנו זה שהחיפוש הוא בעצם כל מיני נורמליזציות וכל מיני מסאז'ינג לאינפורמציה, נגיד Lemmatization ודברים כאלו, או הורדה של אקסנטים או ניקוד ודברים כאלו, עדיין אנחנו מחפשים בעצם על בסיס המילים המדויקות שהיו בשאילתה, אוקיי? מדויקות מינוס נורמליזציה. והדבר הזה מוביל בהכרח ל-recall נמוך מדי, כלומר, אנחנו לא מוצאים את האינפורמציה הרלוונטית.

לא מספיק. אוקיי, והתחלנו פה מבעצם שני מהלכים ששניהם קשורים במישרים ובהקיפין ל-user feedback, אוקיי? אז או שה-user בעצמו אומר לנו מהם התוצאות, או שאנחנו מניחים משהו לפי התוצאות, אולי אפילו לפי התוצאות עצמם, גם אם זה קצת ככה לופ שמוביל את עצמו, אז המודל החזיר משהו, אז אנחנו מניחים שזה תוצאות טובות. זה לא בכך נכון, אבל גם אם נניח שזה נכון, אז מה השיטה? אז השיטה היא הראינו את האלגוריתם רושיו, שאני לא אעבור עכשיו על האלגוריתם, אבל הרעיון הוא שאנחנו רוצים לעשות איזשהו prototype, centroid, וקטור מרכזי, לדוגמאות שהם, החשבנו אותם אם בגלל feedback ישיר או בגלל feedback עקיף, כרלוונטיות, ואלה שהם אינם רלוונטיות.

ואז אנחנו עושים איזו מניפולציה על ידי שניהם ומקבלים בעצם שאילטה מורחבת של, בעצם לוקחת בחזבון עוד כל מיני מילים נוספות, אוקיי? אז זה היה מאמץ אחד בשביל רחבת השילטות, דיברנו גם על מאמץ נוסף שהוא שימוש בלקסיקונים, הדוגמה הקלאסית זה WordNet, אז כאן יש לנו שתי סנסים שונים, שני משמעויות שונות למילה bank, אפילו אחת מהן עם חלק דיבר אחד ואחת עם חלק דיבר אחר, יש לנו noun וverb, אוקיי? וגם בתוך כל אחד מהם יש לנו כמה משמעויות, אז WordNet זה היה מאמץ הראשון בעצם לבוא ולעשות משפחות של מילים, לפי יחסים באיררכיה של זהויות, או יחסי איררכיה של מילה שהיא חלק מקבוצה יותר גנרית, דברים כאלו, נתנו לזה כמה דוגמאות, ולא נעבור עכשיו על יחסים השונים, יש פה כל מיני דוגמאות, ואמרנו כבר שהמאמץ הזה הוא גם מאוד יקר, הוא דורש המון המון עבודה ידנית, אוקיי? תחזוקה לאורך זמן, גם את המאמץ הגדול ביותר עשו עבור אנגלית, ושפות אחרות לא הגיעו לרמה כל כך גבוהה, אז היה צורך בהרחבה של בעצם מילים שקשורות, שהן דומות למילים שחיפשנו, ובעצם כשהגדרתי את זה, הדמיון, הוא לא בהכרח מגדיר יחס שהוא מורדר היטב כמו מילים נרדפות, כמו synonyms, אוקיי? אנחנו יודעים שלפעמים יש מילה שהיא, אפילו אנחנו לא יודעים להגדיר את היחס בין שני המילים מבחינה סמנטית בצורה מדויקת, אנחנו יודעים אבל שיש איזשהו דמיון ביני, כן? הוטל, המוטורינג הוטל, שבאים עם הרכב ונכנסים, בגלל זה זה נקרא ככה, זה דומה להוטל, וזה מה שאנחנו מניסים, ועוד הרבה דוגמאות אחרות שחלקם הבאנו, הנה, כאן יש עוד דוגמאות, car וbicycle, car וhorse, ויחסים שונים של מילים עם שיפוט אנושי, זה דוגמה שהראינו שיעור שעבר. אז מה נעשה בשביל לתפוס את הדמיון הזה? אז ההנחה שבעצם אנחנו מתבססים עליה, זה שמה שנותן לנו את המשמעות למילה זה הקונטקט שהמילה מופיעה יחד איתה. והתחלנו במאמץ הזה יחד עם כהה קורונסס, אז נגיד שאת הביטוי הזה עוד לא הזכרנו, אבל בעצם אפשר להגיד כבר מהשלב הזה, שהמאמצים הראשונים האלו של הצגת מילה בעזרת וקטור, אנחנו יכולים כבר לקרוא לזה world embedding.

אז world embedding זה הוקטור הזה של המילים, שמציגות מילים אחרות, בהתבסס על ההיפותזה של ההתפלגות של הריס. מכאן בעצם הכל מתחיל, וגם בהמשך אנחנו מהר מאוד, כבר עשינו את זה שיעור שעבר, עברנו למה שנקרא dense vectors, וקטורים תחוסים יותר אם נרצה, צופים תחוסים יותר. אז המאמץ הזה נדרש כידוע לכל מיני אפליקציות, לא נרד לפרטים, אבל יש לנו בעצם דבר כזה, יש לנו מילים ומילים שמופיעות בקונטקסט, ובעצם ככה כל מילה בקורונס מטריקס שעשינו, אנחנו סופרים כמה מילים מופיעים יחד איתן, כמו שראינו קודם גם בהדגמה שהיה לנו כאן שלו עד בהתחלת השיעור, אז זה היה ה-input למה? זהו, זה עוד לא דחוס, זה מאוד מאוד, וקטור עם מימדיות מאוד מאוד גדולה, מגיעים לזה כבר, אבל כבר כאן יש לנו, אם נרצה עוד אפילו בשלב הראשון עוד לפני שהגענו לווקטור הזה, יש עוד שלב נוסף, שעשינו תצוגה של... שאילטות הבינאריות של... שלמדנו את השאילטות הבינאריות של ה-IR הקלאסי, של היחזור מידע קלאסי, אז בעצם גם ראינו שלפעמים אנחנו בתור שלב ראשון גם מציגים את המידע ושומרים אותו בצורה בינארית, אז בעצם הדבר הזה בעצם אומר האם הוא מופיע ביחד או לא מופיע ביחד, אז אם נרצה, וקטור מילים זה עוד כידוד של כל המילים בתור שלב הראשון של פקטוריזציה כשעשינו bug avoids, מופיע או לא מופיע, וזה נקרא גם one-hot recoding, מי שמכיר את ההונך הזה מפייטון אולי או מלימדת מכונה או משהו כזה, בעצם הכוונה היא מופיע או לא מופיע, אנחנו יש לנו כל מילה, אנחנו נשתמש באיזשהו אינדקס ליצג אותה, ואז עבור אותה מילה, אם המילה הזאת קיימת במסמך אז נדליק את הפיצ'ר, ניתן אחד, אחרת, ולשאר הפיצ'רים ניתן אפס, זה הרעיון של one-hot recoding, מה? כן, ושם התחלנו, נכון? אבל זה כבר הייתה תצוגה כבר של מילים כוקטורים, ווקטור מאוד עליל, מאוד ככה שטחי ולא אינפורמטיבי, נכון? אבל זה היה המאמץ הראשון, אחרי זה הגענו לקורא קורנסס, להציג מילים לידי מילים מהקונטקסט, ודיברנו על לתת סיונים שונים למילים שונות, בעזרת positive pointwise mutual information, ששמענו על זה בהתחלת השיעור, ואז דיברנו על בעצם כמה וריאציות שונות, שבאות לפתור מעבר לבעיה הזאת, בואו לפתור שתי בעיות ביחד, גם את הבעיה של המימדיות הגדולה, הרי אם אנחנו קודם הצגנו מסמכים, בעזרת ייצוג של one-hot encoder כזה של מילים, ואז עברנו לייצג מילים לידי וקטור של מילים, אז המימדיות אפילו גדלה בריבוע, אז רק תחמיר המצב, אוקיי? אז איך אנחנו פותרים את הבעיה הזאת? אז דיברנו על מטריקות, שבעצם, אגב, בתיאוריה בכלל של כל התחום הזה של עמידה עמוקה, יש תיאוריות שמקשרות בצורה מאוד מאוד חזקה, את כל המעמד שאנחנו עושים פה בעצם לאלגוריתמי תחיסה.

למה? מה אנחנו עושים באלגוריתמי תחיסה? אז אלגוריתמי תחיסה מתחלקים בצורה מאוד מאוד גסה, בשתי סוגים. יש לנו תחיסה שהיא lossless, כלומר אנחנו לא הולכים שום מידע לאיבוד, אוקיי? אז נגיד שאנחנו רוצים עכשיו להעביר איזושהי הודעה ברשת תקשורת, אז אנחנו, לא יודע אם למדתם באיזה קורס אלגוריתם, נגיד קוד אופמן, איך דוחסים את זה? אולי למדתם? אם לא, אז לא. אוקיי, אז זו תחיסה שהיא בלי לאבד שום מידע, כל מי שיצא לשמור תמונות במחשב אולי, או באיזשהו מקום, היום אולי פחות שמורים במחשב, אז יש כל מיני פורמטים, דוגמת JPEG, שהוא בעצם הרבה מידע הולך לאיבוד, אבל ההנחה היא שהמידע של התמונה שאנחנו רואים מותאם לעין האנושית.

והמידע שהלך לאיבוד, רובו הוא לא כל כך משמעותי, אנחנו לא שונאים לב בחשבת, מבחינת הרזולוציה שעין שלנו יכולה לראות. אוקיי, אז זו מידע שהוא loose'י נקרא, כאילו כן הולך לאיבוד. אז ברמה העקרונית אנחנו דיברנו על מידע שכן הולך לאיבוד, בעזרת טכניקות שמתבססות כרגע בעיקר על פירוק SVD, משטבות של Singular SV, Value Decomposition, אוקיי, שהצגנו בעיקר את PCA שמתבסס קצת על זה, והסכמנו קצת את LSA, עכשיו כאילו נראה איזה דוגמה גם עם LSA, אז נדבר על זה בדיוק עכשיו, מה הרעיון של כל הדברים האלו? הרעיון הוא שאנחנו רוצים בעצם לעשות תחיסה, אבל התחיסה הזאת, זה מתבסס על תיאוריה שאמרנו שקשורה לשנות, שרוב המידע מתבסס על הפריסה של הנתונים, יש תיאוריות שטוענות שזה לא רוב המידע, כאילו שזה לא מספיק חזק בשביל למדוד את מירב המידע, אבל זה היה בסוגריים.

הטכניקות האלה שראינו ודיברנו עליהן שניהן, הן בעצם טכניקות ליניאריות, שעושות קומבינציה ליניאריות על הפיצ'רים המקוריים, ודרכם שואבים את רוב המידע לפיצ'רים התחוסים. אוקיי, אז זה הרעיון, מאוד בקרליות ניקח מ-D פיצ'רים, נייצר-K פיצ'רים חדשים, אבל ששומרים את רוב האינפורמציה שנמדדת על ידי השונות. אז בעצם האינפוט לדבר הזה, זה יהיה איזושהי מטריצת כל-ווריינס בין כל זוג של פיצ'רים, שונות משותפת, ואם אני רוצה לדאיר את השונות המשותפת הממורכזת, כי בעצם מה אנחנו עושים, גם שונות עושה את זה בעצם, אז מה אנחנו עושים בשונות משותפת? אנחנו אומרים תן לנו את ה-SUM של, נגיד שיש לנו פיצ'ר 1 פחות ה-AVERAGE שלו, של פיצ'ר 1, אז נעשה פה S1 קטן, כאילו זה איזשהו VALUE לפיצ'ר הזה, תפול, כן ה-SUM זה על הכל, זה כאן בסוגריים, תפול F2 פחות ה-AVERAGE של F2, זה שונות משותפת, זה COVARIANCE בין F1 ל-F2, כן, כבר אני מחלק, חלק למספר הדוגמאות, כן, מה? כן, אגב יש עניין שיחלק ב-N או N-1, אני לא נכנס לזה אפילו, על אוכלוסייה ומדגם, זה לא חשוב, זה אמור להיות ככה כמובן, או אם נרצה, כן זה לא משנה בעצם, או אולי יהיה יותר קל לעשות דבר כזה יותר קרי, בסדר? ומי שמזהה, אז בעצם אם היינו עושים COVARIANCE בין פיצ'ר לעצמו, והיינו מקבלים את ה-VARIANCE שלו, זה ה-INPUT של PCA, מכינים, יש לנו איזשהו DATA SET, מתחילים בעצם מטריצה, שלוקחת את ה-COVARIANCE בין כל זוג פיצ'רים, כן? אז זה יהיה INPUT ל-PCA, למה אנחנו עושים פירוק SSD, אנחנו לוקחים בעצם כל זוג פיצ'רים שהופיע ב-DATA SET, ואנחנו מחשבים את ה-COVARIANCE שלהם, נחשוב שיש לנו פיצ'ר F1, F2, F3, אולי בדרך כלל נהוג להשתמש ב-X1, X2 עד XD, אז ככה יהיה גם בעמודות וגם בשורות, ו-COVARIANCE הוא SYMMETRICS, COVARIANCE של F1, F2 שווה F2, אבל אם אנחנו נשים פה בנוסחה, זה מה שאמרתי לגבי VARIANCE, אם נשים פה בנוסחה פיצ'רים עצמו, אז אנחנו משפילים בעצם את הפרש הריבועי בין כל הערכים לממוצע, סוף מחלקים בפיצ'רים ממוצע, מחלקים בהם, זו בעצם הגדרה של שונות, אוקיי, מי שזוכר, בכל מקרה שונות משותפת מודדת בעצם סוג של קורלציה ליניארית ביניהם, ומי שמכיר את מקדם עמיתם של פירסון, אז אפשר להגיד שמה שמקדם עמיתם של פירסון עושה, זה רק סוג של נרמול, כך שיהיו באותה סקאלה, סקלים ביניהם, כן, כי אנחנו מחלקים בסתיית התקן של שניהם, ככה שה-covariance רק מודד האם יש קורלציה חיובית או שלילית, אבל אי אפשר להצטמח על עוצמת הערכים שיוצאים, על עוצמת הקשר ביניהם, אז הוא בעצם... מה? מקדם עמיתם של פירסון זה קורלציה ליניארית, לא, לא, זה... זה לא... זה דווקא לערכים רציפים, לא פדגוריה.

אוקיי, אז זה מקדם עמיתם של פירסון, אז בעצם בזה שאנחנו עושים את הדבר הזה, מנרמלים את הערכים בין מינוס 1 ל-1, אבל ה-covariance עצמו כבר תופס את הקשר. אוקיי, אז בזה שאנחנו, ב-covariance אנחנו אומרים שאנחנו ממרכזים את הערכים, זה פשוט נמצא בנוסחה של ה-covariance עצמה, מינוס average זה מרכוז. בסדר? זה ה... אנחנו לא עושים את מקדם עמיתם של פירסון, אנחנו נותנים כאינפוט את מטריצת ה-covariance, לא עושים סטנדרטיזציה מלאה.

כן, כן. לא בהכרח צריך את הסטנדרטיזציה מלאה. מרכוז הוא אינפוט מספיק הרבה פעמים ל-PCA, כאילו לא בכל מקרה עושים סטנדרטיזציה.

שיש את הפיצ'ר על הסוויץ. בפרקטיקה זה לא נכון. אם אין לנו פיצ'ר אחד עם סקאלה שהוא בסדרי גודל יותר גדול מהפיצ'ר השני, אני אפילו אגע את זה.

אז רק מרכז. זה מה שעושים בפרקטיקה. בכל מקרה זה לא משנה כרגע, אז אם היינו עושים סטנדרטיזציה לכל פיצ'ר, אז בעצם זה, היינו יכולים להגיד שנתנו כאינפוט את, במילים אחרות את מיטם של פירסון תוך הטריטר.

אם חושבים על מה יש לנו בכל מקום. ועושים פירוק לדבר הזה. כלומר הפיצ'רים החדשים, יש להם קורולציה למטריצה המקורית.

משם זה מגיע. אוקיי, ודיברנו על דבר נוסף שיש לנו. על בעצם ניסיון להרכיב משהו דומה לפי.י.י.י. רק שכאן האינפוט לא יהיה מטריצת הקורואריץ.

בשביל זה נזכיר קודם למה התכווננו בקונטקסט ווינדו. אוקיי, בעצם בדוגמאות שדיברנו בהתחלה. איך יצאנו את מטריצת הקורואריץ בעצם.

אז אפילו לקחנו שני מסמכים שיש מופע משותף. ואמרנו, כאן הם מופיעים ביחד. אבל זה משהו שהוא לא באמת מודד מופע משותף.

ככל שהמסמך ארוך יותר, אז בעצם היחס הזה הוא פחות משמעותי. ומה שעושים זה לוקחים חלון בגודל כמה מילים. כאן יש דוגמה לפלוס מינוס שני מילים.

וזה מה שמודד את הקונטקסט. אינטואיטיבית זה ברור גם שככל שמילה יותר קרובה, זה חוזר על עצמאות ועוד פעמים, יותר קרובה במיקום שלה במשפט. אז אם המופעים משותפים הם רבים, אז כנראה שיש לזה משמעות יותר גדולה מאשר שני מילים שהופיעו בספר ביחד, נגיד, אם אני מגזים לצד השני.

אז דיברנו כבר על ה-PMI או TF-IDF, שיכול למדוד את המופעים המשותפים. זה בהחלט יכול לעשות את זה, אבל הוא עדיין משאיר לנו וקטור מאוד ספרס. אז זה אומר שרוב המילים לא יופיעו ביחד.

אחרי שאנחנו נעשה דבר כזה, אנחנו עדיין רוצים משהו שיקטיל לנו את הוקטור מאוד, ובשביל זה יש לנו את ה-LSA או LSI, בכיום זה נקרא. אז זו המוטיבציה שלנו ל-Vector Dense. אז אנחנו דיברנו קצת על PCA, נתן עכשיו דוגמה של LSA.

זה... היה צריך לסדר את ה... פשטה שלי, היה צריך לסדר את השקפים האלה קודם, אז זה סתם בשביל להראות מה הכוונה שונות משותפת, אז זו סימולציה שתמיד אוהבים לראות. אנחנו רואים פה איזשהו גרף של מופעים משותפים בין שתי פיצ'רים. אוקיי? לפני שום... לא נעשה פה שום תהליך.

ואז אנחנו רואים את החיפוש של... בעצם הציר, שאחרי קיבוץ הוא תופס את החיר בשונות. זה בעצם מייצג את ה-Principal Component הראשון. כן? כמובן שזה... רואים פה את הקו האדום, אז הקו הזה כאן, ובעצם ייצג לנו את ה-Feature עם החיר בוורץ.

אתה שואל למה זה עובד? כאילו לא עשינו פונקציית שד עם גרדיאנטים, ובדקנו שזה... כן, זה קוראים לסימולציה גרדיאנטית. נכון, אז קודם כל יש מימושים שכן עושים את זה, אבל צודק פה שזה אלגברה ליניארית נטת. נכון.

בכל מקרה, אבל אני לא נכנס לזה אפילו, כי לא נגיל. אז מה אנחנו עושים ב-LSA? אנחנו עושים משהו מאוד דומה. ל-LSA יש עקבות של Latents, זו מילה שאנחנו גם משתמשים בה הרבה, ב-Dense Vectors, כאילו יש לנו את המימדיות המקורית, ואת המימדיות הדחוסה, שלפעמים נקראת אפילו לטנטית.

אז בעצם במישור הזה, לשם אנחנו רוצים להגיע, כאשר אנחנו רוצים לעשות איזשהו ייצוג כזה, שהמימד הדחוס המשוחזר נגיד, ייתן לנו את הטעות המינימלית. זה בעצם מה שאנחנו יכולים להגיד כאן, שהוא ייתן לנו בשחזור, את הרנק המינימלי שהיינו רוצים. אז הנה דוגמה שהראינו כבר בעצם, על קשר בין מילים.

בתור Input יש לנו את הדבר הזה, ואז את זה אנחנו מקווצים, וזה יעזור, זה סתם יש לנו איזשהו use case, שמשתמש באפליקציה של Topic Modeling, לזהירוי של נושאים שבמסמך, אז הוא השתמש ב-LSA הזה, זה feature מאוד חזק בשביל לזהות, את הנושאים שנמצאים במסמך. אז זה מה שאנחנו עושים, אז מה בעצם LSA הזה עושה? עוד רגע נראה דוגמה, הוא מקבל בתור Input, זה המטריצת C שהצגנו קודם, בתור Input הוא מקבל מטריצת מילים ומסמכים. זה ה-Input שלו, והפירוק שלו, שהוא מכפלה של שלושת אלו, זה יהיה המטריצות, המטריצה שמעניינת אותנו בעיקר זה המטריצה U, שכאן בעצם יש לנו את ה-term matrix, איך אנחנו מייצגים? הוקטורים שמייצגים כל אחד מהמילים.

אז זה בעצם, תוך כדי הפירוק אנחנו משיגים את התוצר הזה, וזה התוצר שבעצם מעניין אותנו. אז יש לנו פה איזושהי דוגמה עם תשעה מסמכים, משתמשים פה במודל ככה טיפש קצת, וזה ה-Input שלנו, כאן יש לנו כמה הם הופיעו במסמכים השונים בכל עמודה. כל עמודה זה מופיעים של המילים השונות, במסמכים השונים.

אוקיי, יש לנו פה את מקדי עמיתם של פירסון גם, אז זה מה שאנחנו עושים ב-SVD, ומה שאנחנו רואים פה עם הטילדה, זה בעצם אחרי הקיבוץ, זה המימדים המרכזיים שאנחנו משתמשים בהם לשחזור. אוקיי, אז זה ה-U שמעניין אותנו, כמו שאמרנו, זה לוקחים את המימדים הראשונים, את ה-K הראשונים שהם החזקים ביותר, ומה זה הראשונים? זה לפי, רגע בחזרה להכפלה הזאת, אז התוצר של SVD נותן לנו מכפלה בין שלוש וקטורים, הוקטור האמצעי כאן שמסומן על ידי Sigma, הוא הוקטור שנותן משקל לכל אחד מהוקטורים של U. אוקיי, בעצם הוקטורים של U זה ה-Eigenvectors, אם זה אומר לכם משהו, הוקטורים העצמיים, והמטריצה שיש לנו, שמסומנת ב-Sigma, זה בעצם מטריצה של השורש, של הערכים הנוכסוניים בעצם, זו מטריצה אלכסונית, כל שער הערכים הם 0 חוץ מאלכסון, ובאלכסון יש לנו את Eigenvalues. אוקיי, אז ככה אנחנו יכולים למיין ולקחת את הרקטורים החזקים ביותר.

נכיר? כן. עוד אחורה. אגב, תשימו לב, בחרנו כאילו את השתיים הראשונים אחרי מיון, וככה זה מציג לנו הערה קטנה והסישלית, זה כמות המימדים שיש לנו להציג את המילים כמובן, זה לא מימדיות המקורית, שזה לא יצא, רק רוצה להראות, כל שורה פה מייצגת מילה, והעמודות כאן זה ה-Vector Z לצורך העניין.

אוקיי, אז ראינו שרוב האינפורמציה נמצא בשתי המימדים הראשונים, אז השלטנו לשמוע רק את אותם. וזה המשקולות של המילים, כן, עכשיו תשאל. אוקיי.

מסמכים, כן, לידיד של מסמכים. לא, הנה, יש לנו גם שתיים כך, זה פשוט משפטים הרבה קצרים. זה מקדם איתם של פירסון.

אני לא חושב. אם כן, אז נדבר על זה עוד. אני מבין.

אז זה כאילו, אין עדיין, אין שבוע בבוחן, אוקיי? אני מבין. אז מה שלא ברור, תעצרי, ואני כן אגיד עד הסוף, קודם כל. זה עדיין לא אומר שבגלל ששאלת אז יהיה או לא יהיה בבוחן, זה לא פשוט.

אני צריך לחשוב על איך להכניס חלק מהדברים כנסתרות אולי, או בשום קוצם, שאתם צריכים לזכור את זה בעל פה, אבל כן, אתם יודעים איך להשתמש בזה. בסדר? בסדר, ואז תאפשר מה שתצטרכו לדעת בעל פה, אני אגיד לכם. אני לא מדבר על החומר.

מדבר על נוסחאות. מה? אני נוטה שלא. כאילו, אם הוא יהיה פתוח, אז אני צריך לעשות לכם משהו יותר קשה.

אה, בבחנים, כן. במבחן אני מדבר. דיברתי על המבחן, כן.

במבחנים זה פתוח. תשכירי לי את השאלה הזאת לפני ה... כאילו, עוד פעם. בסדר.

כן, כן, כן, כן. סבבה. אני לא יודע איך לשלב את זה.

במבחן כן, אבל בוחן של מודל, שם אני לא יודע איך עושים את זה במודל. אם אתם יודעים, כאילו, היה לכם אי פעם כזה בוחן במודל עם דף נוסחאות? גישה בצעות. לא, בסדר, גם אנחנו אפשרנו גישה.

בסדר. אוקיי, אז... בבקשה. שאלות? כן, בטח.

אז יש לנו... זה לא יהיה שבוע הבא בוודאות. צריך קצת לעשות פה קצת חומר לפני שנוכל לעשות את זה. זה יכול להיות שזה יהיה יותר שיעורים אשר בפעם הראשונה.

בואו נגיד שלפחות עוד שבועיים, אבל אני צריך לראות... לעבור שוב על החומרים, לראות מה הגיוני להכניס לנהלו. אגב, שאלה, האם כבר עשינו הפסקה מתודית כזאת, שמדברת על דברים מנהלים? מתי יש איזשהו יום שני שאין שיעור בגלל השלמות ליום חמישי? באיזה תאריך זה יוצא? אה, זה עוד חודש. יש שמן. 

בסדר. אוקיי. בסדר, אז... הנה... עכשיו, אם אנחנו עכשיו מציגים את המילים בצורה וקטורית כזאת, וחוזרים למימדיות המקורית, אנחנו רואים שנגיד human וuser מקבלים ציון מאוד גבוה, מי שזוכר במקדם המיטב של פירסון, זה בין מינוס אחד לאחד, ויש לנו בנושאים שונים, אם זה קרוב בערך מוחלט לאחד, אז זה גבוה.

אוקיי, כאן יש לנו שתי מילים שקשורים לhuman, אחד מהם מקבל ציון כמעט מקסימלי, 0.94, השני, שהוא גם בעיקרון קשור לhuman, מקבל מינוס 0.83, אז אפשר לבחון האם באמת זה ציון הגיוני או לא, אבל בכל מקרה רואים את האפקט שהיה לנו לעומת מה שהיה לנו קודם. אוקיי, אז זה בעצם התוצר של אותו LSA. תזכירו לי, התחלנו ב-1.25, אז יש לנו עד 0.25 ל-4, נכון? 0.25 ל-3, סליחה.

קצת, אני מוודא שאנחנו... הוא עושה Dense Vector. הוא של הסקור עצמו, אני אדק להשתמש ב... הוא בעצם, מה המטרה של ה... אוקיי, לא מספיק שהתעקבתי על זה. מה היה המטרה של הדבר הזה? המטרה הייתה בעצם... למה אין לי וידאו עכשיו? הוא מחליט על דעת עצמו מתי לחבוט, אני לא מבין.

טוב, נקווה שיהיה היום קלטה עם שמה. הוא בעצם, תוך כדי התהליך, אני רוצה לעשות זה ויזואלית, זה היה לנו זה לפני כמה שקפים אני חושב, אז הדבר הזה מהווה input, בסופו של דבר, אני לא רואה את זה כאן, אבל... הוא לומד את הקשרים האלו בין המילים ויוצר... יש לנו עכשיו וקסטור מקובץ הרבה יותר, שמייצג את המילים, במקום לייצג את המילים. מה? זה בעצם אפשר להגיד סוג של האמבדינג הראשון, אם תרצה.

אז שתי מילים קרובות, וקסטור שמייצג אותן, יהיה לצורך העניין מרחק קרוסינוס קטן. אוקיי? כאילו הייצוג הוקטורי שלהם, עכשיו נחפש את המרחק ביניהם, ייתן לנו תוצאה קצרה. כאילו יחס של יותר קרוב לאחד לצורך העניין.

מרחק קרוסינוס וככה. אז זה בעצם התוצאה המעניין כאן, שגם אם לא נמצא, גם אם נחפש מוטל, נוכל למצוא מוטל לצורך העניין. כי הוקטורים שמייצגים אותם, שאותם אינדקסנו, הם דומים.

כאילו זה כמובן צריך לאנדקס גם את הדברים האלה, במקום לאנדקס את המילים עצמם, אז אנחנו, לא אמרתי טכנית איך עושים את זה עדיין, אבל אנחנו צריכים לשמור את האינפורמציה הזאת בצורה הזאת, ואז ככה נוכל למצוא, להעלות מאוד את הריקור. זה שאלה טובה, ובסוף הנושא של World Embedding היום, זה לא סוף הנושא World Embedding, העיקר זה World Embedding Part 1, מה שעומדים היום, אז התשובה היא בעיקרון לא, ובשביל זה אני אצטרך לראות את מה שיהיה בהמשך. למה לא? כי אנחנו בעצם, ברגע שעשינו את התהליך הזה פעם אחת, אז יש לנו World Embedding סטטי לכל מילה, ייצוג וקטור, נתקלים במילה, לא משנה מה יש סביבו, למדנו את זה אמנם מהקונטקסט, אבל בשלב הראשון, גם אם שמעתם על Glow ועל World2Vec, זה מה שהם נתנו, תוצר סטטי, אין לנו חלוקה לסנסים, אבל World Embedding זה חדשים יותר, הם כן מתעסקים, מה שנקרא קונטקס של World Embedding, שם כן יש הפרודה לפי הקונטקסט.

אז בשלב הזה עוד לא, אפילו לא בסדר הבא. אוקיי, אז ה-LSA כדוגמה עוזר כאן להרבה אפליקציות, ביניהן יש לנו את ה-Information Retrieval. עכשיו נראה את מה שככה מלך בתחום הזה של IRNLP וטקסט להרבה שנים עד שהגיע 2019, ואז עשו מהפכה, אולי אפילו קצת לפני.

אז אולי המאמץ הכי מוכר זה World2Vec, בעצם הוא וגלוב זה שני כלים, אפשר אפילו להוריד את זה, ויש את זה לכל מיני שפות. מה הרעיון? זה אותו רעיון שהיה לנו קודם, אנחנו לוקחים איזושהי מילה בתור input, המילה עצמה מקודדת בתור one-code encoding, כלומר אם המילה הזאת מופיעה, יש לנו אחד בוקטור הענק הזה לפי האינדקס של המילה, אחרת יש לנו אפס. לכל שאר המילים האינדקס שלהם יהיה אפס, זה האינפוט.

ואז יש לנו רצף של מילים כאלה, אנחנו דוחסים את זה על ידי המודל שזה התפקיד שלו. אז עכשיו במקום לעשות פירוק SVD, אז משתמשים מאחורי הכליים בנורל נטווקס, וכן, כאילו, בהמשך אולי נרחיב יותר, כי אני יודע שלא 100% פה יש להם רקע למידת מכונה, אז כן חשוב להתייחס לזה קצת, אבל נוכל להגיד דבר כזה. בפונקציות שיש לנו כאן שאנחנו כבר נראה, יש לי מדלג כמה שקפים נוכל למצוא את זה, אז הרבה שקפים, אז לא נגיד את זה עכשיו.

יש דבר כזה, עוד לפני למידת מכונה בלי קשר, למדתם בקורסים באינפי מן הסתם, אלגוריתמי אופטימיזציה, למדתם טכנות דינאמי או אלגוריתמים חמדניים, אחד מהם למדתם? מה הרעיון? הרעיון הוא שאנחנו מנסים, יש לנו איזושהי בעיה שצריך למצוא בה מינימום או מקסימום, ואנחנו רוצים למצוא את אותו מינימום או מקסימום, אז אני, האמת שזה שווה לבזבז על זה ואני אחזור על זה גם בהמשך, אז נגיד שאם יש לנו פונקציה במישור נגיד, נגיד שהפונקציה נראית ככה, אוקיי, זה באמת ציור מזעזע מאוד אבל לא משנה, אוקיי, אנחנו נמצאים באזור הזה, זה מאוד מאוד ככה נקרופי ידיים מה שאני עושה, אנחנו נמצאים באזור הזה, אז אפשר להגיד שהנקודה כאן היא תהיה המקסימום המקומי, זה הגדרות ממש לא אינפיות, אזרקו עליי אבנים אם אני עכשיו הייתי הולך לקורסים שלי, אבל לצורך העניין, אם אנחנו מה שנקרא קרובים בנקודה, אז בנקודה הזאת, נקודת המקסימום, אין לנו אף נקודה שהערך של הפונקציה לא יותר גבוה מאותה נקודה, אז זה נחשב מקסימום מקומי, איך אנחנו מחשבים את הדבר הזה? אם אתם זוכרים, במקסימום מקומי יש לנו נגזרת ראשונה ששווה לאפס, נגזרת שנייה שהיא שלילית, זה חייב להתקיים, זה חייב להיות גזיר פעמיים, ונגזרת ראשונה היא שווה לאפס, נגזרת שנייה שלילית, אותו דבר אבל הפוך בנקודת מינימום מקומי, אז מינימום מקומי זה אומר שבקרבת הנקודה מה שנקרא, זאת תהיה הנקודה עם הערך הנמוך ביותר של הפונקציה, ובשביל זה צריך נגזרת ראשונה שהיא אפס, נגזרת שנייה שהיא חיובית, אז אפשר לתת הרצאות שלמות על המשמעות של הדבר הזה, משהו לשים לב, שמה שהדבר הזה מוצא זה מינימום מקומי, לא בהכרח הערך האופטימלי, אז אם אנחנו מדברים על אגוריתם שהמטרה שלו למצוא איזשהו ערך מינימלי או מקסימלי, אז זה לא נותן לנו פה פתרון, כי נגיד הערך הזה כאן, נגיד שהוא המינימום הגלובלי של הפונקציה, ונגיד שהערך, גם אם אולי זה הולך כאן למטה, אז נגיד שזה הערך כאן זה הנקודה שזה הערך של המקסימום הגלובלי של הפונקציה, אז יש לנו מקרה מיוחד, שגם בו מתעסקים כשלומדים למידת נכונה, על פונקציות שנראות כמו פרבולה, זה מוגדר, זה נקרא פונקציית קונבקס, תרגום שהוא מאוד מוזר בשבילי, פונקציה כמורה, קראתי הרבה פעמים את ההסברים למה קוראים לזה פונקציה כמורה ולא כאורה, לא השתכנעתי באף אחד מהם, זה נראה כמו קארה, לא כמו ההפך נקרא, לא יודע, בכל זאת קוראים לזה פונקציה כמורה, יש לזה הגדרות פורמליות לצורך העניין, אם אנחנו בוחרים על שתי נקודות על הגרף כלשהן, אז חוץ מאותן שתי נקודות, במיתר שמחבר ביניהן כל הנקודות נמצאות מעל הגרף, אם זה מתקיים אז זה פונקציה כמורה, אז בפונקציה כמורה המינימום הוא מינימום גלובלי, זה מה שמעניין אותנו, ויש לפעמים פונקציות כאלה שאנחנו מחפשים אותן, מה הקשר בין כל זה לענייננו? אז הבנו מינימום מקסימום, חזרה כזה קטנה על אינפי, מה שמעניין שיש טכניקה שאני לא אכנס אליה, לצורך העניין טכניקה שבתנאים מסוימים היא עובדת, בלי למצוא את הנגזרת השנייה, זו גם נגזרת רב מימדית, זה לא נגזרת של fx tag, אנחנו מניחים שכל פיצ'ר לצורך העניין זה מימד אחר, רוצים נגזרת רב מימדית של הדבר הזה, שהיא תהיה את המינימום טעות על ה-training set, זה מה שאנחנו מנסים לעשות, זה הדבר שאנחנו רוצים לעשות לו מינימיזציה, למה? כי אנחנו רוצים לחזות משהו, ואותה הבעיה שאנחנו רוצים לחזות, אז אנחנו רוצים שאותו פונקציה שאנחנו עושים לחיזוי, תהיה כמה שקרובה לערכים האמיתיים, אז כאן אנחנו רוצים לעשות איזה מינימיזציה של טעות, על הדוגמאות שנתונות לנו ב-training set, ויש כל מיני בעיות מינימיזציה שרוצים לעשות שם, אז לצורך העניין זה משהו שעוד נדבר עליו, עוד נחזור על זה, אז מי שאין לו את הרקע אז אולי פחות מכיר, אבל בכל מקרה אנחנו רואים כל פעם דוגמאות ששונות מה-training set, ואם מה שאנחנו מנסים ללמוד הוא לא בדיוק מה שיצא לנו, אז אנחנו עושים תיקון קטן באותה פונקציה שאנחנו רוצים ללמוד, אז אם תחשבו נגיד, נגיד שPCA היה נלמד בצורה הזאת, אז היינו אומרים יש לנו וקטורים של W, נגיד שיש לנו את אותו Z1 שאנחנו לומדים, והמימדיות המרכזית, נגיד הראשונה זה היה שלושה פיצ'רים מקוריים שיהיו לנו, X1, X2, X3, ואנחנו יודעים מה הערך המצופה לאותם פיצ'רים לכל הדוגמאות שיש להם, כאילו הערך המקווץ המצופה זה Z1, אז נגיד שכך היינו רוצים ללמוד את הוקטור הזה, יש לו שלוש משקולות, W1, W2, W3, אז כל פעם אנחנו רואים שיוצא לנו Z1 שהוא לא ה-Z1 האמיתי, נגיד שיש דבר כזה, אז היינו מתקנים טיפה את הערכים לפי הנגזרת, השיטה הזאת בגדול זה השיטה הקלאסית שהיא הבסיס עד היום לא הצליחו למצוא לה תחליף, לבעצם כל ה-Deep Learning, זו שיטה שנקראת Gradient Descent, מי שלמד למצוא נכונה מכיר את זה, אז כאן עושים כל הזמן תיקונים למה שאנחנו מנסים לעשות לו אופטימיזציה, כדי שזה יהיה כמה שיותר קרוב לערכים המצופים, במקרה הזה זה אותה משקולות כדי ליצור את Z1 נגיד, רוצים לעשות אופטימיזציה לערך של Z1, אותו Vector Dense לצורך העניין, ואת זה אנחנו עושים על ידי ה-Training Set, אז בהנחות מסוימות Gradient Descent מוצא לנו את המינימום המקומי, בכל מיני הנחות שמתקיימות, אז הוא באמת, למרות שאנחנו לא מוצאים את הנגזרת השנייה, הוא מוצא את המינימום המקומי, אז מה הקשר לעניינינו? אז אנחנו מפרמילים את הבעיה בצורה כזאת, יש לנו, כל הדבר הזה היה הקדמה ל-Root-to-Vec, כי Root-to-Vec בעצם עושה את הדבר, את שתי ה-Use Cases שלו, בעזרת Gradient Descent, אנחנו נחזור עוד על Gradient Descent ולא צריך בדיוק להבין מה הוא עושה, כן אולי להשתמש בתוצר, אבל לא תצטרכו לממש Root-to-Vec, אז בעצם זה מה שהוא עושה, הוא רוצה לעשות אופטימיזציה, למשהו שנשמע הרבה יותר מסובך מהדוגמה הפשוטה שנתתי עכשיו, אז מה הוא מנסה לעשות? אוקיי, אז בעצם יש לנו שתי גרסאות ל-Root-to-Vec, אמרנו שמילה מיוצגת על ידי הקונטקסט שלה, אוקיי, אז תחשבו שכל פעם אנחנו לוקחים רצף נגיד של חמישה מילים, שתי מילים לפני המילה, את המילה עצמה ועוד שני מילים אחר כך, אוקיי, אז הנה דוגמאות, כאן יש לנו סביבה עם מילה אחת לפני ואחרי, לא, כאן סליחה, כאן יש שתיים אחרי, אוקיי, זה פשוט מילה שהופיעה בהתחלה, אז אין לה מילה לפני, כל המילים שבה לחסון זה המילים המרכזיות, בצבע צהוב, אוקיי, ואז יש לנו שתי מילים לפני, פשוט בגלל שזו המילה הראשונה אין מילים לפני, ושתי מילים אחרי, אוקיי, אז בהתחלה, India זה המילה שאנחנו מחפשים את הקונטקסט שלה, אחרי זה המילה is, now, inching וכן הלאה, אוקיי, זה בעצם המילים המרכזיות שלנו, ואז יש לנו שתי מילים לפני, שתי מילים אחרי, אם המשפט לא נקדח, אוקיי, אז זה בעצם הקונטקסט של המילות, אוקיי, והבעיות שאנחנו מנסים לפתור הן שתיים, הבעיה הראשונה, אנחנו רוצים בעצם לחזות את הקונטקסט בעזרת המילה, זה משהו מאוד גדול, תחשבו שיש לנו ווקאבלרים, אפילו לא כזה מייצג של עשרת אלפים מילים נגיד, אוקיי, אז תחשבו כמה מילים יכולים להגיע עם אותה מילה שעכשיו נמצאת באמצע הקונטקסט, אז אנחנו לצורך העניין, אם אנחנו רואים את המילה inching, אנחנו צריכים לחזות שבקונטקסט שלה יש את is, now, towards, a, לצורך העניין, זה מה שאנחנו צריכים לחזות, אוקיי, אז זה בעצם הבעיה שלנו, אנחנו רוצים שההסתברות שלהם תהיה כמה שיותר גבוהה אם הן בקונטקסט, וכמה שתהיה יותר נמוכה אם היא לא בקונטקסט, אוקיי, אז אפילו יותר קשה מלחזות את המילים, אנחנו רוצים ממש להפריד ביניהם, ככה שההסתברות תהיה גבוהה למילים הרצויות ונמוכה למילים הלא רצויות, זה כבר, תחשבו שאם זה בעיית סיבוג אז יש לנו אולי עשרות אלפי קטגוריות, שקטגוריה זה מילים השונות שאנחנו חוזרים, ורוצים לעשות אופטימיזציה של הדבר הזה, אז זה כיוון אחד, זה אלגוריתם שנקרא Skidgare, זה הגרסה הראשונה של Word2Vec, הגרסה השנייה עושה בדיוק את ההפך, יש לנו קונטקסט, לצורך העניין במקרה הזה יש לנו שתי מילים לפני שתי מילים אחרי, ורוצים בעזרתו לחזות את המילה האמצעית, את המרכז של הקונטקסט, אז הדבר הזה כאן בעצם רוצה שהמילה הנכונה, תהיה לה הסתברות יותר גבוהה מהמילים הלא נכונות, הרבה יותר גבוהה, להפריד את הדבר הזה כמה שיותר, אז זה ברמה עקרונית בלי להסביר מה עושים בדיוק, עושים לדבר הזה אופטימיזציה, כדי שכמה שיותר נפריד, אני לא זוכר אם יש פה גם את הנוסחה של Word2Vec של אופטימיזציה, אבל זה בעצם שתי האלגוריתמים של Word2Vec הבסיסיים, ואחד רוצים לחזות את המילה עצמה בקונטקסט, והשנייה רוצים לחזות את הקונטקסט עצמו, זה כבר ממש מתקרב, אפשר להגיד, זה קרוב למודלים גנרטיביים שנדבר עליהם גם קצת, כמו ה-AI שאנחנו רגילים להשתמש, מה הם עושים בעצם, אלה להם שאנחנו משתמשים בהם, הבסיס שלהם זה לחזות מה תהיה המילה הבאה, Next Token Prediction, אז אם אנחנו חוזים את המילה האמצעית זה כבר מתקרב לדבר הזה, אז ה-input שלנו כאן זה one hot encoded vectors של ארבעת המילים האלו, ואנחנו רוצים ללמוד את ה... בעצם לפני שאנחנו עושים את הדבר הזה, יש לנו תהליך שנעשה מעל וקטורים ומטריצות, שמוכר אולי במי שמתעסק נגיד בעיבוד תמונה, אבל לא רק, זה גם בטקסטים, במונח pooling, עושים, מקבלים לפעמים כמה וקטורים או כמה מטריצות, גם וקטור אפשר להגיד שהוא סוג של מטריצה, ואנחנו עושים איזושהי פעולה נגיד סקלרית עליה, זאת אומרת למשל מעמידים אותה one hot encoded זה אלו, את הוקטורים האלו שכולם באותה מימדיות, פשוט כל אחד מהם... בדיוק אחד במקום אחר, ועכשיו מה שאנחנו עושים זה באיזשהו דרך וקטור מאוחד על כולה, זה התהליך הראשון שיש לנו כאן, איזושהי מניפולציה לקחת את הדבר הזה ולייצר וקטור אחד, מהאינפוט הזה, מה הרעיון? שכל הקונטקסט מיוצג על ידי וקטור אחד ולא על ידי ארבע וקטורים, אז התהליך הזה, אגב פונקציות של פולינג פשוטות שיש לנו זה למשל, לקחת את הממוצע או את המקסימום או בכל אחד מימדים, או כל מיני... או את הסכום של כולה, אוקיי, זה ככה מטריקות מקובלות של פולינג, אז אחרי הדבר הזה אנחנו בעצם מפעילים שכבת אקטיבציה של Softmax, שמקבלת כאינפוט וקטור בעצם, ומחזירת ההסתברות לכל אחד מהקטגוריות, אוקיי, אנחנו משתמשים מאוד בגדול, אחת הבעיות הקלאסיות של מידת מכונה, זה מה שנקרא Multi-Class Classification Problems, יש לנו רצון לחזות איזה קטגוריה מתאימה לאיזשהו אובייקט מסוים, מתוך תמות של יותר משתיים למולטי-קלאס, יותר משתי קלאסים, יותר משתי קטגוריות, למשל רוצים לדעת אם מחר יהיה יום שימשי, אם אולן חלקית או גשום, יש לנו כאן שלוש או ארבע קטגוריות שאפשר לחשוב עליהן, אז אנחנו רוצים שאחת מהן תצא עם הסתברות גבוהה יותר מהאחרים, ובשביל זה משתמשים בSoftmax. אוקיי, אז הדבר הזה, אנחנו עדיין באלגוריתם הראשון של הסיבור, והצד השני אמרנו, זה אלגוריתם שמנסה לעשות אותו דבר אבל הפוך, הוא לוקח מילה אחת ומנסה לחזות את כל הקונטקסט שלה. אוקיי, אז גם פה אנחנו בעצם עושים מניפולציה של משפולות שאנחנו לומדים על המילה הזאת, ואנחנו מפה יוצרים איזשהו וקטור שמפעילים עליו Softmax, ככה מאוד בגדול, נותן לנו עבור כל אחד מהמילים עכשיו, זה Output שונה מהOutput שהיה לנו כאן, כאן יש לנו Output שצריך לקבוע בסוף וקטור אחד, אוקיי, אנחנו רוצים לעשות one-hot encoding שבמילה המתאימה תהיה לנו את ההסתברות הכי גבוהה.

מה אנחנו רוצים לעשות כאן? כאן אנחנו רוצים לתת כמה Outputים, אז בלמידת נכונה לצורך העניין, אנחנו רוצים, נגיד שעכשיו היינו עושים רעיון עבודה, אם אתם מכירים, היום יש כל מיני עיסיונות, לדעתי אין משהו ממש מוצלח עדיין, עושים רעיון מול מצלמה, שמי שמרעיין בעצם, זה יכול להיות מרעיין, אבל בסוף ה-AI מודד כל מיני דברים. נגיד שמה שהיום רוצים לחזות לגבי כל אחד מכם, זה איזה שפות תכנות הוא יודע. אוקיי, אז יש לנו בעיה שלמידת נכונה, אנחנו נקרא לזה Multi-Label, יכול להיות שאתם יודעים גם Python וגם Java וגם C++, אוקיי, אז לכל אחד מאלה יכול להיות תשובה חיובית, זה בעצם מה שיש לנו כאן ב-Output.

אוקיי, יש לנו ארבע קלאסים שאנחנו רוצים עם הקונטקסט וינדוס שלנו בגודל הזה. אוקיי, ולא כמו קודם, שרצינו רק מילה אחת. אוקיי, זה ההבדל בין מצבים השונים.

ותוך כדי הדבר הזה, בעצם אנחנו לומדים בשיטה שהוכיחה את עצמה, אני לא אסביר את הרקע המתמטי למה, אבל הוכיחה את עצמה, בעצם יש לנו תוצר לוואי של הדבר הזה. כאילו הבעיה התחילה מחיזוי הקונטקסט או מחיזוי המילה באמצע. אבל תוך כדי למדנו ייצוג שהוא מאוד יעיל למילים בצורה dense, בצורה מקובצת, תחוסה.

אוקיי, זה התוצר הכי מעניין שזה נתן לנו, כן, מתוך המאמץ המקורי של משימה של מורכבת, של לחזות את הקונטקסט ולחזות את המילה עצמה, למדנו את הייצוג הזה, ובעצם זה התוצר הכי מעניין שיש לנו כאן, יותר מאשר הניסיונות חיזוי עצמה, וככה בסוגריים. והנה כבר מה שאנחנו מנסים לעשות לאופטימיזציה באלגוריתמים השונים, מראים לנו את זה כאן, אז בעצם, שוב, נחזור ונדבר על זה עוד פעם, אבל מה שאנחנו עושים בלמידת מכונה כשאנחנו מנסים לעשות אופטימיזציה לטרנינג סט, זה לצורך העניין תחשבו שאחרי התואר אתם רוצים לטוס עם חברים או עם משפחה לחול, ואתם חוסכים קצת כסף, ועכשיו יש לכם כל מיני פרמטרים, ואתם רוצים בסוף, הטיול שההוצאות עליו הוא הכי נמוך, רוצים לעשות מינימיזציה של המחיר, אז המחיר אצלנו בלמידת מכונה, אם ניקח את הדבר הזה, הוא בדרך כלל איזשהו מרחק מאיזושהו ציפייה מסוימת, כמה אנחנו רחוקים, או איזה טעות יש לנו על הטרנינג סט או משהו כזה, ולזה רוצים לעשות מינימיזציה, זה פונקציה J שאנחנו רואים פה, על תטא שזה משקולות שאנחנו לומדים, זה המחיר, ככה מסמנים פונקציית מחיר, והדבר הזה כאן שאני לא אכנס אליו כרגע, הוא פשוט פונקציית המחיר, עבור הצד הזה של חיזוי הקונטקסט שקראנו לו SkipGraph, אגב קודם היה לנו את Sibo, היה כדאי להגיד שSibo זה Continuous Bug Averse, קודם היה לנו Bug Averse, עכשיו זה Continuous Bug Averse, יש לנו רצפי מילים שאנחנו לוקחים מחשבון, ולא רק מילה שהיא תגושה לגמרי מהקונטקסט, ה-5 גרם הזה לצורך העניין היה רצף של מילים, בגלל זה זה נקרא Sibo, והיו כל מיני תוצרים מגניבים בפעם הראשונה שזה התפרסם, שהם עוד התלהבו מהם, והתוצר כאן הוא לא ברור כמה הוא מעניין, אבל בכל מקרה מה שכמובן כן מעניין זה הקרבה של כל מיני ביטויים שהם קרובים אחד לשני באמת, אם נגיד שאנחנו לא מבינים בתוצרים האלה שיש כאן, אז נניח שאנחנו מבינים בדברים שקשורים אולי למדעי המחשב, אז יש לנו קומפיוטר סופטוויר, קומפיוטר מיקרו, פרינטר, מיין פריים ו... הדבר הזה... כן, כן, אני מבין את השאלה. אני... לא, נראה לי שזה... בוא נראה אם זה כתוב לנו כאן.

אני דווקא חשבתי אחד לשני, אבל יכול להיות, כן. יכול להיות שזה בין קומפיוטר וסופטוויר, יכול להיות. יכול להיות שזה גם הדוגים.

לא יודע, בסדר. אוקיי, יש לנו השפעה כמובן לקונטקסט סייז, אוקיי, ולגודל המימדיות שלנו, אנחנו רואים שכאן הוקטור המקווץ הוא בגודל שלוש נוט, וכאן הקונטקסט לנט, איזה מרחק מילים בעצם חשבנו, חשבנו שלושים מילים, אז יש כל מיני בעצם תוצאות שונות שיצאו, כאן צריך לעשות לזה הרבה משחקים, אבל לא ניכנס לזה כרגע. אז כאן יש לנו מרחק למילה king, כן, אז הדוגמאות שאנחנו רואים מהסימן של הקודם, זה ביחס למילה king, מלך, אוקיי, אז רואים את הדמיון לכווין, או מונר, כן, משטר מלוכני, מונרכי, או פרינסס, וכן הלאה, ואחד מהניסויים המעניינים שהיה, זה ממש עשו יחסים כאלו של חיסור וחיבור וקטורים, כמובן שהדוגמאות המעניינות הציגו במאמר, וכולל של עובדים אז לא מציגים, אז לוקחים king, מורידים ממנו גבר, מהם כאילו, ומוסיפים אישה ויוצא כווין בערך, כאילו יוצא משהו מאוד קרוב לכווין, שזה היה מגניב, אז כאילו זה בעצם היה אחד מהחידושים הגדולים אגב של וורטובק, זה הרלציות המורכבות האלו, שהיה אפשר ממש כאילו לחסר כל מיני משמעויות מיליוליות, ולהגיע לרלציות חדשות, ובוא נגיד זה עבד מספיק טוב כדי שאפשר להתייחס לזה, אבל לא מספיק טוב שאפשר לסמוך על זה כמובן כשגר ושכח ככה, אז זה בוודאי שלא.

אוקיי, אז אחת מה... ככה טיפה על החסרונות של הדבר הזה, אז זה נראה ממש טוב, זה באמת עבד בהרבה דברים הרבה יותר טוב מהלסה, מה בכל זאת היה הדרובאקט שלנו, מה היו בכל זאת החסרונות, אז יש לנו פה דוגמה טובה, לאיפה זה עובד ממש לא טוב, אוקיי, אז יש לנו משפט אחד שאומר It was not good, it was actually quite bad. ומה זאת משפט שני? It was not bad, it was actually quite good. כמובן שזה ההפך אחד מהשני, אבל אם מנוחים את אותו קונטקסט, אז לא אכפת לנו המיקום בקונטקסט, זה זהה אחד לשני, אתה יקבל בדיוק את אותו וקטור.

אוקיי, אז כמובן שכאן יש לנו איזושהי בעיה, זה דוגמה רק לבעיה אחת, הסדר של המילים לא מקבל פה שום משמעות, עדיין יש לנו פה כל מיני בעיות, ואחת הבעיות שיש לנו כאן, שבעצם השאלה שנשאלה פה קודם, שאין וקטורים שונים שהם תלויי הקשר, זה לא דינמי, כאילו עכשיו יש לנו וקטור שחישבנו פעם אחת ושמרו בזיכרון, מעכשיו נגיד את המילה בנק, או עוד כל מיני מילים שאפשר לחשוב עליהם שהם הרבה משמעות וזה תלוי הקשר, יהיה לנו את אותו וקטור בדיוק שמציג אותו. אין לנו שום דבר שלוקח את הקונטקסט הנוכחי ולוקח אותו בחשבתי. אוקיי, אז בעיה אחת שבעצם אומרת שאם אנחנו משתמשים בזה לצורך IR, אז אנחנו מצד אחד מגדילים את ה-recall, אבל יכולים מאוד להוריד את ה-precision.

אם אנחנו משתמשים בזה לצורך query expansion, אנחנו מוסיפים גם הרבה מילים שהם לא באמת קשורים ל-query שלקחנו כאן. אוקיי, זה בעצם חשש שאנחנו עושים פה קצת topic shifting, עוד רגע יש לנו פה את הדוגמה הזאת. למשל אנחנו מחפשים במנוח היפוס שלנו מלונות באוסטריה, הוטלזן אוסטריה, והוא אומר אוסטריה אחרי ההיפוס שלי, אז ראיתי שגרמניה מאוד קרובה לאוסטריה מבחינת משמעות.

אוקיי, למה? כי הם הופיעו בווקטורים כאלו אחת ליד השנייה אז, אז הוא הביא לנו תוצאות מהוטלזן ג'רמני ב-query expansion. אוקיי, זה מה שקרה בפועל. אז יש איזושהי טכניקה שעושה כזה לצורך העניין tuning, retrofitting זה נקרא, שהיא לוקחת את הוקטור והיא קצת עושה לו pruning כזה נגיד, היא קוצצת חלק מההקשרים שהוא לומד, מזיזה אותו כך שהרחבה שלו לא תהיה כל כך גסה כמו בדוגמה שראינו עם גרמניה ואוסטריה.

אוקיי, אז הנה דוגמה של הtopic shifting על lsa, lsi, אותו דבר, שנלמד על ויקיפדיה לעומת skipgram שנלמד על ויקיפדיה. אוקיי, אז מה קורה כשאנחנו מקבלים בתור input את אוסטריה, הזמן שהskipgram למד זה אוסטריאן, גם הlsi למד אוסטריה, אוקיי, זה הגיוני, זה צמיחות פשוט, אבל הוא גם מביא הונגריה וג'רמני, ווינה שזה דרך כתוב, ווינה, כן, זה עיר בירה של אוסטריה, אבל הונגריה וגרמניה הם לא משהו שהיינו רוצים. אוקיי, בדוגמה שחיפשנו מלונות באוסטריה, אנחנו לא רוצים שהוא ייתן, זה הגיוני, שהוא ייתן מלונות בווינה, אבל לא בהונגריה ובגרמניה, זה לא טוב, אוקיי, אז לדבר הזה צריך לעשות איזה תיקון, כמו שאמרנו קודם לאמבדינג, בשביל לקבל את הדבר הזה, בעצם לקצץ את ה... הרחבות הלא רלוונטיות, כאן יש לנו עוד דוגמה עם Austrian, כן, שזה מה שקרה כאן.

אוקיי, אז כאן אנחנו כן רוצים גם דברים שנלמדים ב-LSI וגם דברים שנלמדים בסקידה. עכשיו, עוד איזושהי אפילו הרחבה של הבעייתיות שיש בה Expansion עם הכלים האלו זה שהרבה פעמים עושים אפילו את התהליך הזה כמשוב פנימי, שבעצם עושים כמה איתרציות של ה-Query Expansion הזה. התוצאות שחזרו אחרי זה מה-Query Expansion מתשווים רלוונטיות, אז עושים עוד פעם-Query Expansion ומרחיבים את זה עוד יותר.

או בואו נראה מה ה-World Embedding של ה-World Embedding. כאילו אם חזר גרמניה כמילה שדומה לאוסטריה, עכשיו בואו נראה מה קרוב לגרמניה לצורך העניין. ואז מקבלים עוד יותר רעש, אז הדבר הזה אמנם הרע פוטנציאל, הוא הגדיל לנו את ה-Recall בצורה יפה, אבל החשש של ה-Precision הוא ברור.

טוב, אז נראה לי זה זמן טוב לעצור עכשיו. כאילו בדיוק סיימנו את ה-World Embedding Part 1, ואחרי הפסקה אנחנו נדבר על-Question and Answer. היו שאלות? לא? אני ככה ברצף נראה? מה? אז לא? אז תעצרי אותי.

אוקיי, אז אני אתן, אני מצטער שאני מעכב אחרי שפסקתי עוד חצי דקה ככה. סתם להכניס לקונטקסט. אז דיברנו על זה שהיה לנו כמה מוטיבציות לייצר את הוקטורים האלה של המילים, השתיים המרכזיות הם בשביל להקטין את המימדיות, ובשביל למצוא במקום את המילה עצמה עם נורמליזציות אבל עדיין את המילה עצמה במסמכים שלנו, אז למצוא מילים דומות למילה הזאת.

אוקיי? אז זה היה המוטיבציה לייצר איזשהו מישור דחוס של המילים, שבגלל התחיסה הוא מחריף למצוא הקשרים בין המילים. אוקיי? ויש פשוט כל מיני אלגוריתמים של איפה לעשות את זה. אחד היה LSA, הוא דמה על הפירוק הסביבי אבל האינפוט היה וקטורים של המילים והמסמכים, והשנייה זה היה כבר שיטה שמשתמשת בנקרא לזה רשתות נוירונים לצורך העניין, באיזה שיטה של למידת מכונה כדי לחזות את הקונטקסט על ידי המילה וההפר, לחזות את המילה על ידי הקונטקסט.

אוקיי? זה מה שדיברנו עכשיו, קירור תובק. אמרנו שזה הצליח להרחיב יפה אבל לפעמים הוא מאוד מכניס הרבה רעש, דברים שלא קשורים. אז פה הצער.

כאילו בקונטקסט זה מסתדר. זה היה חלף בפתיח. פעילת הסמונטים.

בפעילת הסמונטים. בפעילת הסמונטים. כדאי.

כדאי. כדאי. כדאי.

כדאי. כדאי. כדאי.

כדאי. . . . . . . . . . אוקיי! אנחנו חוזרים. אם יש מישהו שמסכם, אז עכשיו אנחנו מתחילים חלק בית.

חלק ראשון בעצם, לקחנו את הנושא שהיה בסוף שיעור שעבר, והבנו שיש פה עוד פרספקטיבות נוספות להסתכל עליו, בעצם כווקטור, הדחוס החדש שלנו שמציג לנו את המרחב החדש ש... בעזרתו אנחנו מייצגים את העולם השפתי שלנו, את המילים. אוקיי? ראינו ניסיון עם LSA ועם הור טובק בכמה כרסאות, וזה היה הסוג של הרחבה הראשונה שראינו למערכת הקלאסית של איחזור מידע, אז עכשיו אנחנו מדברים על עוד שתי רחבות שבעצם נושקות בעולם של איחזור מידע, לפעמים שמים אותן, מגדירים אותן כי איחזור מידע, ולפעמים לא. הנושא הראשון זה יהיה question answering, פעלות ותשובות, שבעצם אני חושב שבטח היום אנחנו מתקשרים בצורה הזאת, אבל זה משהו שחשבו עליו גם די בהתחלה, שבעצם אנחנו רוצים לתקשר כמו שמתקשרים אנשים, לא לכתוב כל מיני מילות מפתח ולקבל תשובה, אלא לשאול שאלות כמו שאנחנו שואלים אנשים, אז כאן יש לנו כאן כמה דוגמאות, אחת מסירי, שזה היה במקרה הזה האוזרת הדיגיטלית הראשונה שהייתה של אפל, ומי שמכיר עוד מוצר ותיק מהתחום של נגיד Data Science, Wolfram Alpha, אוקיי, כאן הלקוחות זה חברות בדרך כלל, כפה דווקא אולי, אז צירי שואלים, Do I need an umbrella tomorrow in San Francisco? אוקיי, זה היום היינו קוראים למה שכבר מזמן עושים בטלפונים, אפליקציות של AI מסוג Agentic AI, כפי שמכיר, אוקיי, מחברים את זה לאפליקציות, כאן זה אפליקציות ב-iPhone במקרה הזה, ואולי לא כל אפליקציה סירי מקבלת רשות ב-iPhone נתחבר, היום יש את האוזרים הדיגיטליים שהתחילו מ-OK Google ועבר לג'ימיני, ועוד כל מיני אחרים בגוגל, ויש את אלקסה ויצאו הרבה גרסאות כאלו, סירי הייתה הראשונה, אז כאן יש אפליקציה של מזג האוויר, אמורים לעשות עיבוד שפה טבעית להבין גם באיזה אפליקציה מדובר, ולתת תוצר רלוונטי, כן, התוצר פה הוא לא מסמכים בכלל, אוקיי, זו תשובה לשאלה ששאלנו, אז עד כדי כך אנחנו לא נגיע עכשיו, את האמצים הראשונים ביחס לדברים שאולי כבר התרגלתם היום יראו לנו קצת פרימיטיביים, אבל זה עוד שלבים מחכים בדרך בשביל להגיע לדברים שאנחנו מכירים היום, אוקיי, וזה יכול להיות שאלה אינפורמטיבית שהיא לא בהכרח מחוברת אולי לאפליקציה כאן, כמו במזג האוויר, שאז אפשר להיכנס גם לחיזוי מזג האוויר לא דרך השאלות של סירי, זה נגיד כמה קלוריות יש בעוגת קרם בננות, בננה קרימפאי, אוקיי, כמה קלוריות יש, ואז התשובה בסוף זה 702, שתי סלייסס, שתי, כן, פתאום.

איפה מתייבת לבד, נגיד להם, נגיד להם פרופאו, אתה כאילו מקדים את הנרוחה לגמרי, כאילו, נגיד פרופאו ואף פרקים, כאילו, אופ, אבל, אין צורך, באמת, נכנסים לישירות למה שאתה אומר, זה חלק מהטכניקה לדבר בזה, כן, הם כאילו מורידים, כן, כאילו, עשית את כל השאלות הממחות, תשאיר עוד משהו להגיד, אה, אוקיי, אפשר לחלק את השאלות, אנחנו נתמקד עכשיו בשאלות הקלות יותר, שאלות של עובדתיות, שאנחנו רוצים תשובה שהיא עובדה, אוקיי, זה בעצם אפשר להגיד קצת יותר קל, נגיד, מי כתב את ה... אין סיכוי שזה ארבע, זה אמור להיות דרגון, אני, בסדר, לכאן זה, לא היית צריך לראות במקומה, ארבע מופיע, אז יש פה איזה שיבוש, אז, בכל מקרה, מי כתב או כמה קלוריות או מהו ממוצע הגילאים של משהו או שאלות של איפה, מה שנקרא באנגלית, אם אני מניח שכולכם עשיתם בגרות באנגלית בתיכון, אז זה, WH-Question זה צורך העניין, כן, זה, what, who, גם how נפנס לזה איכשהו, אולי גם yes, no question כמו is, are וכאלה, זה שאלות עובדתיות, מהי עיר הבירה של, למשל, כן, שאלות מהסוג הזה, ויש חלק שני שהוא מורכב יותר, ששם צריך לעשות יותר סינתזה של אינפורמציה, של כל מיני מקורות בשביל להגיע לתשובה, ואנחנו לא מצטטים שהתשובה תהיה ישירה באיזשהו מקום, אוקיי, אז זה ההבדל בין סוגי השאלות, אנחנו נטפל יותר בסוג הראשון של השאלה, אוקיי, השאלה שבעצם אנחנו אמורים לבנות עבורה איזשהו knowledge base, אוקיי, זה משהו שנשמור להמשך, ואנחנו מדברים כרגע על משהו שיותר מתבסס על מקורות שיש לנו ב-IR, אוקיי, אז נאמץ רב שנים, זה data set שבנו בדיוק בשביל QA שנקראים track, וכאן יש לנו עוד כל מיני מערכות שעשו את זה בעצמם, אבל אם נגיד היינו שואלים במנוע החיפוש, איפה הוא, מי פה צרפתי ממש עם מבטא כזה, מוזיאון הלובך, משהו להגיד זה במבטא הנכון, הלובך, זה לא לובר, כמו שאומרים, זה לובך, כן, אז איפה נמצא מוזיאון הלובך? אז יש פה תשובה כאן, אוקיי, אז מוזיא דלוב, ואז פתאום עוברים לאנגלית, זר, is located in Paris, אוקיי, location within Paris, בלה בלה בלה בלה, אוקיי, אז כאילו, מה אנחנו בעצם עשינו, מה הקשר בין השאלה לתשובה? יש פה כמה דברים שצריך לשים לב אליהם, אני לא בטוח שזה כתוב כאן, אבל בעצם, אם קודם קיבלנו איזשהו מסמך בתור תשובה, אז עכשיו יש לנו את מה שהמנועי חיפוש נותנים לנו מעבר למסמך עצמו, שכולכם מכירים, אני לא יודע אם אתם מכירים את השם, זה הסניפט, אוקיי, גם הסניפט עצמו זה אלגוריתם שלם של איך נמצוא את המקטע הרלוונטי בתור הטקסט שאנחנו מופנים אליו, אוקיי, זה מה שמוצג תמיד במנועי חיפוש, לכל המנועי חיפוש הגדולים יש את זה, זה הדבר הזה, ואפילו לפעמים, כמו פה, יש לנו הדגשה של מילים שרלוונטיים לשאילתם, אוקיי, דבר שני, שאלו where is, שאלה כזאת של איפה, היה פה איזשהו מיפוי שנעשה, מ-where ל-location, ולכן אנחנו גם רואים שבתשובה כאן יש לנו את המילה located, אפילו, אוקיי, וגם לצורך זה צריך להבין, אז שאלו יש גם ב-where הולכים, ואנחנו מביאים את זה פה. נכון, יש אלגוריתם, אנחנו נראה את האלגוריתם, יש flow ממש של הדבר הזה, כמובן צריך לדעת מאוד גם שפריס זה שם של מקום, אוקיי, אז גם את המידע הזה צריך, יש משימות שלמות של nlp שנכתבו על כל אחד מהדברים האלה, הוא בטח לא נגיע, אבל לעשות את הדבר הזה, זה היה תהליך מאוד מאוד מורכב פעם. אוקיי, זה דרש הרבה מאמצים, והנה ככה בגדול שלבים שאנחנו עושים, שוב, בשביל הסוג השאלה הפשוטה, שאלות של עובדות, היה הרבה פקטוראי q&a, q&a, question and answer, שאלות ותשובות.

אוקיי, אז דבר ראשון, אנחנו רוצים לעשות איזשהו עיבוד של השאלתה. מה סוג השאלתה? איזה תשובה מצופה? איפה בעצם להתמקד? איזה סוג של יחס יש כולו? כאן יש לנו מוזיאון הלוב, ממוקם בפרי. גם לא פריז כידוע לכם, כן, אני פה שומעים פרי.

אז לא עובד בזה. לא עובד, כן. בגלל זה, אגב, כל הפעמים שאתם כתבתם באנגלית ונאבקתם, כנראה זה מקור מצרפתית.

כל ה-OUGH זה מצרפתית. אנגלית זה ככה, אנגלית הייתה תרכובת של לטינית, צרפתית, גרמנית, ואולי עוד משהו. היידיש זה... לא, אבל במקור באנגלית... האנגלית כאילו היא דבר מגיע, נעמם על גרמנים, צפון של גרמנים, מסוימת.

נכון, אבל לא רק. רגע נתקע עוד מסוימון מסוימון. בריטים, ההולנדים, הספרדים, הפורטוגזים, זה הקובשים הגדולים בעיקר.

או רומא. זה קודם, בסדר, אבל... למה היבשת האיזוטרית הזאת אירופה נהייתה כזאת מרכזית? כי הם... הם בסדר. לא, כי הם קבשו את כל העולם.

מה? הם לא ממש עוד בסדר. היום, אבל עובדה שעדיין יש להם השפעה, וכאילו אפשר לשאול מה מעניין את כולם עליהם, אז יש להם עוד השפעה. עוד פרידים מפעם.

תשאלי את ההודים באיזה כיוון בתנועה הם נוסעים. למשל, כן? או את הפקיסטנים, נגיד. למה הם נוסעים הפוך מאיתנו? בגלל הבריטים.

ליפניהם לא הגיעו, או ל... לא, ליפניהם לא הגיעו, אני גם נוסעים בפקיסטנים פשוט... בפקיסטנים פשוט שלנו. לגמרי, כן. טוב, אז... נגיד, שאלה של וואי... איך נגיד... איך אנשים אינדיאנים... נסעים בשדה, זה לא שאלה פקטואית פשוטה.

אוקיי? זה שאלה מורכבת יותר, אז אנחנו לא נמצאים. אז דבר שני, וכאן בעצם יש קצת ממה שראינו כאן. אוקיי? במקום להחזיר את המסמך השלם, כבר דיברנו על זה קצת קודם, אבל כאן זה ממש נדרש, אנחנו עוברים להסתכל על פסקות ולא על מסמכים שלמדים.

אוקיי, אנחנו מנסים להחזיר פסקה רלוונטית, זה עדיין לא התשובה שהיינו רוצים לקבל נגיד מ-AI כזה שיענה לנו וממש הסנטז לנו את המידע הוא בעצם יסביר לנו למה הכוונה לפי המקורות שלו במילים שלו לצורך העניין, אבל זה כן ממקד הרבה יותר את התשובה זו החלוקה לפסקאות מראש, מהדקסים פסקאות במקום מסמכים וגם הרנקינג קצת השתנה בעקבות זה, הדירוג של התשובות וכמובן יש לנו ייצור של התשובות שגם הוא מושפע מכל התהליך הזה אז הנה יש לנו פלור כאן של QA אוקיי, יש לנו איזשהו שאלה ואנחנו צריכים לעשות עיבוד של השאלה אוקיי, אז כבר ניגע וניכנס פנימה אבל כמובן שיחד וכתהליך מקביל אנחנו אמנם כאינפוט אולי עדיין נותנים מסמכים אבל כבר בשלב הראשון שוברים את המסמכים לפסקאות ומה שרוצים לעשות אחרי זה זה לא דוקימנט רוטריבל אלא פסס רוטריבל כאילו החזור פסקאות במקום החזור מסמכים זה עדיין לא תשובה ממש אבל זה צד אחד בכיוון של לתת משהו יותר ממוקד התהליכים של מה רלוונטי מתחלקים כאן אם אנחנו רואים קודם כל זה שיטת המסננת מי שמכיר אוקיי, אז כמו שיש לנו גיוס מועמדים לעבודה או פרסום שהוא לא ממוקד בהתחלה ואז כמו הקפה פילטר שאני אוהב להראית זה בתור דוגמה יש לנו את הקפה לפני ואז נתחן דק דק ורק מה שעובר דרך החורים הקטנטנים האלה של הפילטר זה מה שבסוף אנחנו מקבלים בקפה כאן נגיד אז אותו דבר כאן אנחנו מתחילים עם המסמכים זה האינפוט הרחב יחסית ואנחנו אחרי זה עושים רירנקינג על הפסקאות אז כאילו יש פה מאמץ של לא לאבד לגמרי את האחיזה מהמסמך אבל כן בסוף להחזיר פסקאות ולעשות רירנקינג לפי הפסקאות כלומר רירנקינג זה משקול מחדש איפה נמצא המידע בכל זאת הרלוונטי שמעניין אותה גם לשאלות אנחנו רוצים לעשות איזשהו עיבוד אוקיי, שחלקו כבר הוזכר פה מה הרלוונטי בעצם מתוך השאלה שעליו אנחנו רוצים לענות מהם רק מילות קישור כאלו, מה סוג התשובה שאנחנו מצפים לקבל אוקיי, אז נגיד אם השאלה היא where אז אנחנו רוצים לקבל איזשהו location אוקיי, זה סוג תשובה אוקיי, אז הנה פירוט קצת יותר גדול של המשבצת הזאת של ה-Question Processing אז יש לנו את זיהוי סוג התשובה אוקיי, זה מושג שקוראים לו בעיבוד שפות טבעיות ב-NLP Name Entity Recognition קוראים לזה אם זה מתוך הטקסט וכאן זה Name Entity Type Extraction כאילו לחלץ את הסוג של ה-Name Entity ומקום וסוג האישות כן, מקום בין אדם, נגיד מה השם של ראש הממשלה במדינה מסוימת או מתי גילו את יבשת אמריקה אז כמובן התשובה זה לא יודע מינוס 2500 לפני הספירה אני חושב נכון, מתי היו היבשות בין סיביר לארה״ב מחוברות והמונגולים עברו מסיביר לאמריקה אתם מכירים את זה? כאילו כשאומרים מי גילה את אמריקה בתשובה קולומבוס כי קולומבוס היה אחד מהכובשים הספרדים שהגיע לאמריקה אבל כמובן גילו אותה הרבה לפניו, הרי היו שם מה שהאמריקאים קוראים להם אינדיאנים שזה דיוי לא נכון שלא עודים מה? אוקיי, אבל זה לא היה סרט, זה היה חיבור בין, זה בחוד דרך הקרח אז כאילו ההנחה היא שהמקור של מה שקוראים להם אינדיאנים זה היה כנראה לא, הוויקינגים זה הרבה אחרי, אבל כאילו הנדידה בין היבשות זה אומרים שזה כנראה מונגולים זה קרה רק בעידן הקרח, כי באמת, כי היה חי גשר בין העם לסלב כן, נכון, אבל מי העם שעבר מהיבשת הזאת להיבשת הזאת אני כמעט בטוח שזה מונגולים זה מה שאני מכיר אני חושב שהם כאילו, העבורות שלהם כן, כן, הרבה לפני שנות אז אמרתי מינוס אלפיים וחמש מאות זה סתם הערכה זה אומר מינוס עשרים אלף? סבבה כן, בבקשה כן, כן לא יודע? הם מליצו עליך בתור ההיסטוריון של הקבוצה אז אתה חייב לדעת הוויקינגים לא היו פה עודם מי? כן, כן אוקיי, משהו כזה בסדר אוקיי, משהו באמצע אני הייתי קרוב יותר למה? אני אמרתי לפני חמש תלפים שנה אמרתי מינוס אלפיים וחמש מאות בערך וזה יותר קרוב למינוס עשר תלפים אשר להסתער כאילו תקשיב תשים פה את הכסף וזהו בלי עניינים אז שאלה היא לא אתה אמרת לפני עשרים אלף שנה אני אמרתי לפני חמש תלפים בערך אז אני הייתי קרוב יותר מצטער נריב אחרי זה, בסדר? אבל לא, אנחנו נריב, כן? סבבה אז מאותה סיבה גם בדרום אמריקה מדברים ספרדית ופרוטוגזית כמובן זה ברור וידוע כי הקופשים שם היו מספרד ופרטוגל אז אוקיי אז זה נאמן TTIP צריך לדעת בעצם מה הסוג של התשובה בכלל שאנחנו עונים כמובן שבמידה המעונדה צריך לדעת שנגיד נשיא מדינה מסוימת או אני לא יודע מה ממציא מי גילה את הנורה ראשונה או לא יודע מה את החשמל אז צריך לדעת שם של בן אדם אולי את הזמן אם שואלים אותנו מתי זה קרה צריך לזהות מה הסוג של ה-entity ואת זה צריך גם להנדקס כדי שנדע לחפש את זה אז מה זה? זה רולבייסט נכון, לא הכל רולבייסט נגיד באינדוקס לזהות את ה-entity זה לא בכך בכלל רולבייסט זה מערכות שמאוד מבוססות למידת מכונה אבל לא Deep Learning כי זה משהו שרק בעשרים ומשהו שנים האחרונות עברנו לשם אנשים עשו דקטורטים שלמים על יחסים נאים את ה-entity אז יש פה חוקים אבל גם רובי למידת מכונה זה מערכות שמשלבות למידת חוקים הרלציות זה גם משהו מאוד מורכב יש לנו נגיד בדוגמא עם ה-loop אז להבין שיש לנו ה-loop נמצא בפארי אז זה רלציה זה יחס אז הנה דוגמא לאותו חילוק של-name entities למשל who founded the virgin airlines as person זה היה התשובה זה ה-name entity type זה לא התשובה לשאלה what Canadian city has the largest population אז התשובה היא צריכה להיות לא רק location אלא city משהו יותר ספציפי אז יש לנו כל מיני סוגים ויש לנו גם קיצורים כאילו למשל מה מה מה? אתה צודק נכון, בסדר לא, כי גם הפרסן הזה שאמרת ארגון אז בהגדרה סמנטית זה לא מדויק כי בעצם כמו שאני אגיד את המילה they הם, ואז אני יכול להתייחס או לקבוצה של אנשים נגיד אני אגיד they are great students ואני אתייחס אליכם או שאני אומר they ואני מתייחס נגיד לחברת IBM אז כאילו אי אפשר היה להגיד they אם החברה לא הייתה מורכבת מאנשים בסדר, אז כאילו מבחינת name entity זה לא מדויק שזה לא אנשים גם גם אם זה לא לגמרי כן אנשים זו התשובה בגדול של איך התייחסו לזה אוקיי, אז יש עוד כל מיני קלאסים אחרים שאנחנו רואים נגיד שאלו אותנו what city זה לא סתם location זה משהו יותר ספציפי וגם פה יש לנו כל מיני דברים כמו גם מה שנאמר פה אם זה שם של חברה, שם של בן אדם קבוצה וכולי וכולי אז הנה יש לנו כאן דוגמה לכל מיני שאלות ומיפוי של entity type שלהם אז בואו ניקח משהו אולי טיפה פחות טריוויאלי אז הנה what does stock vaccine presents אוקיי, אז אנחנו צריכים להבין שהתשובה היא זה אמור להיות מיפוי לדזיס, לא למדסה כאילו מה זה מונע אז מונע זה מחלה כלשהי, זה משהו מורכב קצת לעשות אז במערכות האלו הרבה פעמים הייתה שלובה לא רק זה התחיל מהרבה מערכות פוקים אבל זה אם דיברנו על ריקול הריקול פה היה מאוד נמוך ושילבו בזה הרבה למדעת מכונה אבל עדיין המדעת מכונה הייתה נראית פרימיטיבית לעום הדברים שפותרים היום זה בכל זאת לזהות מתוך הרבה מאוד entity types שמאוד ככה ספציפיים יותר אז לזהות מה רלוונטי לשאלה מסוימת אוקיי אז זה התחיל בתור באמת כל מיני regular expressions כזה בכלל כל העולם של חילוץ מידע, information extraction התחיל מהמון regular expressions מאוד מורכבים אגב והיררכיים גם קודם לזהות משהו ואז האנטיטי הזה אחרי שזיהית את הקידומת הזאת ככה בתחילה נראה אני אמתתי extraction ואחרי זה שולבים הרבה מאוד למידת נכונה אז חזרה ככה לפלואו הזה אז עדיין כחלק מהמאמץ כאן אנחנו רוצים לעשות חילוץ של keywords מילות מפתח או ביטויים מפתח הרבה פעמים אז קודם כל אנחנו מתחילים עם השאלה זה ממש קשור למה שאגב שאלת ואמרת קודם אז אנחנו רוצים להעיף מהשאלה את המילים שהם stop words אוקיי אלא אם כן הם עזרו לנו בהתחלה לחילוץ name entity type אז עכשיו זה כבר לא מעניין אותם אז יש פה באמת כל מיני מטרות של איך לאבד בצורה חוקית כזאת בעצם אנחנו מזקקים את מה שמעניין אותנו בסוף הופכים את השאלה ששאלו בעצם זו השיטה שהייתה לנו גם פעם למשהו שדומה לשאיל תוק של יחזור מידע שאנחנו מכירים במקום מילות מפתח אבל עם כיוון למה אנחנו מחפשים בעצם זה בעצם השיטה שבה עבדו אז אם יש לנו פועל זה דבר חשוב כי הפועל הוא רלוונטי בצורה ישירה על היחס שיש לנו בין האנטיטיסט נגיד זה ממוקם שם אז מיקום דווקא זה לא דוגמה כל כך טובה אבל מי שעכשיו שולט במדינה הזאת והזאת הוא הבן אדם הזה אולי דוגמה קצת יותר טובה נומינל אגב זה סוגים כאן של שמות עצם ונפי זה נאום פרייז ביטוי של שם עצם בעברית קוראים לזה ביטוי שמני אני לא מכיר ביטוי מוצלח יותר להגיד מאשר הביטוי שמני אבל רובם פשוט משתמשים בנאום פרייז נאום פרייז יכול להיות נגיד באנגלית the president או אולי אפילו the president of אני לא יודע מה איחוד האמירויות נגיד אז יש פה name entity בתוך name entity אוקיי אולי איחוד אמירויות זה שליט של מה הוא אני עכשיו כאילו מרגיש שהייתי בעובדות קצת רוחמת בן זייד בן זייד סליחה מה זה יורש העץ לא זה בן סלמה אני חושב זה לא סעודי סליחה לא לא אני שואל מה התפקיד נשיא יורש העץ ואז אחרי שהוא ירש מה הוא נהיה הוא לא מלך נכון אז הוא כן מלך יהיה לא עכשיו מה הוא אני שואל איך קוראים לראש המדינה לא בערב הסעודית זה כן מלך בערב הסעודית זה כן מלך הוא לא המלך עדיין אבל התפקיד של ראש המדינה זה מלך אז איך זה נקרא באיחוד אמירויות אני שואל אז מה יש נשיא פרזדנט זה נשיא איחוד אמירויות נשיא ראש ממשלה יש גם ראש ממשלה הוא אטליאן גם לא אבל יש זה לא בטח ככה יש טייטל מרכזי אתה מתבלבל בין איזושהי סדרה שראית עם שמונה עונות שהיה שם הרבה טייטלים זה משהו אחר זה לא קשור לא התכוונתי לזה בכלל לא התכוונתי לזה בכלל לא יודע מפריבת איזה עידן הקרר אני מבחינתי הסתיים לפני איזה שנה או שנתיים אולי שלוש שנים עידן הקרר חמש נראה לי יצא לפני שלוש שנים הלו אה לא זה בסדר אוקיי מה באמת בגלל כל האיי איי וזה בגלל שהסרטן היה טוב לא רווחי טוב אחרי שבאמת דנו בדברים באמת חשובים אז נחזור לאזוטריה כאן אז תראו את השיטה שבאסרחי שזיהינו את הנאמנטטי שהוא באמת נדרש לענות על השאלה הזאת אז אנחנו פשוט מפלטרים מילים ככה זה נראה פרימיטיבי אבל ככה זה נעשה הוא זה כבר מילה לא מעניינת זה כמו סטופורט כזה זה זה בטח בסטופורט כן והיז לא מעניין אוקיי אז מה נשאר לנו אז כאן יש לנו אפילו דירוקשן המילים כן אז סייברספייס ונורומנסר ותרם ונובל וקויינד זה הדירוק שלהם כאן רואים בצד ימין את ה ומתוך הדבר הזה אנחנו מאחזרים פסקאות רלוונטיות אוקיי אז כאן דיברנו על שתיים השלבים של השאלה של בעצם העיבוד של השאלה האם זה חילות של הנאמנטיטי טייפ והשלב שבעצם מזקק את המילים החשובות מהשאלה שאותו אנחנו רוצים לחפש אחרי זה בתשובה וגם בשביל הרנקים זה חשוב כמובן אוקיי אז מה אנחנו עושים בשביל להחזר פסקאות אוקיי אז אנחנו רוצים כמה דברים קודם כל יש לנו צורך להתייחס לזה בזמן האינדוקס אז ובאינדוקס אמרנו שאנחנו עושים מהלך כפול אז קודם כל בשלב הראשון ביחזור אנחנו מוציאים את המסמכים ויש להם את הרנק שלהם בקשר לשאלה אוקיי אז יש לנו הנה המסמכים שחזרו עם רנקינג מסוים ועכשיו אחרי שהוצאנו את המסמכים בעצם פילטרנו את המסמכים הלא רלוונטיים יש לנו את המסמכים הרלוונטיים ומתוכם עכשיו אנחנו רוצים יחידות קטנות פסקאות למשל או כמה משפטים גם אם זה לא פורמלית פסקה ואחרי שמצאנו את המה שהיא יותר פינפוינט יותר מדויקת יותר טיונט כזה אז אנחנו עושים רי-רנקינג עם הפסקאות שקיבלנו בשלב הזה על איזה פיצ'רינג אנחנו מתבססים אז יש פה כמה דוגמאות מספר הנאם אנטיטיז הרלוונטים אוקיי כמה מילים אחרי הפרוססינג של הקוורי יש לנו בפסקה הרלוונטית וכמובן דמיון בין הקוורי קי-וורדס לאלה שנמצא בפסקה וכן הלאה זה ככה מהשלבים נראה אם יש לנו פה איזושהי דוגמא אוקיי הנה Who is the Prime Minister of India אז הוא is זה כמובן האמת היא של פרסם בואו נראה ממתי השאלה הזאת לפי התשובה זה עתיק הרבה יותר זה לא בראש ממשלה עכשיו של הודו אתם רוצים לעזור לי? אני לא רוצה אני יודע שאני לא אצליח לקרוא את השם הזה אני מנסה זה לא מתגלגלי הראשון מה? איפה? יש לך דיקציה בוא נגיד אתה רק צריך להוסיף את מה שלפני הדיקציה והכל טוב הדיקציה אז Prime Minister of India had told left leaders that bla bla bla אוקיי אז יש לנו פה אפילו ברמת המשפט התאמה אוקיי אז זה אפילו יותר ספציפי אולי השאלה היא מסוג יותר קל כי בעצם התשובה היא אם מה שהיינו רוצים לא כל כך פשוט אולי אבל זו התשובה כאן הדבר הזה כאן כמובן שאם אנחנו לא מנסחים את התשובה אז חסר פה פועל או משהו כזה אבל יש פה את השם ואת התיאור של Prime Minister of India אם היינו מצליחים להוציא את הדבר הזה אז זה כבר לא תקני באנגלית אבל זו התשובה How tall is Mount Everest? אוקיי טוב לדעת אה זה לא רלוונטי לחלוטין בשבילנו כמובן פיט זה סביב ה-30 סנטימטר אם אני לא טועה 30 סנטימטר או שזה אני חושב ש 4080 800 800 חדר קצת קטן אוקיי משהו קצת יותר מורכב Who was Queen Victoria's second son? אני חושב שגם כאן שוב יש איזשהו רלציה קצת יותר מורכבת אוקיי אז המערכות האלו כאמור בהתחלה התחילו מערכות מבוססי חוקים אבל מאוד מורכבים כן זה נשמע לנו פשוט שזה מבוסס חוקים אבל זה היה צורך ומומחה באוטומטים כן אוקיי אז זה מהר מאוד עברו גם ללמידת מכונה כי הניסוח של החוקים האלה זה משהו שאי אפשר לתחזק באמת גם אם הם יחסית מאוד מדויקים אז זה נותן לנו ריכול מאוד נמוך ויש לנו פה עוד כל מיני יחסים נוספים איך מודדים את ההצלחה אז אחת המדדים הפופולריים למדינת הצלחה של מערכות שאלות ותשובות של QA אז מעבר לאקיוסי שמכירים בטח בכל מיני תחומים אז מה שעושים זה בסוף יכולים לחזור כמו במנוח איפוס כמה פסקאות נגיד שונות על השאלה וכאן אנחנו מדרגים אותם לפי סדר המופע אוקיי הראשון שהופיע אם הוא נכון אז הMRR שלנו שווה ל-1 אוקיי איך אנחנו עושים את זה אז יש לנו כאן זה לשאלה אחת כן רוצים לדעת הציון לשאלה אחת שאלה ספציפית בסוף לוקחים N שאלות ועושים ממוצע ביניהן זה הMRR אוקיי אז הריספרוקל ראנג כאן זה לוקחים את המיקום עושים מרחב חלקי המיקום התשובה במקום ראשון תקבל ציון של M1 במקום השני חצי, במקום השלישי שליש וכן הלאה אוקיי אז עושים את כל מיני תשובות את המוצע בין הדבר הזה מקבלים את הMRR כי אני חושב שזה באמת אפשר לעדן את המדד אבל אם תחשבו רגע על מנוע חיפוש שיחזיר תשובות תגידו לי צ'אנק של עשר במקום הראשון נגיד כל עשר התשובות הראשונות זה עוד איך שהוא בא ניתן לזה נגיד ציון אחד אבל אין ספק שככל שאנחנו מתקדמים בתשובות שמוחזרות אז אנחנו פשוט לא נקרא את התשובות הבאות זה הכוונה עכשיו צריך לזכור שזה תשובה למערכת של שאלות ותשובות זה לא תשובה לחיפוש רגיל שעה זו נגיד 100 ל-10 תשובות זה בסדר אז כאילו ההנחה היא שאנחנו קוראים את התשובה הראשונה רואים שהיא כנראה לא קשורה או לא נכונה עוברים לתשובה הבאה אם אנחנו מתבססים גם על המידע הזה בשביל משהו אחר שאנחנו צריכים עוד יותר גרוע אז אפילו לא נתן לנו אולי את התשובה הנכונה נגיד שזה כן הנאם אנטיטי הנכון אבל זה לא התשובה הנכונה אז יש לזה מחיר גדול אז יש היגיון מסוים באנשה הזאת אולי זה האנשה עם עין עם עונש אולי הכנס פה הוא כבד מדי זה מה שנבין בין השירות אז זה היה רק הצצה לעולם של QA באמת אפשר לעשות על זה קורסים שלמים גם עוד איזשהו משהו שאולי המרחק שלו קצת יותר גדול מאשר מערכות יחזורו מידע אבל אחרי שדיברנו על איזשהו שאילתא ואחרי זה השאילתא הפכה לשאלה בכלל שרצינו עבור התשובה אז מכאן המרחק לא מאוד גדול למערכות המלצה במערכות המלצה היום בכל מי שמשתמש באינטרנט בעצם בצורה כזאת או אחרת באפליקציות שמשתמשים בהם אנחנו מקבלים המלצות אם זה אתרי חדשות אז אולי יעניין אותך גם הכתבה הזאת או יעניין אותך אם זה סדרה או סרט בנטפליק שמאוד התפרסמו על מנועי המלצה שלהם אז אולי יעניין אתכם גם הסדרה הזאת המוצר הזה של אמזון או של הליבבה יעניין אתכם מוצר אחר מוזיקה וכדומה אז זה מאוד חזק בתחומים האלו ואם אנחנו עדיין לא הולכים לכיוון שמשתמשים בהם אז אנחנו יכולים להגיד דבר כזה סרט מוצר שיר כל הדברים האלה בעצם מורכבים במילים אז כמו שאנחנו עושים query expansion ואנחנו רואים שקיבלנו תשובה מסוימת ועכשיו ניקח את הדבר הזה בתור שאילתה זה בעצם מה שעושים עכשיו בואו נראה מה דומה לשאילתה הזאת, ממה מופיעה את השאילתה מכל המילים האלה שאנחנו כבר מכירים, זה היה המאמץ הראשון של מה שעשו זה לקחו את התוכן וחיפשו תוכן דומה אוקיי אז אנחנו כבר נראה את הדבר הזה אבל יש פה עוד דבר נוסף שהגיעו כאן וזה בעצם דמיון בין היוזרים שכולכם בטח גם מכירים את זה כבר, זה כבר די נפוץ אז יש לנו יוזר דומה אלינו אז לכן אם יש יוזר שדומה לנו אז אולי אם הוא רצה גם את המוצר הזה למשל או ראה גם את הסדרה הזאת אז זה יניין אותנו גם סדרה או סרט שהוא ראה, כל הדברים האלו אם זה נראה מוכר, זה מוכר כי קודם מה עשינו בעצם, חיפשנו גם שאילתה, שאילתה זה נגיד אנחנו בתור יוזר או אנחנו בתור מוצר שקיימים באינטרנט ועכשיו תמצא לי דברים רלוונטיים שבאים יחד איתו כאילו זה היה שאילתה היוזר או המוצר או הסדרה או השיר ששמענו, תמצא לי עכשיו משהו שדומה לשאילתה הזאת זה הקשר, ואז יש לנו אותה מלפא קוסינוס נגיד שעושים בין הוקטור של זה לוקטור של זה של זה אנחנו מכירים אז הנה דוגמה שזה כבר נראה אולי מוכר לנו יש לנו פה בציר של השורות משתמשים סתם עשו את זה לפי ABC אניטה, ביונסה כנראה זה היה מזמן שביונסה הייתה מאוד מפורסמת מניח קלווין ודיוויד לא אבל אוקיי סליחה אני מצטער שכחתי שאתה מעריץ חרוף של ביונסה לא לא בסדר זה היה משכנע בסדר אין דבר כזה פרסום רע אתה לא מכיר? כל פרסום הוא פרסום טוב כולם עכשיו מכירים אותה בגללך מי שלא הכיר קודם עכשיו ילכו ויקשיבו לשירים שלה יעלו את הקליקה שלה, יתקבל עוד כסף בגללך בעצם עזבתי עזבתי אז שנייה נדבר על זה מה שיש לנו כאן זה סרטים שיש אני רואה בהתחלה רציתי להגיד ספרים כי היה כתוב הריפוטר אבל זה סרטים רואים את הטווילאט ואת הסטאר וורז אז זה סרטים שמשתמשים שונים הביאו להם דירוג או שלא הביאו להם דירוג בכלל עכשיו הדירוגים אני לא יודע כמה אתם מכירים דירוגים באינטרנט אבל איכשהו התקבע גם רואים את זה הרבה פעמים בנותני שירות שרוצים שייתנו להם דירוג אז זה ממש משהו שהתקבע, אחת זה מאוד לא מרוצה נגיד או מאוד חושבת שהשירות שנתנו לי ממש תחת לכל ביקורת וחמש זה ממש, אם נספס משהו אחר, נתקל יפים על זה, אבל לא מצליח שנתקל. צבע חמש ממש מרוצים. אז זו שיטה לא כל כך טובה, אנשים גם לא כל כך טובים במספרים, זה לא השיטה שאנחנו אנשים חושבים בדרך כלל, וכנראה גם חמישה כוכבים גורם לזה שהמדד הוא מאוד סובייקטיבי, יש הרבה חסרונות בשיטה הזאת של רנקינג, אבל לא לשם כך התכנסנו כאן היום.

אז בואו נראה, ויש גם עוד כל מיני שאלות, מה עושים אם יש סרט שלא קיבל רנק? נגיד ביונס לא נתנה שום רנק על סטאר וורז 3, נגיד, אז מה עושים? אז היא אוהבת את זה או לא אוהבת את זה? זה ממש קשה לנו. אוקיי, אז אנחנו רוצים, הבעיה אני חושב ידועה, ויש לנו פה כל מיני קשיים, אנחנו למשל רוצים לדעת מה עושים אם אין רייטינג על משהו מסוים, ואיך אנחנו גם משערכים את הפתרון בסוף, אז בשביל לענות על השאלות האלו יש לנו תשובות שעונות על השאלה הזאת בצורה ישירה ובצורה עקיפה, למשל לשאול את האנשים איזה רייטינג הביאו, כלומר, האם אהבתם את הסדרה או את הסרט שראיתם? אז אני לא יודע איך אתם, כמה סדרנות יש לכם וכמה אתם בעצם תורמים לחברה שם בזה שאתם עושים רייטינג על הדברים האלה, אני אישית, אני לא כל כך, מה? אוקיי, אז יש לנו זמן צפייה, אם יש לנו מוצרים מסוימים אז אחד מהדברים הכי מפורסמים זה קליק רייט, כמה קליק הם עשו על מוצר מסוים, ככה יודעים שאהבו אותו, נגיד חיפשו מוצר, ואז היו כל מיני המלצות, שנגיד בגדול הם היו בכיוון, אז בחרו בסוף מוצר ספציפי, נכון? הסימן שאהבו אותו יותר, למה דווקא אותו? אז לומדים את הדברים האלה, אוקיי? אז ודיברנו על כל מיני, ב-user feedback על כל מיני, כשדיברנו על user feedback על כל מיני דברים עקיפים, שעושים גלילה במסך ונשארים תקועים על אזור מסוים, אז כנראה שזה או שיצאנו להחזקת קפה או שזה העניין אותם, אוקיי? אז בעיות נוספות שמאוד מאוד מפריעות לנו, זה קודם כל, כל וקטור כזה הוא מאוד דליל, מאוד ספרס. אוקיי? יש לנו קצת מידע על נתונים מסוימים, וחסר לנו את זאת המידע, לא יודעים מה דירוג על השאר.

בעיה שנייה שבעצם, בעיה שמאוד חזקה נגיד אצל ה-AI שעובדים איתה, שמדבר גם על המונח הזה גם שם, call starts, אוקיי? אנחנו רוצים מערכת שתעבוד בצורה די טובה, גם אם היא תשתפר זה סבבה, רוצים שזה מי שמכיר את הביטוי, איך מומלץ לרוץ מראטון, אז מתחילים הכי טוב שלכם, ואז מגבירים את המהירות, כן? נפצע סבתא נגד. אוקיי, אז כבר אתם רואים שיש לנו פה עוד שימוש נוסף, אם אתם זוכרים את המירה latent, אוקיי? אז גם זה מהווה חלק מהפתרון גם לצורך recommendation systems, אבל כאן יש לנו בעצם, דיברנו בעיקר על content base, על התוכן, שלפי זה אנחנו יודעים כמה וקטורים דומים, אבל גם על עוד דבר שאתם יכולים, זה פשוט לא יאומן איך השם השיווקים ממש תופס, זה נשמע וואו רציני, אבל נגיד הציר השני, זה נקרא collaborative filtering, תבינו לבד מה זה אומר, הציר אחד זה התוכן, אז השני זה ה-users, אוקיי? מיון בין ה-users, אוקיי? אז מבחינת מתמטית זה אותו דבר, זה ציר אחד והציר שני, זה לא, זה כמו שאנחנו לצורך העניין יכולים לעשות מניפולציה פשוטה, ואז להפוך את ה-PCL, algorithm clustering, אוקיי? אז מי שלא מבין מה אני רוצה אז שכחו מי, זה לא חשוב. אז זה הרעיון המרכזי, לקחת לקוח X, ושעניין אותו נתן רייטינג מסוים קודם, ומה להמליץ לו עכשיו? אז יש כאן את הדוגמאות שדיברנו עליהן, אז הרעיון הוא לקחת מילים, מילות מפתח, יש לנו כאן משהו קצת יותר מורכב מסתם מילים, כי יש לנו כאן כל מיני קטגוריות בכל זאת, אולי לא מספיק דיברנו על חיפוש שהוא חצי מובנה, יש לנו כל מיני שדות, אז כאן יש לנו כל מיני שדות, יש את ה-genre של הסרט, יש את השחקנים ואת הבמה, אז יש כל מיני ג'אנרים, אם היה שחקן מסוים, אז ימליצו לנו על עוד סרטים שאולי אותו שחקן הופיע, אם ראינו ג'אנר של, אני יודע מה, אימה רומנטית נגיד, יש כזה דבר, נראה לי, סרט מתח אוטוביוגרפי, אז אולי אנחנו רוצים לראות עוד סרט מתח אוטוביוגרפי, אוקיי, התקופה של הסרטים, בואו נגיד אם תשאלו בהנחה שההורים שלכם רואים סרטים וטלוויזיה, או אולי לא, אם כן, אז יש איזה אהבה לתקופה מסוימת של סדרות או סרטים, ואז הסדרות הרדודות האלה של הצעירים, כל פעם כל דור חושב שמשהו הוא ראה ומשהו הוא חווה זה הכי הכי, וכל הצעירים האלה מתעסקים בשטויות כל הזמן, עם הסרטונים המטופשים שלהם, מימים וכל מיני כאלה דברים שממציאים, כן, וככה כל דור חושב שהוא הקלאסיקה והאיכות, וכן, אני לא יודע אם אתם מבינים מה אני אומר, אבל נראה לי שנתקלתם בטח בדו-שיח כזה, ולא משנה אם צעירים מכם או בוגרים מכם, אז כאילו אני נגיד את הקטע של המימים לא מבין למה, משהו שמתאים ל-regular expression של מימ זה צריך להיות מצחיק, זה מתקשה להבין את זה נגיד, סליחו פעם, כאילו גם דור שלהם צריך להיות מומחה באוטומטים, אני לא מבין את זה, פאטרן matching כזה, יש לזה הסבר? כאילו אתם, לא? למה מימים? הוא מתאים לפאטרן מסוים, אז כאילו צריך לעשות במוח שלנו את זה, פאטרן matching מורכב כזה, ואז בגלל שזה עונה על, יש לו מעל סף מסוים של דמיון אז אנחנו צוחקים? זה קרה בזומן, משהו שלא התחשב בו, אז כאילו אתם מאוד חזקים נגיד במרחב וקטורי, טוב, בסדר, אז כאילו ביחס לזה, אז הדור שלי הרבה יותר פרימיטיבי, אין מה להגיד, אין התאמה וקטורית כזה לפני כל בדיחה, אז אוקיי, אז גם פה אנחנו יכולים להכניס tf-idf, לשים איזה סף, וככה לקבל המלצות, אז הנה למשל שתי סרטים, יש לנו כל מיני, דווקא מידע מאוד מובנה כאן, שחקנים שהשתתפו, אולי, כל מיני אנשי צוות, בהרבה סרטים איכותיים, נחשבים שגם הבמיי, וגם התסריטאי, הם מאוד מוכרים, לא יודע, נגיד פעם, היום גם זה נחשב פשוט, בעלה של, איך קוראים לפיק, דניאלה, אז פעם טרנטינו היה נחשב משהו משהו, היום זה בעלה של דניאלה פיק, אגזמתי קצת, רק בארץ זה ככה, אבל פעם טרנטינו, היית רואה סרט של טרנטינו, כנראה אתה אוהב את הז'אנר, תרצה לראות גם את האחרים, אז, יש את הרייטינג הממוצע, לכל אחד מהסרטים, לא רק, האם מופיע או לא מופיע שחקן מסוים, אז יש רייטינג ממוצע, אם יש לנו שתי סרטים, רייטינג דומה, אז כנראה מי שאהב את זה, ממוצע אהב גם את זה, זה משהו מאוד ככה, גס, ואז, מה שעשו זה, חילקו את כל הפקטורים, לשתי בקטים כאלה, בקט אחד זה, נגיד כל הצוות ששיחק, ג'אנרים, דברים כאלו, כל זה שמו ביחד, ובקט שני זה הרייטינג, ואז החליטו כמה הרייטינג חשוב, זה כאילו ממש מערכת חוקים כזאת, כמה זה חשוב, ואז עשו מרחק פוסינוס, בין שתי הסרטים, ונתנו רק, שתי סרטים, עם מרחק שובר שחק מסוים, כמובן ניתן קודם את הסרטים, שהמרחק פוסינוס ביניהם, או הדמיון פוסינוס ביניהם, הוא הכי גבוה, אז זו השיטה, שפעם היו עושים, גם היום עשו מרחק ויקטורי, זה נפוץ, אבל הכניסו כאן, כל מיני פקטורים נוספים, אז למשל, אנחנו רוצים להתייחס, לא רק לסרט ספציפי, ולהגיד כמה סרט אחד, דומה לסרט שני, אלא להתייחס ליוזרים עצמם, ואז נוכל לשאול, כמה יוזר מסוים בכלל, קרוב, בשלב הראשון, שוב אנחנו בעולם של, סרטים מסדרות, אז אפשר, מי שממש, יש סלידה מסרטים מסדרות, כי לא יודע מה, זה משחית את הנפש, או שזה, בזבוז זמן, אז, זה רק דוגמה, אל תיקחו את זה, אוקיי? אני מכיר אנשים, שממש אידיאולוגית, זה מפריע להם התחום הזה, אז אם אתם, אני לא בוחר פה צד, אוקיי? אז כמו שעשינו מרחק, בין שני סרטים, יכולים לעשות מרחק בין יוזר, לבין סרט עצמו, ואז מהדבר הזה, אנחנו יכולים ללמוד, כמה יעניין אותנו, סרטים אחרים, שהם יותר דומים לטעם של היוזר, אז, מה שכמובן מאוד בעייתי פה, זה ש, אין לנו המון אינפורמציה, על רוב הפרמטרים, אוקיי? יש לנו שוב וקטורים מאוד ספרס, תחשבו שאנחנו עושים כבר וקטור ארוך, אחד, שמאחד את כל השחקנים, את כל הג'אנרים, את כל ה... אז, זה מאוד מקשה, ויש לנו כאן עוד קשיים נוספים, שכבר אנחנו ניתקל בהם, אה... אז, יש לנו כאן, אה... פתרון, ניסיון לפתור את הבעיה, עם שיטה, שאז זה היה חדש מאוד שהמציאו, שנתנו לה את השם המפוצץ, קולאבורטיב פילטרינג, או, כמו שאנחנו יכולים לקרוא לזה, Waze, זה גם סוג של קולאבורטיב פילטרינג, אוקיי? אז יש לנו, אה... משתמשים אחרים, שהם דומים למשתמש שלנו, במה הם דומים? בזה שהם נתנו דירוגים דומים לדירוג שלנו, בזה הם דומים, אוקיי? שוב, קופצת אותה שאלה, מה עם המידה החסרה, אבל כבר נגיע לשם, בואו נראה למה אנחנו מתכוונים, יש לנו נגיד, ראינו כבר את השקפית הזאת, אני חושב, יש לנו כאן את כל הסרטים האלה שמשתמשים שונים ראו, בין השאר, הזמרת האהובה פה של ביונסה, זה בי? סיכמנו על זה שאתה הגרופי של ביונסה, לא? לא? טוב, אז לא.

לא קראתי נכון בין השירות. אז, למשל, ביונסה, מאוד מאוד אהבה את סרטי הרי פוטר בדוגמה הזאת, אבל אין לנו שום מידע עליה לגבי שאר הסרטים, ויש לנו אפילו את, נגיד, היוזר די, דיוויד, אני חושב, שהאמת, אני לא יודע למה נותנים לו להיות יוזר, כי הוא האמת שיוזר מצוים, משלם כסף ולא רואה כלום, לא גונב רוחב פאס. אז, יש לנו כאן שתי משתמשים, מה אנחנו עושים? אנחנו עושים את המרחק קוסינוס בין ה... וקטורים שלהם.

וקטורים שמייזגים אותם, וקטורים שהם דירוגים. מה בעין קופץ שאולי הוא קצת בעייתי? אז יש לנו כמה בעיות, יש לנו ערכים חסרים. יש לנו את הדירוגים עצמם שהם בעייתיים, והבעיה בדירוגים עצמם, מעבר לזה שעוד רגע נסביר מה בעייתי אצלם, הם גם מאוד סובייקטיביים. 

אני לא יודע, אם כן יצא לכם לענות על סקר, אתם יודעים להגיד על עצמכם, האם אתם סופר אובייקטיביים, או שקשה לכם, נגיד, לסמן על משהו אחד, כי אז זה נותן פידבק שלילי, ואתם לא רוצים להיות עד כדי כך שלילי, אז אף פעם לא תיתנו אחד, אפילו שתיים אולי לא, ואותו דבר הפוך גם, להגיד חמש על משהו זה באמת צריך להיות תופעה פנומינלית, אז לא ניתן חמש אף פעם. אז למשל אפשר להגיד על די, על דיוויד, שהוא אולי תמיד נותן שלוש. כאילו, כל עוד זה באיזשהו דווח מסוים, אז הוא ניטרלי, זה היה בסדר.

זה לא היה מעולה ולא היה גרוע, מאוד קשה אולי לדיוויד להוציא תמיד מעולה או גרוע, אז עצם הדירוג עצמו הוא גם בעיה בפני עצמה, אבל זה לא נגמר רק בזה, זה גם, אוקיי, המשמעות של הדירוגים פה הולכת לאיבוד לגמרי במרחק קוסינוס, למה? כי עצם זה שיש, נגיד את דיוויד שענה על אריפוטר שטיין, אז כבר נראה כאילו חמש ושלוש די קרובים אולי, אוקיי, או אותו דבר עם טווייליט וסטאר וורז אחד, עם A וC, אז יש לנו תשובות לשניהם, אז הם כבר נהיו די דומים, אבל בעצם אם נסתכל על שתי הסרטים האלה, היוזר A נתן דירוג מעולה לטווייליט ודירוג גרוע לסטאר וורז אחד, ודי הפוך, פשוט כנראה שC קצת יותר מנרמל את התוצאות ולא נותן תוצאות קיצוניות בדרך כלל, אז הוא נתן שתיים וארבע, אוקיי, אז מה המחק בין A ל-C למשל? 0, 3, 2, 2 אוקיי, לעומת זאת, המחק בין A ל-B הוא יש לנו רק אינדיקציה אחת, את אהבה של שניהם לסרט של ארי פוטר אחד, כי שאר הסרטים פשוט הם לא ראו אותם סרטים, אוקיי, אז כאן זה בעצם די קרוב בפקטור הזה, וכל שאר הפקטורים, אין לנו שום דבר להגיד כי הם לא ראו את זה, אז אנחנו מודדים את זה כ-0, אין לנו שום דבר, אז יצא בערך שהדמיון בין A ל-B ול-A ל-C הוא די דומה, בין A ל-B זה קצת יותר קרוב מ-A ל-C, אבל אין לנו משהו מיוחד להגיד, והדבר הזה, בהנחה ששניהם נגיד אהבו את אותו סרט, והרבה פעמים המידה שלנו מאוד דליל, כמו שיש לנו פה, כי אם תחשבו על מוצרים ורכישה, אז בכלל זה יהיה עוד יותר דליל, כי זה, אם יש שמים בחיים ואנחנו כל היום רואים טלוויזיה, או מחורים לזה או משהו, אז זה משהו אחד, אבל רוב האנשים אולי אין להם זמן לזה, צריך לעבוד, צריך ללמוד, צריך, לא יודע, משפחה, חברים, אין זמן לבזבז כל היום על סרטים, ולכן סוף סוף אם הסכימו לדרג משהו, זה יהיה דליל מראש בהגדרה. אוקיי, אז זה בעיות שונות, בעיה של A מול B לעומת A מול C, בזה אנחנו כן קצת יכולים לטפל. מה יעוזר לנו לטפל בבעיה הזאת? אז כדי לטפל בבעיה הזאת, מה שאנחנו צריכים זה קודם כל לעשות איזה נרמול של הערכים, אוקיי? אז מה שאנחנו יכולים לעשות, למשל, זה קודם כל לעשות מרכוז של הערכים, כלומר שהאמצע יהיה ב-0 ולא ב-3, זה הדבר ראשון, והדבר הזה גם יאכשר כמובן דירוגים שליליים, ו למה חילקנו את זה ב-3? כי זה מספר סרטים שראו פה, אני חושב.

הדוגמאות האלו, כן, אני צודק. אוקיי, אז לכן d בעצם נתן דירוג ניטרלי, וזה עכשיו 0 בגלל זה, אז ל-0 כאן יש לנו משמעות הרבה יותר חזקה. בנוסף הדבר הזה גם קצת מצמצם לנו בדמיון, ואנחנו נוכל לקחת רק, אם אתם זוכרים, עשינו משהו דומה כזה לגבי רנקינג ב-tfidf, שלקחנו רק על טוקן עם שופי ובשילטה, אז גם בהשוואה בין היוזרים אפשר לעשות משהו דומה, ולקחת אולי רק את הפיצ'רים המשותפים, גם מחשבות של משהו שאפשר להשתמש בהם.

אוקיי, אז זה כבר נותן לנו, אם אנחנו רואים, משהו שנראה הרבה יותר הגיוני, כי אם אנחנו מפעילים מספר חיובי במספר שלילי, המספר השלילי בעצם זה משהו שלא אהבנו, ומספר חיובי זה משהו שאהבנו. אם שני הערכים הם בין 1 ל-5 זה מאבד את כל הטעם בדבר הזה. אוקיי, אז בעצם המרכוז כבר עונה לנו, כאן יש לנו את הטכניקה השנייה, כשאנחנו משווים בין דברים, נשווה רק ברכיבים המשותפים, אז זה בעצם נותן לנו את ההתמודדות השנייה, ובסופו של דבר אנחנו עושים משהו שהוא דומה קצת, נקדם איתם של פירסון, אז הגענו למקום הזה, אז בואו נעשה איזה שהוא בעיה נוספת שיש לנו כאן, זה התמודדות עם הערכים חסרים, עם זה אנחנו עדיין לא התמודדים.

אז איך אנחנו נטפל בבעיה הזאת? אז אחת הדרכים להתמודד בבעיה הזאת, זה להעריך מה הציון. אם אנחנו נעריך מה הציון, אז כן, אנחנו יכולים לתת הערכה לפי מקומות שבהם כן היה ציון, נגיד לפי יוזר 6 ולפי יוזר 3, ואיך אנחנו נמדוד את הקשר בציונים יחד אליהם, זה קודם כל גם פה חשוב לעשות את הנרמול, מאוד חשוב, ולתת לנו גם, אנחנו רוצים גם לתת איזשהו משקולת שונה שיהיה לנו לכל אחד מהתוצאות שאנחנו משווים אליהם. אז איך אנחנו עושים את המשקולת הזאת? אז אנחנו עושים דמיון קוסינוס, אבל בשיטה שאמרנו כאן עם הרכיבים משותפים בלבד, בין שתי יוזרים, ואז ניתן משקל לפי הקרבה ליוזרים, שזה לדעתי מה שאחד מכם אמר פה קודם.

אוקיי? כלומר, אנחנו עושים ממוצע משוכלל לציון לפי הקרבה בדמיון קוסינוס בין הוקטורים לא לתת אותו משקל לכל אחד מהיוזרים, ורק לעשות ממוצע, אלא ככל שאותו יוזר יותר קרוב אלינו, אז ניתן לו משקל בהתאם לפי הדמיון קוסינוס. בואו נראה אם יש לנו פה עוד חידושים שאנחנו חשבו שנשלים. אז בזה פחות או יותר זה לא היה כניסה עמוקה מדי, אבל עשינו כזה דריף קצר לשתי שיטות של Recommender System, אחד שהתבססה לפי הדמיון פיצ'רים שונים. 

כאן ראינו פיצ'רים שהם לאו דווקא מבוססי טקסט בהכרח. אפילו עם שמות של שחקנים יש בזה string, אבל זה לא טקסט במובן שהכרנו קודם, של טוקנים. ויש לנו גם דמיון של יוזרים למשל, בעיקר לפי הרנקים שהיה להם כאן.

הראשון זה היה content-based, השני זה היה collaborative filtering. אז עכשיו כאיזושהי פתיחת סוהר לתחומים הבאים, אנחנו לא נדבר עדיין בכלל על הקשר, עדיין נחזור מידע, אבל אני רוצה בזמן שנשאר לנו, רק להציג קצת שתי דומיינים שאנחנו רוצים להרחיב גם אליהם. דומיין אחד זה הדומיין של תמונות, אבל לא סתם תמונות, תמונות כמו שאנחנו רואים אותן, אנחנו מין האנושים מאוד אגסנטרי, אז הכל בעין האנושית.

אז human vision, אנחנו רוצים לעשות computer vision שיודע לענות על הcomputer vision, כלומר על human vision, כלומר לתת לאנשים מה שהם היו מחפשים. אוקיי? אז נדבר קצת על זה וממש על הבסיס של איך מחשב רואה תמונה ואם נספיק גם קצת על אודיו. על איך מה זה מידע סיגנל של שם. 

אוקיי? יש לנו פה שני סוגים של סיגנלים, סיגנל של תמונה וסיגנל של שם. אוקיי? אז איך נראה בעצם המידע הזה אצל בני אדם? אז הנה תקריב של העין האנושית. אוקיי? אז בעצם בשניהם אפשר לחלק בצורה מאוד גסה את הקליטה של המידע שלנו.

אוקיי? כידוע יש לנו גוף ויש לנו כל מיני חושים שהעיקריים ביניהם, שחלקם יש גם redundancy בחלק מהחושים שאנחנו לא מודעים עליו בכלל. אז העיקריים ביניהם אנחנו נוהגים להגיד שיש ראייה, שמיעה, טעם וריח ומישוש. יש כמובן עוד חושים אבל זה העיקרים שמתייחסים אליהם.

אז בעצם איך חוש הראייה עובד? אז ככה נראית העין. כידוע מרכז של העין כאן זה הישון. בעצם האור נכנס דרך הישון.

בשעות היום אנחנו רוצים חשיפה מינימלית לשמש ולכן הישון מתכווץ. בשעות הלילה יש לנו מעט אור אז רוצים חשיפה רחבה ולכן הישון מתרחב. יש לנו כל מיני שרירים מאחורה שמפעילים את העין.

ובעצם מה שמחבר את העין לתמונה שבסוף אנחנו רואים זה הנוירונים שלנו. העין בסוף מעבירה את הסיגנל של אורחי גל השונים שאנחנו רואים בעיניים דרך נוירונים לחלק האחורי של המוח כבר אנחנו נראה את כל החיבור הזה. אז יש לנו אישון.

המידע עובר ומוקרן על ההדשה בצורה הזאת שכידוע יש לנו שתי סוגים של סנסורים כאלה את העין. יש לנו סנסורים מיוחדים ליום וללילה. אוקיי? אז יש לנו על העין בזורים קונוסים וגלילים כאלו שהראייה ביום היא הרבה יותר חדה, הרבה יותר גם צבעונית כידוע וכדי לראות בלילה בגלל הפושי והמחסור אז הראייה שלנו בלילה היא גם מי שמכיר מבוססת גם על תנועה יש פה אני לא אעבור ואיתמק זה בכל זאת לא קורס בביולוגיה אבל יש לנו ממש הפעלה שרשרת ככה של דברים של מה שקורה אחרי שהאור מוקרן לנו על העין ומה העין שאנו בעצם רואה? כידוע אנחנו רואים שלושה אורחי גל ספציפיים שהמוח שלנו מתרגם אותם לשלושה בסיסי צבע אבל מדובר בעצם באורחי גל מה שאנחנו מכירים כצבע זה תרגום של המוח שלנו לאורחי גל האלה כן מדובר כמובן על ה-RGB על ה-Red, Green ו-Blue ויש לנו לכל אחד מהאורחי גל השונים האלה את החרוטים כל האורחי גל השונים גלאים שונים בעין שאורחים נקלוט את אותו אורח של גל זה הסיבה בסוף אם נרצה הצבע שאנחנו רואים זה קומבינציה של שלושת אורחי הגל האלה אז נגיד צבע לבן הוא פשוט אומר שיש לנו הרבה מכולם הרבה משלושת אורחי הגל האלה אבל דווקא הראייה הצבעונית זה הקומבינציה שהמוח שאנו עושה מעורכי הגל השונים מכמה אנחנו רואים כל אחד מהם האות הזו שאנחנו רואים מעובר על ידי הנוירונים לויז'ואל קורטקס שנמצא בחלק האחורי של המוח כאן אוקיי? זה חלק מאוד מורכב עכשיו, אם למדתם למידת מכונה, אז בעצם האזור הזה עושה ממש קלסיפיקציה לכל מיני... אפשר אגב לראות עד כמה המוח עושה את זה דווקא כשאנחנו טועים בכל מיני דברים, למשל אם יצא לכם פעם, אולי ראיתם בן אדם שאתם הייתם בטוחים שאתם מכירים אולי אפילו היווחתם את עצמכם, נופלתם לשלום או רצתם לכיוונו, משהו כזה אז יתברר שזה זיהוי לא נכון אוקיי? אז מה שקורה זה שהמוח שלנו מנסה להתאים את התמונה שהוא רואה לתמונות שהוא מכיר כאילו אז אם זה אנשים שאנחנו מכירים אז הוא בודק אם זה מספיק קרוב אז הוא עושה את הזיהוי, זה בן אדם שאתם מכירים ואז קורה הדבר הזה אוקיי? של זיהוי מוטרי אז איך התמונה בעצם מתגלגלת על המוח? אז אנחנו יודעים שבעצם יש לנו גם ראיית עומק, יש לנו בעצם שתי תמונות שמועברות, אחת בעין ימין ואחת בעין שמאל בעצם בערך באזור הזה במרחק הזה בערך אנחנו מצטלב והתמונה שמגיעה מפה הולכת לעין ימין והתמונה שהולכת מפה מגיעה לעין שמאל אז אם אנחנו מתקרבים יותר מדי העיניים מתחילות לבזול בגלל זה כי זה המרחק בעצם שקורא ההצטלבות הזאת בערך ואז המוח מרכיב לנו תמונה תלת מימדית מאותן שתי תמונות שהוקרנו על כל אחת מהעיניים והדבר הזה עובר דרך הנוירונים לחלק האחורי של המוח שלנו לוויז'יול קורטקס אז כל זה הייתה הקדמה מה נעשה אצל בני אדם קצת קצת קצת ממש בקטנה על מה נעשה במחשב איך אנחנו בעצם גורמים למחשב לראות תמונה תחום שמאוד פרח שהביא בסוף גם לפריחה של בינה מלאכותית ולמידה מכונה, למידה עמוקה אז זה בעצם כל ההתעסקות עם הגרפיקה הרצון למכור מחשבים לגיימרים שאוהבים איבוד מאוד חזק בגרפיקה אנחנו בעצם מדברים על מטריצות שמייצגות פיקסלים אז כאן אנחנו רואים בעצם מטריצה של ארכיאה פורמה שנקרא Gray Levels זו מטריצה דו ממנית שכל פיקסל הוא בעצם מייצג מקום מסוים במטריצה מקום מסוים שאולי אנחנו רואים במסך, בתמונה ויש לו ערכים בין 0 לצבע שחור ל-255 לפעמים מנרמלים את זה בין 0 ל-1 בין 0 ל-255 זה צבע לבע זו תמונה שאין לנו פה זה לא תמונה צבעונית, זו תמונה של ארכיאה פור כבר אפשר לזהות שמדובר למרות שהרזולוציה ממש גרוע שמדובר בבן אדם יש לנו פה את העישונים שהם בצבע שחור באמצע זה אפשר קופץ בצד שמאל, בצד ימין הם מספרים אחרים מספרים נמוכים יותר קרובים לשחור גבוהים יותר קרובים ללבה מה שיש בצד ימין זה המטריצה הצבע שאנחנו רואים זה אנחנו רואים את הצבע כל זה עבור תמונה בגווני אפור איך אנחנו מייצגים צבע? בשביל להציג צבע יש כל מיני דרכים אבל נדבר כרגע רק על rgb למרות שזה כנראה לא הדרך האופטימלית להציג צבע אז הסלחו לי כל מומחי העיבוד תמונה יש דרכים יותר טובות יש לrgb כל מיני חסרונות של אין מספרים שלילים וכל מיני דברים כאלו אז מה שאנחנו עושים זה במקום להציג פיקסל עם ערך אחד שהוא גווני אפור בין 0 ל255 אנחנו נציג כל פיקסל עם שלושה ערכים אחד בשביל הרד אחד בשביל הגרין ואחד בשביל הבלו ואז הדבר הזה יכול לייצג צבע אז כך נוכל להגיד שהצבע הירוק נגיד אז ירוק הכי כהה יהיה עם 255 ברכיב של הירוק אבל 0 ברכיב של האדום והכחול לבן כמובן יהיה עם ערכים גבוהים בכל הרכיבים שחור בערכים קרובים ל0 בכל הרכיבים הכחול המובהק יהיה רק ערכים גבוהים ברכיב הכחול והאדום מובהק רק ברכיב האדום אוקיי? אז איזה משימות אנחנו עושים בעיבוד תמונה אז זה יהיה משימות של כאן יש לנו משימות של למידת מכונה יש לנו למשל זיהוי אובייקטים אוקיי? יש לנו פה או משהו שדומה לפונקציה של סופטמאק שהזכרנו קודם למשל, נותן את הוצר כזה שכל הקטגוריות האפשריות מקבלות משהו שנראה כמו הסתברות כזה בין 1 ל-1 זה כמו הסתברות, זו לא באמת הסתברות, אבל כן משהו שנראה כמו הסתברות כן, בסדר, זה משארך את ההסתברות יש לנו, וואו זה ממש גרוע, זה לא ריבוע סביב הכלב זה קיווץ את זה במצגת הזאת בקיצור זה אמור לתת גבולות של האובייקט כאן יש גם לזה כל מיני דרכים להגיד מה הפיקסל העליון השמאלי נגיד, ואז מה הרוחב ומה הגובה של האובייקט ועוד כל מיני, אני אדלה כאן עוד כל מיני משימות כאן, אחרי זה יש לנו כל מיני קשיים מיוחדים אז כל הדבר הזה היה איזושהי הקדמה על מידע מסוג תמונה עכשיו ניתן רגע הקדמה על מידע מסוג שמה, כן ומה האתגרים שלו, אז גם לגבי התמונה אמרנו את זה יש לנו שתי אתגרים בעצם האתגר התפיסתי והאתגר הגולמי הפיזי אז התמונה שמגיעה לקצה של העין ומוקרנת על האישון זה, סליחה, על ההדשה, זה בעצם המימד הפיזי אבל אחרי זה העיבוד של כל הדבר הזה שנעשה בסוף הוא מידע תפיסתי שהוא נעשה בעונה האחורית בקורטקס הויזואלי אז באותו אופן גם למידה של השמה יש שני הצדדים את הצד הפיזי ואת הצד התפיסתי וכשאנחנו רוצים לחכות את המנגנון הזה שנחשב שעובד מצוין הצל אנשים, אז בעיבוד תמונה זה ממש עבד כניסיונות של חיכוי של מה שלמדו ויודעים על האוזן האנושית אז כך נראה המנגנון שעובר אצלנו הכל מגיע דרך האוזן החיצונית, דרך האוזן הפנימית ובסוף מגיע לקורטקס שממוקם לא רחוק מהאוזניים באיך כל האזורים האלו כאן אז הנה ככה תמונה של מבנה האוזן יש לנו פה את האוזן החיצונית, יש לנו תעלת השמה שבסופה יש את אור התוף מכאן אנחנו מגיעים לאוזן הפנימית אז יש לנו את העצמות הקטנות הפנימיות האלו וזה מגיע אחרי זה לכוחלה הדבר הזה שנראה כמו שבלון ומשם לאוזן הפנימית שבעצם זה המוח אז איך הדבר הזה עובד? אז יש לנו שערות קטנטנות כאלו בתעלת השמה כבר כשמגיע גל של קול זה מתחילות לרטוט ככה לא סתם נקרא אור התוף אור התוף כי ממש כאילו מטופפים עליו ואז בעצם התפקיד שלו זה לקחת את הדבר הזה שאמור לייצג את הסיגנל של השמה להפוך אותו, הוא ממש עובר טרנספורמציה לאיזשהו תדר המוח שלנו עובד כאילו עם תדרים לא עם הסיגנל המקורי אלא הוא עושה לזה איזושהי התמרה אוקיי? אנחנו נדבר קצת על ההתמרות בהמשך אז אני קצת אדלג על הפירוט פה יש פה באמת להעשרה פירוט על איך השמה עובדת אצל אנשים ואיזורים שונים בתעלת השמה עוד בעצם מיועדים לקלוט תדרים שונים של שמה אוקיי? נדבר עוד על תדרים אבל כשאנחנו אומרים תדר גבוה הכוונה היא שהסייקל שלנו קצר כמו שיש לנו בצד שמאל רואים שהמעבר בין שלב לשלב הוא הרבה יותר צפוף וכשאנחנו אומרים תדר נמוך אז מתכוונים בדיוק להפך שעד שיש חזרתיות עובר הרבה זמן אז אנשים וגברים גם למשל זה אחד מהדברים שאפשר וחכו ממש רגע אולי אני אעביר כמה משימות ואז נסיים אז למשל אנחנו יודעים שלגבר יש תדר נמוך יותר בפיץ' של הקול שלו אוקיי? אז אם אנחנו מעבירים שתי סיגנלים נגיד אחד של גבר ואחד של אישה ומתרגמים את זה לתדרים אז השיטה שאנחנו יכולים לזהות היא מדובר בגבר או אישה אוקיי? אז זה כבר יכול להיות משימה אחת לא לעשות את זה לבד יש לנו חבילות שעושות את הדברים האלו ויש לנו גם בנאמפאי אבל גם דברים יותר ספציפיים שהתפקיד שם זה לגלות באיזה תדר מדובר אם מדובר בגבר או באישה אז לקחת ככה כמה אינפוטים ולנסות בעצם לתת את האוטפוט של התדרים שיש לנו מאינפוט של שם אוקיי? אז אני אתן עוד כל מיני משימות אחרות של גם עיבוד תמונה קל וגם עיבוד של שאלות ותמונות לפי מערכות חוקים כאלה שאתם יכולים לראות במצגת ושל recommendation system בין שתי אובייקטים כאלו שראינו קודם אז זה יהיה בערך נראה לי סביב ארבע מטלות כאלו להגשה לשבוע הבא אם אתם רוצים ואני אכתוב את זה כמובן קראים אוקיי? עוד שאלות לפני שמסיימים? אז עד הפעם הבאה מה? הרבה כיף כי...