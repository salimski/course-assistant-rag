יש לנו כל מיני תהליכים נוספים שאפשר להוסיף גם לפריפרוססינג אז אפשר לראות פה עוד תהליכים נוספים גם טיפול שדורש עוד האינדקס עצמו בשגיאות כתיב בהגיעה שונה קצת של מילים אוקיי, גם את הנושאים האלו הזכרנו אז בשלב הבא יש לנו Converted Index, בעצם כך זה יהיה מושג בשביל מודל בוליאני בעצם יש לנו הטרמים השונים זה המילים האטומיות, לאו דווקא מילים המושגים האטומיים שאותם אנחנו נהנדק זה יכול להיות רצפות טיעות, זה יכול להיות רצפי מילים, מילה בודדת אם נרמול, בלי נרמול דיברנו על כל מיני מארג של אופציות וכל אחד מהטרמים האלה מצביע על כל המסמכים שהוא נמצא פה זה ה-Inverted Index שלנו והתצורה הזאת מתאימה למודל בוליאני כי אנחנו גם לא כותבים כמה פעמים זה מופיע בכל מסמך גם לא כותבים את המקרומים אתם זוכרים שדיברנו בסוף שירות שעבר על Positional Indexים ששם מזכירים גם את המקרומים של כל אחד מהם מאפשר לנו שילטות מורכבות יותר ובעצם הצד השני של המודל הבוליאני זה קודם כל לנתח את השילטה השילטה מורכבת מאופרטורים AND, OR ו-NOT אוקיי? אני חושב שזה יצא קצת מטושטש אבל בכל מקרה יש לנו במקרה הזה ביטוי של NOT זה אומר שאנחנו מוצאים לצורך העניין כאן יש לנו את ג'ולי אז מוצאים את כל המסמכים שג'ולי מופיע בהם אבל אנחנו בעצם מנפים אותם או שמראש אם רוצים אפשר גם, לא כל כך יעיל אבל אפשר גם להנדץ את כל הדבר שכל ביטוי לא מופיע בהם כנראה לא הכי יעיל לעשות את זה אז אנחנו מוצאים את כל הביטויים שלא מופיע בהם ואותם אנחנו חותכים עם איזשהו ביטוי נוסף אם היה לנו AND ל-OR הנה כאן יש לנו דוגמא על ביטויים אחרים של שפה יותר פשוטה באנגלית ולא מחזות של שייקספיר אז כאן יש לנו למשל cat and ice ו-cat and afraid אוקיי? יש לנו... למדתם פה הקורס בלוגיקה? אוקיי? אז אתם בטח מאוד מאוד אוהבים ביטויים כמו קוניונקציה זוכרים את זה? למדתם מהם? מוכר לכם? לא? conjunction ו-disjunction? לא? לא מוכר? אוקיי יש לנו... איכות וחיתוך? איכות וחיתוך? יש לנו איכות של חיתוך או חיתוך של איכות? זה הקוניונקציה ו-disjunction כלומר, לצורך העניין אתה אומר תביא לי את הטרם הזה ועוד הטרם הזה ועוד הטרם הזה או טרם אחר ועוד טרם אחר ועוד טרם אחר אוקיי? זה הקוניונקציה אוקיי? אז כאן יש לנו איחוד על כל מיני ending קל לזכור שיש פה סדר קדימויות בלוגיקה בטח לימדו אתכם את זה שהופכים את החיתוך לכפל ואת האיחוד לפלוס ואז יש לנו סדר עודיפויות בדיוק כמו פעולות חשבון אז זה בעצם השיטה שיש לנו כאן ואני רואה ש... אולי אני מסתיר את זה כאן רואים את זה שם, יש לנו כאן בעצם שתי אפשרויות או לקחת רק את מסמך אחד שהוא עונה על השאילטה של cut וnice cut וגם nice ואז השאילטה של cut וגם afraid המסמכים d3, d4 וd5 עונים עליו אוקיי? צריך ש... אנחנו עושים ending אז שכל הביטויים האלה יהיו אחדים ואז אנחנו עושים or על זה ועל זה כלומר אנחנו מוסיפים את כל המסמכים ביחד אוקיי? יש פה עוד שלב שקצת דילגנו עליו שאנחנו נדרשים להוסיף את זה למיינסט אולי כי יכול להיות שאם יש לנו קוניונקציה מהסוג הזה של or על כמה וגם אם אז בעצם המסמך מסוים יופיע גם בפסוקית הראשונה וגם בשנייה הוא יענה על כמה האפשרויות ולכן צריך לוודא שיש לנו רק unique document בסך הכל שלא יהיה לנו נגיד כמה הופעים של d2 אוקיי? אז אחרי שהדבר הזה מחזיר מסמכים צריך רק לדאוג שלא משנה באיזה שיטה עושים זה מבנה נתונים שלנו או האלגוריתם שלנו נותן לנו זה היה בעיה שכיחה בגלל זה אני מציין אותה שהיו מחזירים עושים ככה or על כמה אופציות ואז היו מחזירים אותו מסמך כמה פעמים זה כמובן לא משהו רצוי אוקיי? אתה רוצה רק תוצאה אחת unique ולא כמה הופעים שלה זה ככה שלב בסוגיים שמוזכר אפילו לא שמתי אותו בשקפים אוקיי? אז בשלב הזה אנחנו נעשה כל זה זה בוליאן רטריבל יש לנו פה בעיות שונות לגבי בוליאן רטריבל שאנחנו נענה עליהם עוד מעט ביניהם שהוא מאוד מאוד נוקשה אוקיי? מה קורה? תחשבו על שילטה הרבה יותר עמוקבת מזה ויש לנו נגיד הרבה מילים וחלק מהם מופיעים חלק מהם לא מופיעים מה לגבי אפילו אם משתמשים במודל בוליאני אם כל אחד ישתמש פה במנוע חיפוש בוויב יודע שמעבר לכמה תוצאות הראשונות אנחנו בדרך כלל לא מסתכלים עליהם כי אנחנו נואשים מאוד אנחנו מצפים שהתוצאות הרלוונטיות יהיו נגיד בחמישייה הראשונה והייתי נחמד אולי זה אפילו בתוצאה הראשונה או השנייה אוקיי? אז אין לנו מענה לא על זה ולא על זה אין לנו שום גמישות בפתרון ואין לנו רינקים שיטה לעשות דירוג בין התוצאות אוקיי? אז זה משהו שאנחנו צריכים לפתור אבל לפני שאנחנו נגיע לזה אז אנחנו נעשה עכשיו את ההצגות שזה אומר זה זה אוקיי? אז כאילו אני יכול להזכיר אבל יש פה מישהו שטען רשום שעושה ביחיד אבל עושים את זה בזוג או משהו שמציגים אין לנו משהו כזה אוקיי? אז אתם זוכרים יש לנו אני לא רואה אבל אתם זוכרים יש לנו את זה לא נכון בכלל יש לנו שני מסכמים בסדר? זה מסגר אתם רוצים להתחבר מהמחשב שלכם? כן אוקיי? אז ה-HTMI מספיק לנו? אוקיי, איך אני אתחיל? רק שנייה על מה אתם עבירים? על פרט הסטמר סבבה אני לא אתחיל ישר מהקוד כי יש פה איזושהי לוגיקה מאחורי הקלעים כאילו להעמיר אותה לקוד למשהו אחד להבין אותה למשהו אחר אז קודם כל לפני שאנחנו מתחילים יש מלא מירים שבשביל שנעשה את הסטמר אנחנו נצטרך להבין אם הסיומת שלהם זה באמת סיומת של המילה או שזה סופיקס כלשהו אז בעצם קודם כל אנחנו בודקים לפי איזשהו אלגוריתם האם זה שייך כאילו למילה או שזה סופיקס נוסף או שזה רבים ריבוי של מילים ואחרי זה אנחנו באמת עוברים לבדיקה והורדה של הדברים האלה אז כלומר M1 וM2 זה כנראה מצביע לנו על שינוי מילים כלשהו בסופיקס ואם אנחנו לא עוברים את M0 אנחנו כנראה לא נרצה להסתמן כי אנחנו מניחים שזאת מירה שלמה ביחד ואז כשאנחנו עוברים לקוד אנחנו בעצם עושים איזשהו מיין עם מילים לדוגמה מן הסתם ובונים כשישה פונקציות כל פונקציה אחראית על חלק אחד מההורדה אם זה 8 כל הדברים שיש SS I, SS בצורות אחרות ובמקביל אנחנו בודקים את הM גדול מ-0 אז יש פה בעצם ארבע פונקציות שכל אחד מהן אחראית על משהו אחד שהיא עושה ממש כאילו כזה כתבתי בקומינסט כזה אם מישהו מעוניין לקרוא ואז אנחנו באמת ממשיכים לפונקציות הקטנות של לכל אחד יש פונקציות שונה אם זה להוריד S, E, S ו-I אם זה להוריד E, B ו-I וככה אפשר להמשיך בעצם בכל אחד מהם אם זה I, אם זה 8, I אז כאילו אני לא הולכת לבוא עכשיו אחד אחד על זה כי זה נראה לי מיוצר זה אמן חוקי, כן אבל זה פשוט בעצם להגדיר את החוקיות לפי האלגוריתם של פורטר ובאמת אנחנו מריצים פשוט על המילה אחד אחד אחד את הדברים שקורים פה ואו זה איזשהו הרטרם לכל העוסף שעשינו פה לדוגמה מן הסתם אפשר להשים את זה עם קובץ מה שבא לנו זה פשוט דוגמה קטנה צעצוע וזהו וככה זה יוצא אוקיי נראה יפה מאוד כל הכבוד שאלות לגבי פורטר סטנר מישהו רוצה לשאול אגב אני אגיד בסוגריים אמרתי את זה, כל מי שמוכן זה לא מופנה רק אליי כל מי שמוכן להעלות את העבודה שלו ככה שאחרים יוכלו להסתכל עליה אז אני אשמח ככה אם נוכל לצרף את העניין הזה ואז סבבה אז בבקשה זה נראה לי בHDMI אבל צריך להסגר את זה הכבל לא לוקח עוד קצת זמן לביטים לערוך כן איך שאתה רוצה להקדים אתה תנחה את הערוך טוב בואו אנחנו מתחילים הושיגים הצגה שנייה עכשיו אז מה שעשיתי בעצם זה לקחתי ויצרתי טקסטים נקודה טקסט כמה ארבעה טקסטים אם אתם מכירים מפורגם מהלברית ויצרתי כל כמה שורות כל טקסט אחד מה שבאמת פשוט ביותר ובאמת ברמת הקוד זה לא רגע לראות נגיד אחד מהטקסטים סתם להתרשם לא זה ברמת השורות זה טקסט פשוט באנגלית כן הנה טקסט פשוט באנגלית זה ממש גם מאתר באתי ולקחתי עכשיו ברמת הקוד אנחנו בעצם יוצרים בסופו של דבר דיקשינרי ואנחנו יוצרים פה פונקציה של tokenize שזה נחלק את כל מערך של מילים כל הטקסט בסופו של דבר מחלק את זה למילים בנפרד ויוצר מערך של מילים בודדות שלא חוזרות אחד על השנייה כלומר אם יש לי דיס פעמיים באותו קטע יש לי דיס פעם אחת במערך הזה עכשיו מעבר לזה זה פשוט לעזוב ולעבור באמת על כל קובץ לפתוח אותו כמובן לקרוא אותו ולעשות עליו tokenize ומפה בסופו של דבר לעבור על כל מילה כזה וליצור את הinverted index כאשר עבור כל מילה מסך המילים שיש לי ב-array אני יוצר inverted index עם אותה מילה ואני מוסיף בסופו של דבר את השם של הקובץ ובאמת זה התוצאה שבאמת בסופו של דבר אלה כל הקובץ כפי שאתם רואים זה מופיע גם בטקסט 2 וגם בטקסט 3 אם נלך קצת לדברים שהם מילות ייצור כמו לדוגמא off, t1, t3, t4 עכשיו על זה בסופו של דבר על התוצאה הסופית אפשר לראות ולעשות חישובים כמו or, end כמו שעכשיו אמרתי אז תודה רבה אוקיי אז קודם כל אז גם לך אני אגיד אני אפתח אולי איזה תירה יהודית לזה או משהו אבל סבבה אבל אני לא רוצה להעלות בלי אישור שלכם עבודות אז זה דבר ראשון דבר שני ככה כמה הערות התרגול שעשינו אמרנו הוא היה כפול קודם כל התחלנו בזה שיש לנו פה איזשהו מאמץ לעמוד ולדבר שהוא לא הוא לא פשוט תמיד לכולם וחשוב ככה לשבור את הקרח אז עצם המאמץ הוא מאוד חשוב שהעבודות כמובן מתחבכות לשנייה חוסר היכולת שלי לעשות דברים במקביל אוקיי אז רק נגיד ש בתור התחלה אנחנו יכולים לשלב ראינו שיש שם כל מיני מילים היה אפשר להריץ עליהם נגיד את הסטמר ואז להנדקס את המילים שהם סטמפ שנייה עוד כמה הערות אנחנו יכולים מעבר להתרגל לעשות את זה כAPI מאוד גנרי יש לנו פה רכיבים שחוזרים על עצמם בהחזור מידע אז אנחנו מדברים על איזה API כללי יחסית לא נדבר על משהו שמתאים לזה אבסטרקטי כמו בפורס של Object Oriented או הנדסת תוכנה או משהו אלא משהו מאוד מאוד כללי שיש לנו אינפוט וארטפוט נגיד של סטמינג ואז אפשר להחליף את הקופסה הזאת בפונקציה של סטמינג אחרות למשל וכדאי אחרי זה כשנעלה פה את העבודה שלך נגיד לראות את הדוגמה של איך מתחברים לגוגל דרייב שוב גוגל קולאפ זה כלי של גוגל גם למי שאין חשבון ג'ימל כדאי אפילו בשביל הלימודים לעשות חשבון גוגל ואתם יכולים להעלות לשם את העבודות שלכם ואז לתת לו גם להתחבר לקבצי דאטה בדרייב אז כדאי ללמוד איך עושים את זה בפועל עוד איזשהו דיוק קטן כמה דיוקים קטנים לגבי חלק מהדברים אז זה נכון צריך למיין כמו שראיתם שנעשה פה את נגיד ספקסס שמורידים את הסיומות שמורידים בצורה כזאת שהפתרון הכי כללי הסיומת הכי כללי כמו נגיד מילה שמסתיימת ב-S היא תהיה האחרונה וקודם כל אנחנו ניקח פתרונות נגיד כמו ES שמוכלים בתוכה נעשה את זה לפי הסדר ממוין בצורה כזאת כדי ש זה יהיה אלגוריתם לא כתבנו את החוקים אבל החוקים שאנחנו רוצים שהם יהיו יותר ספציפיים מתפסו ראשון כמו שראינו מבחינת האינדקסים היינו כדאי שנשים לב שאם אנחנו ממש רוצים לארגן מערכת שהיא אולי קצת יותר יעילה היינו שמים מראש איזשהו כנראה מערך גדול עם שמות הקבצים ואז כשאנחנו עושים קבצים שמצביעים אליהם רושמים את האינדקס בתוך המערך במקום את שם המערך זה יכול להיות גם יותר מהיר ויותר חסכוני ובנוסף היינו עושים את זה מי שמכיר על פעולות של סטים היינו עושים את זה גם בתור סט בטח שאם זה מודל בוליאני במקום לשמור את זה במערך אנחנו שומרים את זה בסט מה שמקנה לנו מעבר לזה שזה שומר עותק אחד אבל זה לא מעניין סט אפשר מה שאי אפשר לשנות זה טאפל טאפל זה כמו קונסט ליסט אז סט מה שתועלת נוספת זה עכשיו יש לנו שתי טוקנים ואנחנו רוצים לעשות end ביניהם יש לנו שתי סטים ויש לנו פעולה כזאת של חיתוך שבפעולה אחת אנחנו חותכים אותם אחד עם השני אז אם יצא לכם לממש מערכת יותר גדולה שוב שעושה inverted index אז כדאי לעשות את זה בצורה הזאת אז עד כאן כל הכבוד למציגים שאתם מאוד הטובה ונשמח גם אם יהיו ככה גיוונים בהמשך אני רגע אטען את המשך במצגת אוקיי אנחנו לא נעבור על יש פה יש לי פה מצגת גדולה מדי לסטופ שלנו אז אני אני בהפסקה עליה פשוט ערכתי אותה עוד ממש כמה דקות לפני השיעור בגלל זה אני מצטער שלא ראיתי אותה לפני זה אני בהפסקה על זה אוקיי כן עכשיו זה בסדר אוקיי אז רואים? בסדר אוקיי אוקיי כן אז עכשיו מי שמסכם בחלק א' זה החלק המרכזי אז בעצם הצגנו כבר בעצם משהו שנראה כמו צד שמאל מה יש לנו בעצם אנחנו מדברים עכשיו על מודל שנקרא Vector Space יש לנו כמה הצגות של הדבר אנחנו נחבר אותו בעצם מודל שיאפשר לנו לעשות ציון לכל אחד מה מסמכים ובתור התחלה אנחנו רואים דבר כזה יש לנו אתם רואים את המספרים שיש לנו כתב לה? רואים את זה? יצא בסדר? אז יש לנו כל מיני מסמכים וחלק מההכנה של מה שאנחנו עושים זה לספור כמה פעמים כל מונח מופיע במסמך אוקיי אז בעצם המסמכים יוצאים לנו מעין וקטורים שלא אין לנו מופעים בוליאנים של מופיע או לא מופיע אלא אנחנו מראש אומרים כמה פעמים מופיע מה הרעיון של הדבר הזאת לפני שדיברנו על הכנה של ציונים של מונח מסוים שמופיע הרבה פעמים במסמך אחד לעומת מונח מסוים שמופיע מעט פעמים במסמך אחר אז כנראה יש לו חשיבות יותר גדולה במסמך הראשון מאשר במסמך השני אבל לפרצה כזאת זה לפרצה מדוללת נכון נכון כן כן כן אמרתי בהחסקה אני אעלם אוקיי עכשיו בעצם מה אנחנו מודדים נגיד שיש לנו שאילטה של אני לא איתן להם השלטה יש בזה בעציות נכון יש לנו שאילטה של אינפורמיישן אינפורמיישן סיסטם רטריבל אוקיי או אינפורמיישן רטריבל סיסטם בעצם אוקיי נגיד שאנחנו עדיין במודל של בג אבור זה מה שאנחנו נמצאים עכשיו רואים פה את שלושת המימדים האלו בעצם אנחנו במודל של שלושה מימדים ובעצם בשלושת המימדים האלה שזה שלושת המילים האלה יש לנו וקטורים שאותם אנחנו מציגים על ידי כאן הם כתובים בתור מספרי מסמכים אוקיי ובנוסף יש לנו עוד וקטור נוסף שזה השאילתה אוקיי ובמרחב הזה אנחנו בעצם שואלים כן עדיין לא הסברנו איך אנחנו עושים את זה אבל אנחנו בעצם שואלים לאיזה מסמכים השאילתה הכי קרובה אוקיי מבחינה וקטורית עכשיו אנחנו מציגים במימדים שונים במודל שהוא מפשט מאוד את הזאתה כי הוא מודל באגב גורץ מציגים את כל אגב אפשר להציג ככה גם מודל בוליאני כן מה עשינו במודל בוליאני עד עכשיו הוא התבטא בזה שהוא בוליאני בכמה אופנים קודם כל ספרנו רק פעם אחת כל מילה היא מופיעה או לא מופיעה אוקיי זה מה שהראינו בהכנה שלנו יש לנו טרם ואז המסמכים שמופיע בהם בלי להגיד במסמך הזה הוא מופיע חמש פעמים במסמך הזה הוא מופיע עשרים פעמים וכולי אוקיי אבל בנוסף גם עבור שפה בוליאנית של שאילתה ששפה לוגית של שימוש ב-AND, OR ו-NOT עכשיו השימוש בדברים כמו AND הוא חייב להיות מאוד מאוד נוקשה מאוד סטריקט אוקיי ואם זה לא עובר בדיוק את הפילטר של כל המילים האלה אז פשוט הוא מחזיר לנו קבוצה ריקה של תוצאות וזה משהו כמובן שהוא לא רצוי אז החלופה לדבר הזה זה פשוט לבדוק אולי מעל סף מסוים איזה וקטורים הכי קרובים לשאילתה עכשיו יש פה בעיה של מימדיות, זה נכון? ואנחנו צריכים לפתור אותה אוקיי מן הסתם אנחנו נתייחס לטוקנים או לטרמים שמופיעים בשאילתה יותר מאשר לכל טרמים שלא מופיעים בשאילתה ובדרך הזאת הוקטורים שלנו לא יהיו במימדיות של כל הווקבלרי לצורך העניין אם כי זה אכן בעיה כל הסיפור הזה של התצוגה שהיא מאוד מאוד ספרס כלומר בעצם רוב הטרמים לא מופיעים ברוב המסמכים יהיה לנו קאונטר של 0 לצורך העניין ברוב הטרמים זה הציפייה ואנחנו רוצים לפתור אותה בשיטה הזאת שתהיה פחות נוקשה מצד אחד היא לא תציג לנו נגיד בצורה רחבה מדי כל ביטוי שאולי מופיע מתוך איזה שאילתה ארוכה וגם לא משהו מאוד ספציפי שרק ייתן לנו התאמה מלאה ומדויקת לשאילתה אלא משהו בין לבין שאנחנו נעזר בקרבה יש לנו פה עוד דוגמא של יכול להיות שנתנו אותה כבר של המחזות של שייקספיר מופיעים של מילים שונות אז אנחנו עושים פשוט קאונט של מילים שונות וכמו שאמרנו אז אם יש לנו איזושהי מילה נגיד שמופיעה נחפש שזה לא 0 בחלופות אז נגיד הנה נגיד ברוטוס מופיע 157 פעמים במחזה של יוליוס קיסר אבל הוא כן מופיע במחזות אחרים נגיד במחזה של האמלט אבל הוא מופיע רק פעם אחת אצל האמלט אז מן הסתם המסמך שיותר ייצג את ברוטוס זה המחזה שייצג את ברוטוס זה יוליוס קיסר כמו מאשר האמלט עדיין לא בצורה פורמלית אז היינו רוצים לענות שוב על שתי בעיות בעצם מה השלבים שיש לנו שאנחנו רוצים להגיע לשם בסופו של דבר רוצים גם אפשרות שהיא תפתח לנו קצת את השילטר של השילטות הבוליאניות שיתן לנו יותר תוצאות מצד אחד אבל מצד שני בעזרת הרנק שלנו בעזרת איכות ההתאמה זה גם יצמצם לנו תוצאות וגם יידרג אותן לפי סדר ההתאמה זה בעצם מה שאנחנו רוצים להשיג מכל הדבר הזה וואו מגיעים לשם אוקיי אז יש לנו פה תזכורת אנחנו עובדים עם מודל bug of words כמו שחלקכם הזכיר מה? סליחה אנחנו במודל bug of words איזה מבלבל כן כי השקופית הנוכחית הוא מציג לי בקטן ולא אז אנחנו עובדים במודל bug of words שזה לא הכרח המציאות ראינו גם אפילו באופציות הדומות לזה שיש לנו גם positional indexים ואנחנו נדבר אחרי זה שכבר חלקכם הזכיר על הקצת בעייתיות שיש במודל הזה מכל מיני בחינות אנחנו נגיע לשם אבל אנחנו עדיין במודל הזה של הצגות של המסמך בעצם מודל bug of words זה מודל שעוזר לנו להפוך את המסמכים לוקטורים מי שיש לו רקע פה חלקכם עשיתם קורסים בלימידת נכונה אתם יודעים שבעצם כל אובייקט אנחנו מציגים בעזרת וקטורים עם כל מיני משתנים מאפיינים אז כאן בעצם באותו אופן אנחנו אומרים עוד לפני ש כנראה עוד לפני שלימידת נכונה יחזור מידה יתפתח עוד לפני לימידת נכונה אז גם כאן היה לנו הצגה וקטורית של מסמכים על ידי המהירים שלה המילה או שוב by grounds על ידי הטרמים הבסיסיים מה שגדרנו כטרם בסיסי זה בעצם המאפיינים של המסמך ואז יש לנו דרכים לייצג אותו בתור וקטור ואיך אנחנו אחרי זה מודדים קרבה וקטורית זה כבר יש לנו כלים מתמטיים לעשות להגיד זה יותר קרוב וקטורית וזה יותר רחוק וקטורית וכמו שנתנו דוגמה אני אזכיר אז וקטור בעצם ברגע שהפכנו את זה לוקטור לצורך העניין יש לנו כאן הצגה של וקטורים נגיד של יוליס קיסר של המחזה יוליס קיסר אז כאן הוקטורים מוצגים בתור עמודות אז הצגה הוקטורית יש לנו כאן 1,2,3,4,5 6,7 אם לא טעיתי מימדים וברגע שאנחנו עוברים לווקטורים זה נכון שזה הגיע מאיזשהו מידול מסוים של המסמך בשביל להגיע לאותם מימדים אבל ברגע שהפכנו את זה לווקטור אז קצת מתעלמים מהמקור שזה המילים ועכשיו אנחנו משתמשים בכלים המתמטיים שאנחנו יודעים לעשות עם וקטורים. מי שלא זוכר אז הצגה כזאת אז אני רגע אפסיק את השיתוף כי הנקסטר זה קצת לא עובד לי צריך לעשות פה דופליקייט נגע מה? אני צריך לשיתף את השתף הזה בסדר אוקיי אז רק כתזכורת אנחנו מדברים פה על הדבר הזה אם נגיד שאני לוקח עכשיו סתם בשביל שיהיה יותר קל נגיד שאני לוקח את המילים האלו של ברוטוס ושל סיזר אז קיסר יש לנו פה בעצם במיוחד מי שלא מכיר את זה אז מה שאנחנו עושים זה דבר כזה כל אחד מה שיהפוך למימדים אחרי זה בוקטור של המסמך אז אנחנו נהפוך את זה לצורה פרמטרית למשתנים פיצ'רים עם מספר רץ שזה מתחיל מ-1 עד כמות המימדים שלה אוקיי אז יש לנו משהו כזה לא יודעת כמה זה אליינד אבל בערך אוקיי אנטוני הופך ל-X1 ברוטוס הופך ל-X2 סיזר ל-X3 וכן הלאה אוקיי עכשיו לצורך הדוגמה בואו נגיד שאנחנו מתייחסים רק ל-X2 ו-X3 אוקיי אז בעצם המחזה של האמלט אוקיי אז נוכל להגיד שהוקטור של האמלט הוא מוצג בתור 1,2 אוקיי כי לקחתי פה רק את X2 ו-X3 בתור מימדים אז כאילו כי הערך של X2 זה 1 והערך של X3 זה 2 לקחתי רק את X2 ו-X3 לתור דוגמה בעיקרון הוקטור של האמלט זה לא רק 1,2 כמובן זה 0,1,2 0,0,5,1 אוקיי יש לו שבעה מימדים אוקיי של המחזה האמלט ובעצם זה התצוגה הוקטורית של המחזה האמלט מה התצוגה הוקטורית של מקבץ נגיד זה 0,0,1,0,0,1,0 אוקיי אז מה יהיה תגידו לי התצוגה הוקטורית של נגיד אופלו 0,0,1,0,0 בסדר כולם מבינים עכשיו למה זה תצוגה הוקטורית הסברנו שמי שלא כל כך זוכר את התצוגה האלגברית הוקטורית הזאת אז בעצם אנחנו מדברים בגלל זה הצגתי פה רק שתי מימדים כדוגמה אוקיי אז נגיד שיש לנו אותה סקאלה לצורך העניין אז יש לנו כאן לא יודע כמה אני מדייק אז זה יהיה x1 או במקרה הזה x2 כי התחלנו מx2 וזה יהיה x3 זה אמור להתחיל מx1 זה סתם שיהיה דוגמה אז זה x3 לפי הסדר אז בעצם 1,2 זה יהיה הנקודה כאן ואנחנו יודעים שכל נקודה במישור זה וקטור אם אנחנו רוצים לקבל את הגודל של הוקטור מה שאנחנו קוראים נורמה פשוט מותחים ישר לראשית הצירים והזווית של הוקטור זה הזווית בין מה שהוא בדרך כלל ציר הx לווקטור הזה אז יש לנו פה וקטור עם נורמה ועם גודל ואיך אנחנו מחשבים נורמה של וקטור למדתם את זה בקורסי מתמטיקה למיניהם? l1 וl2 למדתם? אוקיי אז כולם מכירים dot product? יודעים מה זה? אוקיי אז בעצם אפשר להגיד ש מה שאנחנו עושים ב l1 זה לקחת את הסכום של הערכים בערך מוחלט זה l1 אז במקרה הזה l1 יהיה שווה 3 כי זה פשוט 1 ועוד 2 אין מה לעשות דרך מוחלט כי שניהם חיובים אז l1 זה 3 זה הנורמה 1 וl2 במקרה הזה אנחנו עושים פשוט אם נרצה dot product ומוצאים שורש שורש של dot product של הוקטור בעצמו או פשוט סכום של הערכים בריבוע ואז עושים שורש אוקיי אז במקרה הזה זה 1 בריבוע ועוד 2 בריבוע שווה 5 אז l2 שווה לשורש 5 בסדר? הבנו אותה חישובים שהן נורמות? l1 וl2 או שאני אחזור על זה אם שצריך? מה היא הנורמה נגיד של כל מקבץ? לפי l1 ולפי l2? לפי l1 ולפי l2 שורש 2. נכון כן? כי זה 1 בריבוע ועוד 1 בריבוע שורש 2 שורש 2 זה l2. בסדר? צריכה דוגמה מישהו או שאין צורך? זה נדבר על זה כשנדבר על מרחקים בין וקטורים אוקיי זה דרך משתמשים בזה אוקיי אז זה הרעיון אז בעצם יש לנו פה כלי בעזרת מודל של bag of words אז אנחנו ספרים את כמות הטרמים שמופיעים בכל מסמך בעצם למדתם איזה קורס הסתברות או משהו בעברכם? אוקיי אז יש דבר כזה שאנחנו קוראים לו תדירות frequency אוקיי בעצם אנחנו סופרים כמה פעמים זה מופיע זה עדיין לא הסתברות אבל לצורך העין בואו נניח שיש לנו בהסתברות אוכלוסייה סופית נגיד כמות מסוימת של מסמכים זה האוכלוסייה שלנו אוקיי יש לנו נגיד מאה מסמכים אז העין שלנו שווה למאה ועכשיו אנחנו או לא נכון מסמך אחד סליחה מסמך אחד מכיל אלף מילים נגיד ועכשיו אנחנו סופרים כמה פעמים מתוך האלף מילים מופיע טר מסוים בסוף זה יפוך להסתברות כי זה חלקי הסך הכל אבל כשאנחנו סתם מדברים על תדירות פשוט שואלים כמה פעמים מופיע אז מה שיש לנו כאן זה בעצם אקאונט הזה זה בעצם מספר המופעים שלנו וזה ההתחלה שלנו לאותו ייצוג וקטורי כמו שאמרתי אז יש לנו פה מה שנקרא טרמפריקונסי אנחנו שואלים כמה פעמים טר מסוים מופיע במסמך d מסוים אוקיי הטרמים אמרנו כבר מה זה זה היחידות הבסיסיות שיש לנו של טקס שלצורכם אני אגיד שזה המילים שלנו אבל כמו שאמרנו מיצת תוכן מים זה לא חייב להיות מילה זה יכול להיות שני מילים או פרוססינג על מילים וכדומה אוקיי אז בצורה גולמית אנחנו באמת מתייחסים לכמות המופעים ואם אנחנו רוצים משהו שהוא טיפה יותר מתוחכם אז יש לנו דרך לנעמל את הדבר הזה אוקיי אז למשל אנחנו אומרים דבר כזה זה ההיגיון שלנו אומר ורק זה בוא נחזור אחורה לכאן אנחנו אומרים שאומנם נגיד ברור לנו מאליו שברוטוס אצל יוליוס קיסר הוא מאוד דומיננטי באופן יחסי אבל באמת האם המופע ה-101 של ברוטוס לעומת המופע של 100 יהיה דומה למופע השני לעומת הראשון ואם עוד יותר נרצה להקטין את זה אז נשאל האם באמת המופע ה-0 שווה למופע הראשון אם זה לא מופיע בכלל עכשיו הוא מופיע בפעם הראשונה באיזשהו מסמך אז האם להפרש בין אי הופעה להופעה אחת זה להפרש בין 100 פעמים ל-101 פעמים אוקיי אז מה התשובה לדעתכם? זה כן מבחינתך זה אותו דבר לא, אני אגיד לך למה תגיד, אני אשמח אם אנחנו מדברים על הבדל בין 1 ל-100 אז זה רומנטי אבל הבדלים מפריץ את הזו בין 0 ל-1 הם הבדלים קטנים בטח שאני מסתכל עבור כל מילה ביחס לעצמה ביחס לחנות עבור כל שקל אין בעיה בפטרט אם אני מסתכל בין 0 ל-1 כנראה זה עדיין כמות מוחב ואם אני מסתכל בין 100 ל-101 אז זה עדיין כמות רבה אין ספק שהמספר הכי גדול הוא איזשהו מדד שזה חשוב יותר כן, אבל אם אני מסתכל כל המספרים הגדולים אז כולם באותה קתדורה וכל המספרים האחרונים כולם באותה קתדורה אבל הבדל, המשמעות היא אוקיי סבבה, אנחנו אומרים משהו דומה לזה, בעצם אנחנו אומרים גידול ליניארי של בעצם הכוח שאנחנו נותנים לכל טרם הוא מוגזם אנחנו רוצים משהו סב ליניאר מה שנקרא, אם יצא לכם לקחת כל מיני קורסים של לא יודע מבנה נתונים, חישוביות, אלגוריתמים וכדומה אתם מכירים סדרי גודל מצוין כבר, בטח שמחתם על זה מאוד, אז סב ליניאר מכירים את המושג סב ליניאר, זה כאילו משהו שהוא קטן יותר מליניארי נקרא סב ליניאר במקרה שהיה לנו אנחנו ניקח פשוט פונקציה לוגוריתמית כדוגמה, זה אחד מהבעצם משקול של טרמים הפופולריים אוקיי אפילו אולי אני צריך קצת להרוך את ה... אנחנו נשתמש אגב בלוג 2, כאן כתוב לוג 10, השקפים באופן כללי טובים אז לקחתי אותם אבל בדרך כלל נשתמש דווקא בלוג 2 לא משנה, נעזוב את הבסיס, אז אם אנחנו לוקחים לוג של הטרמים אז לוג היא פונקציה שהיא סב ליניארית זאת אומרת שאם אנחנו מדברים על הגידול בחשיבות הוא לא יהיה משהו כזה אלא כמו הפונקציה הלוגוריתמית הוא מתקדם מאוד מאוד מאוד לאט אוקיי עכשיו זה הרבה יותר קל להדגים את זה בלוג 2 אוקיי אם יש לנו לוג 2 של מספר מסוים אז מאיפה זה יתחיל להיות משמעותי ברגע שזה יהיה חיובי, ערך חיובי של המשקול שלה אוקיי אז מתי זה מתחיל להיות חיובי כשיש לנו כשיש לנו את הcount מספר המופעים שאנחנו נסמן אותו ב-tf term frequency שזה יהיה רלוונטי כאשר הפרמטר הזה הוא יהיה גדול משתיים נכון? אוקיי ברגע שהוא גדול משתיים זה יהיה רלוונטי אז אנחנו עושים כאן בדרך כלל כמה דברים האמת שהוא גדול שווה לשתיים זה מתחיל להיות רלוונטי אוקיי אז אם הוא שווה לשתיים לוג 2 של 2 כמה שווה? 1 נכון אוקיי אז מה שעושים זה בדרך כלל עושים 1 ועוד term frequency לזה עושים לו ככה אם זה מופיע פעם אחת אז הוא יקבל בעצם דרוג של 1 ולא דרוג של 0 כי לוג 2 של 1 זה 0 בסדר? אז אנחנו לא נחשיב פה תרמים שלא מופיעים בכלל במסמך כי אז יהיה לנו גם ערכים שליליים אוקיי לוג של 0 לא מוגדר בכלל אבל כן מעניין אותנו אפילו מופע 1 וכדי שזה יקבל ניקוד של לפחות 1 פשוט נוסיף לקאונט 1 ואז הלוג של 2 של 2 נותן לנו 1 אוקיי? אז זה בעצם הדרך שאת ה-1 במקום להוסיף כאן אנחנו נוסיף לכאן אוקיי לתוך הלוג עצמו ובדרך הזאת נגיד 2 מופעים זה גידול קטן זה שווה יותר ממופע 1 אבל הגידול הוא מאוד מאוד קטן כן כמובן אם אנחנו עובדים בלוג 2 אז נגיד 8 מופעים זה נותן לנו כבר 3 אבל צריך מאוד להתאמץ ככל שאנחנו עולים במספר וזה מתאים קצת ללוגיקה שאמרנו קודם שמס הגדולה כן רוצים שתהיה יותר משמעותית אבל צריך להגיע למס הגדולה לא להוסיף 1 ואז להעלות בצורה משמעותית כן אנחנו אבל נשתמש ב-2 דווקא אני קצת אערוך את ה... בסדר אז זה הרעיון ומה שאנחנו עושים זה דבר כזה אוקיי לצורך העניין כרגע כדי לתת ציון למסמך אוקיי זה משהו מאוד פשוט תוקחי את ה-query יש לו נגיד 10 תרמינים שרה תרמינים אוקיי לצורך העניין בשביל שזה לא יהיה באוויר נגיד שהתרמין שלו זה זה מה שראינו בסרטון הקודם כן אז נגיד שהתרמין שלו זה information, retrieval וsystem זה מה שמופיע אוקיי פעם היו כותבים ככה שאילטות גם במנוי חיפוש הגדולים היום כבר מדברים אליהם או כותבים כאילו זה היה בן אדם אבל נעזוב על זה כרגע information, retrieval וsystem יש לנו שלושה בעצם מימדים אז אנחנו שואלים שאלה כזאת כמה הדירוג של הterm frequency לפי מה שלמדנו עכשיו או לפי הcount בהתחלה זה בעצם כרגע יש לנו שלושה אפשרויות בעצם שאנחנו גם נסכם אותם להציג כל אחד מהמסמכים אפשרות שאמרנו בהתחלה כל אחד מהמילים מופיע או לא מופיע אם הוא מופיע מקבל תוצאה אחת אם הוא לא מופיע מקבל תוצאה אפס אוקיי אז נגיד שבמקום שinformation, retrieval יקבל ציון אפס אם המילה system לא נמצאת ויקבל דירוג שתיים לא דירוג ציון שתיים אוקיי וככל שהציון יותר גבוה אז הדירוג יותר נמוך כלומר הוא מנצח את החלופות שלו כן אנחנו נשים בתור מקום ראשון את זה עם הציון הכי גבוה כמובן אז באופציה הבוליאנית שלמדנו בהתחלה אנחנו פשוט גם עושים sum של כל הטרמים שמופיעים גם בquery וגם בדוקימנט סתם זה לא פורמלי מה שמוצא כאן אוקיי וכל אלו בעצם אנחנו אפשר אפילו להחליף את זה לכתוב פה count דומה קצת לפונקציית אקסל כזה מי שמכיר count if term if נמצא בתוך החיתוך כמה פעמים הטרמים מופיעים בחיתוך אז נגיד בדוגמא שנתנו בוא נגיד שבמסמך זה יידרג את התוצאות ככה סתם הקריט אני אומר d1 נגיד d2 יהיה מקום ראשון כי יש לו שלוש terms יהיה לו גם את information גם את system וגם את retrieval ואולי d5 יהיה מקום שני כי יש לו שתי terms את נגיד information אני אכתב את זה סתם בקיצור ואת system אוקיי אין לנו פה שום חשיבות למילה אחת ביחס למילה אחרת כמובן או מספר מופעים או משהו כזה זה הבעיה כשקודמים פה בzoom זה control z1 מחזיר רק שנייה סליחה אוקיי אז בוא נגיד שזה אמרנו d5 ולמשל אחרי זה יש לנו d3 שגם הוא מקבל אותו דירו כמו d5 נכתוב פה r1 זה הרנק זה r2 וגם זה יהיה r2 אותו דירוג אותו ציון אפילו וגם לו יש שתי terms וזה יהיה נגיד information retrieval אוקיי וכן הלאה ככה אנחנו יכולים לדריג תוצאות גם בשיטה שאנחנו סופרים בצורה בולונייני מופיע או לא מופיע של כל term במסמך זה אומר שמתוך הטרמים של השאילתה אנחנו רוצים לדעת כמה מופיעים במסמכים שאנחנו מחפשים אם אין אף term שמופיע נגיד כאן היה לנו information system retrieval זה השאילתה אם אין לנו אף term שמופיע במסמך אז הוא לא יהיה בתוצאות כל פעם הוא מוסיף אחד אם הוא מניצה כאילו לצורך העניין נבדוק אם זה פייטון נבדוק מה הלן של החיתוך ביניהם d2 זה איזשהו מסמך שאינדקס נותן מסמך מספר 2 כן הוא המקום ראשון והמקומות שניים ביחד זה שני מסמכים שהיו להם שתיים תוך שלוש מילים ואחר כך אנחנו אומרים שיכול להיות שמסמכים השונים ואם אנחנו בלקום לספור לעשות פשוט count או len של כמות הטוקנים המשותפת אנחנו נעשה איזשהו sum כן זה ההרציה השנייה בלקום count אנחנו נעשה sum רגע ראשון היה count שיטה שנייה אני אכתוב את זה סטייל פייטון מי שזוכר איך קוראים לזה בפייטון comprehension אנחנו עושים את זה בצורה כזאת נגיד שיש לנו פונקציה כזאת או קבלה לא משנה frequency של term for term בחיתוך אז זה בעצם השיטה השנייה אנחנו סוכמים את כל האקאונטים האלו מה אנחנו עושים? נגיד שכאן יש לנו אולי d5 למה? כי info הופיע נגיד שלוש פעמים ועוד system שהופיע נגיד ארבע פעמים אז סך הכל יש לנו שבע בגלל זה הוא היה הראשון אפילו ה-d2 שהיה לנו קודם אולי הסכום שלנו שווה נגיד ארבע אולי יש לנו נגיד information פעם אחת retrieval נגיד פעמיים וsystem פעם אחת סך הכל ארבע ובשיטה הזאת ל-d שלו הרנק נגיד מספר שלוש נגיד שכאן היה לנו ציון שלוש גם אז זה נגיד מופיע פעם אחת ואמרנו שזה information system זה information retrieval שהופיע פעמיים, סך הכל שלוש אז זה רנק שלוש אז בעצם אמרנו דבר כזה השיטות השונות כאן זה שיטה של boolean count וכאן זה שיטה של term frequency אז ה-term frequency וה-boolean count בצורך העניין נתנו לנו תוצאות שונות עכשיו אם אנחנו נוסיף את ה-term frequency שהוא בגידול לוגריתמי הוא יכול לתת לנו גם רנק אחר למשל כנראה ש-d2 שוב יהיה מקום ראשון אפילו אם היה מופע אחד לכל אחד מהטרמים האלו אז זה שווה לשלוש לעומת זאת אני אכתוב את זה במשרד כאן ועוד פעם אולי כאן אחר כך זה log normalization אז כאן המקום הראשון יהיה שוב d2 זה יהיה הרנק הראשון כי info מקבל 1 רטריבל מקבל נגיד 1.1 פעם אני רושם 1.5 זה סך הכל ייתן לנו 3.5 גם פה אנחנו סוכמים את הציון ברמת ה-term השיטה העקרונית היא אותה שיטה רק שההבדל ביניהם הוא שאיך אנחנו מנגדים כל אחד מהטרמים ה-query יכול להיות פשוט טקסט חופשי אינפורמיישן רטריבל סיסטם ומפרקים את זה למילים ואז שולחים את המילים לשאילתה מקבלים אפילו כל אחד מהטוקינים שמופיע והחזירים לנו את כל המסמכים אלה שמופיעים בצורה לא אופטימלית זה איחוד של הכל כאילו היה אור אפשר לעשות לזה גם אופטימיזציה אבל לתואר התחלה זה ייתן לנו את כל המומדים הרלוונטיים ועכשיו נעשה רנקינג ונגיד מחזירים את ה-10 הטובים ביותר או את ה-5 הטובים ביותר אז כאן אמרנו שזה הופיע שלוש פעמים אז נגיד כאן רשמתי 1.5 אולי זה נגיד, אפשר לבדוק כמה זה log 2 של 3 לא יודע, זה בא לכם נגיד שזה 1.5 למשל נעזוב את זה כאן אם זה מופיע שלוש פעמים אז בעצם שלוש מוסיפים על ה-1 אז יוצא לנו 2 log 2 של 4 זה 2 ואם זה מופיע ארבע פעמים אז בעצם זה כמו 5 אז נגיד שזה 2.3 אז זה נותן לנו האמת שזה נתן לנו משהו יותר גבוה מהראשון בכל זאת אבל תאורטית, אז בואו נגיד שזה היה יותר קטן נגיד שזה הופיע רק פעם אחת אז אם זה הופיע רק פעם אחת זה יקבל גם ב-log 1 וכאן יהיה לנו 3 אוקיי, את הרעיון הכללי הבנתם? זה יכול לתת לנו גם דירוגים שונים המשמעות שכל תר מקבל צריכה להיות משמעותית היא תהיה משמעותית רק אם הגידול בעוד גדול אז זה יכול בעצם לנצח שתי תרמים שהופיעו במוסמך אחר זה הרעיון מוסמכים גדולים בכלל נבחרו יותר אז זה עוד בעיות שצריך להתמודד איתה בואו כרגע נתעלם מהבעיה הזאת ונניח שכולם באותו גודל ונגיד שהם יחסית קצרי אוקיי, זה נכון, זה בעיה, מה? מה יקרה אם אנחנו נתעלם, לא נעשה log אז אם נסתכל נגיד על הדוגמה שהיה לנו במחזות של כל המחזות שראינו קודם אז יהיה לנו תרמים שהופיע המון פעמים אז בואו, מישהו צריך עוד את השקף הזה? אפשר לדעתי גם פה לסתם לעשות ככה נוכל אחרי זה להעלות את זה לפחות פעם היה אפשר לעשות אני אעשה פשוט סקרינג שאתה יגיד למה אין לי פה וידאו פתאום? אוקיי, בסדר, שאלות עד עכשיו? אוקיי, אז אני מתקדם אני אעזוב כרגע את הדוגמה עוד פעם שראינו קודם עכשיו יש לנו עוד אלמנט נוסף שמישהו ציין את זה כאן קודם או מישהי ציינה את זה כאן קודם אנחנו אמרנו בעצם דבר כזה עד עכשיו אמרנו דבר כזה בין עם נרמול של לוג ובין בלי נרמול של לוג אוקיי, אנחנו בעצם אומרים ככל שהטרם מופיע יותר פעמים במסמך אז סך הכל הוא יקבל חשיבות גדולה יותר ובסך הכל אנחנו סוכמים עבור כל טרם בקוורי שמופיע במסמך סוכמים את הנקוד ברמת הטרם שהוא קיבל מהמסמך זה בעצם מה שהראינו עד עכשיו אבל בעצם לא הבחרנו בין מילים חשובות יותר או פחות אוקיי, את זה לא עשינו עד עכשיו ולמה הכוונה? אז הדוגמה שיש לנו כאן יכול להיות שכל המילים חשובות באופן יחסי אוקיי, אבל אפשר לתת דוגמה טריוויאלית של הסטופרות שדיברנו עליהם בשיעור שעבר כל המילים שמופיעות כל הזמן הן עד כדי כך לא חשובות שהרבה פעמים אפילו מורידים אותן הן מופיעות פשוט כל הזמן אבל גם פה אפשר להסתכל ולראות שיש מילים שונות שהן יחסית מאוד מופיעות בהרבה מקומות בוא נסתכל נגיד על המילה מרסי אוקיי אז מרסי מופיע בכמעט כל המחזות של שייקספיר פה הוא מופיע גם באנטוני אנקליאופטרה רק באיליוס קיסר בעצם הוא לא מופיע בכל השאר הוא מופיע אוקיי, אז בוא נגיד שהוא די שכיח הצורה שהיה לנו לדבר על מונחים שהם מייחדים מונחים שלא יופיעו בכל מסמך אפשרי אלא יופיעו במעט מסמכים אוקיי, אז הרעיונו אנחנו רוצים להוסיף עוד פקטור נוסף מעבר לשכיחות בתוך המסמך רוצים להופיע על החשיבות של הטרם ואת זה אנחנו מודדים על ידי הנדירות של הטרם מודדים את הנדירות על ידי היחס לשאר המסמכים ככל שהוא מופיע פחות בשאר המסמכים אז הוא נחשב יקר יותר, יותר משמעותי אוקיי, למונח הזה יש כמה מטריקות אנחנו בעצם נשתמש במטריקה שנקראת Inverse Document Frequency אז Document Frequency בעצם מודד לנו בכמה מסמכים הופיע אותו טרם טרם פריקונסי נתייחס בעצם לעבור מסמך מסוים כמה פעמים הטרם הופיע או לוג של כמה פעמים לעומת זאת Document Frequency נספור בכמה מסמכים הופיע הטרם הזה אוקיי, לצורך המילה הזאת בסדר, אבל אנחנו לא רוצים להתייחס לדוקימנט פריקונסי אמרנו, דוקימנט פריקונסי אומר דווקא ההפך עם מה שאנחנו רוצים הוא יהיה יותר גבוה ככל שהמונח יהיה יותר נפוץ באופן יחסי בדאטה קולקשן או בקורפוס שלנו כן, והאוסף המסמכים שלנו ככל שהוא אופיע יותר הדאקימן פריקונסי יהיה יותר גדול אנחנו רוצים דווקא את המונחים הנדירים אז מה אנחנו עושים? אנחנו אומרים דבר כזה בואו נגיד שיש לנו עשרת אלפים מסמכים לצורך העניין ומונח מסוים מופיע רק בשלושים מסמכים לעומת זאת יש לנו תרם אחר שמופיע בחמשת אלפים מסמכים אז רוצים לתת סיום גבוה יותר לזה שמופיע רק בשלושים מסמכים אז אנחנו עושים דבר כזה אנחנו אומרים בואו ניקח את כמות המסמכים תעזבו שנייה את ה-log ששוב אני אפוך את זה ל-log2 אבל n בעצם מייצג לנו את כמות המסמכים שנמצא לנו בהסופת המסמכים שלנו בדאטה קולקשן או בקורפוס שלנו כמה מסמכים בעצם אינדקסים? זה ה-n df של t אומר לנו מה הדאקימן פריקונסי של t באופן כללי ככל שdf יהיה נמוך יותר אז המנה הזאת n חלקי df של t תהיה גבוהה יותר ולכן בעצם ה-idf יהיה גבוהה יותר כדי לא לתת לו שוב גידול ליניארי אז אנחנו שוב נעשה כנראה אחד ועוד המנה הזאת נעשה לזה log אז שוב נוסיף כדי שאם זה מופיע במקום אחד אנחנו רוצים שזה יהיה סליחה אחד זה בתוך ה-log אז כאן יהיה n חלקי df של t לדבר הזה אנחנו נעשה log אז אם עכשיו אנחנו נחזור לדוגמאות שהיו לנו קודם אז נוכל לראות שמונח שהופיע מעט פעמים מונח שהופיע מעט פעמים כמו נגיד כלפורניה אז הוא יקבל ה-idf גבוהה הוא מופיע רק במשמח אחד מתוך כל המחזות של שייקספיר לעומת זאת כמו שאמרנו מרסי יקבל ה-idf יותר נמוך וורסר גם יקבל מונח יותר נמוך זה הרעיון אז יש לנו פה את הדוגמה אם זה היה בסיס 10 זה מאוד קל שוב תחשבו על משהו שהוא קצת יותר עדין עם בסיס 2 אוקיי כאן פשוט כל פעם שמכפילים p10 זה ה-idf גדל באחד אז יש לנו פה כמות המשמחים בדוגמה הזאת מיליון אז אם זה מופיע בכולם הוא פשוט מקבל ציון 0 כאן לא הוסיפו 1 אוקיי אז מה עושים עם ה-idf הזה? יש לנו פה עוד ציון אז הדרך שבה עובדים כדי לשקלל גם את המספר המופעים במשמח וגם את הנדירות של אותו טרם זה להכפיל את הטרם פריקונסי ב-idf זה מה שנקרא tf-idf מי ששמע תחילים את ה... למשל כאן את ה-log של מספר המופעים במשמח ואת ה-idf שמופיע בצד ימי אוקיי עכשיו יש לנו מודל חדש גם במודל הזה אנחנו בעצם סוכמים את כל הטרמים אבל במקום לסכום רק את הטרם פריקונסי או את ה-log של הטרם פריקונסי בכל מילה שמופיעה גם בקווריה וגם בדוקימנט אז בעצם מה שאנחנו נעשה זה לסכום את ה-tf-idf לכל מונח בסדר? מה יקרה במקרה הזה? אז במקרה הזה בואו נעשה דוגמה נלך רגע שוב נגיד אולי לדוגמה הזאת או לדוגמה שנתנו עם ה-information retrieval system לא משנה אז יש לנו פה נגיד את... בואו ניקח למשל את d3 יש לנו את המילה complexity השורה הראשונה זה complexity ויש לנו את המילה traffic עכשיו שתי המילים מופיעים שלוש פעמים בדיוק ב-d3 אוקיי? שלא שם לב נדבר על זה אוקיי? אז גם complexity וגם traffic מופיעו בדיוק שלוש פעמים ב-d3 נכון? אבל traffic היא מילה נדירה ו-complexity זה מילה שכיחה בקורפוס שלנו אוקיי? שימו לב שזה גם תלוי קורפוס אם זה קורפוס שמדבר נגיד רק על complexity למשל זה מופיע בכל מקום וזו לא מילה כל כך מעניינת אם זה משהו שמופיע על נושא מחקר במדעי המחשב אז אולי לא תמיד נראה complexity אוקיי? אז בעצם בשיטה שלא שכללנו את ה-idf לא הכפלנו ב-idf אז שתי המונחים מקבלים ניקוד זה בשיטה שכן שכללנו את ה-idf אז traffic מקבל ניקוד יותר גבוה מאשר complexity ו-d3 בסדר? אני מייש? יפני מייק צריכים ללמד אותי גם איך אומרים זה בערבית אני שכחתי זה משהו כמו פאימתא לא משהו כזה לא? פאימתי בערך בדיוק בערך זה אהבתי אוקיי אז אנחנו כבר יוצאים להפסקה עוד רגע אז סיכום ביניים כזה יש לנו ככה נראה למשל ה-tf-idf של אותה מחזות של שייקספיר אוקיי אחרי שנגיד עשינו גידול של לוג בטרם פריקונסי והכפלנו את זה ב-idf בחישוב שערים אוקיי אז זה הציונים השונים ומן הסתיים אנחנו רק סוכמים את המופעים המשותפים של המילים שהופיעו בשאילטות וגם במסמך אז אנחנו נקבל ניקוד שונה ממה שראינו קודם נגיד רק במודל הבוליאני או אפילו עם הקאונטרים בסדר אז בשלב הזה מי שהיה אחראי על החצי הראשון של הקלטה אנחנו נפסיק נצא עכשיו להפסקה נגיד עד כן אני צודק זה עכשיו לא היה נראה לי בבחצי אין אורים לצאת קצת איחרתי את זה כי התחלנו לפה לא זה שעה וחצי זה קיצורים בידי עכשיו ברבע כשהתחלת בידי עכשיו ברבע זה תהיו שעה וחצי אז בוא נצא עכשיו לחצי שעה עד שלוש ועשרה חברה אנחנו ממשיכים אנחנו עכשיו ממשיכים אני מי שעושה את חלק בית של הסיכום אז מתחילים עכשיו אז סיכמתי פה אני מצטער לא הספקתי להעלות את זה היה לנו עוד עשר דקות אז אני היום מקווה להעלות את זה מוזמנים להזכיר לי אם אישו חייך אבל זה התוכנית בכל מקרה ניסיתי לפרמל את זה קצת יותר ממה שראינו קודם אז אנחנו נגדיר בעצם את הציון שאנחנו מקבלים על מסמך מסוים עבור שאילטה מסוימת כמו נגיד השאילטה שהיה לנו קודם אינפורמיישן רטריבל סיסטר אז אנחנו מקבלים על סכום על עבור כל טרם שמופיע בחיתוך בין הקוורי למסמך אז אנחנו סוכמים את הציון של הטרם והציון של הטרם אנחנו כבר מדבר זה בעצם הציונים השונים שדיברנו עליהם הרנק שהיה לנו לצורך העניין עשיתי את זה קצת פייטוני כזה על האינדקס במערך אנחנו כמובן נדרג למעלה ברוב נמוך ככל שזה ציון יותר גבוה אז אנחנו נמיין לפי הציון ונעשה ריברס כי מיון בעיקרון זה מלמטה למעלה אנחנו רוצים שברנק הגבוה יהיה את הציונים הכי גבוהים ולא את הכי נמוכים בגלל זה עשיתי ריברס בסדר או זה סתם שיהיה ברור למה הכוונה ואז הסקור ברמת הטרם יש לנו פה הסקור הכי בסיסי זה פשוט לתת 1 עם הטרם במסמך ואחרת 0 כתוב כמו Lamda בפייטון אם אתם מכירים tf-score בעיקרון זה אקאונט של הטרם זה אמור להיות במסמך אם הטרם מופיע במסמך אקאונט שלו זה עשיתי copy-paste זה אותה טעות פעמיים ה-log דיברנו על זה אחד ועוד אקאונט ואם הוא מופיע במסמך יש לנו פה tf-idf כאשר קראתי פה כוכבית זה יכול להיות או tf או אם קראנו זה log tf אז אחד משתי אלה ברכיב של ה-tf כפול idf, שוב idf זה log של n חלקי ה-df-term כאשר ה-df-term זה מספר המסמכים שהטרם מופיע בהם n זה מספר המסמכים בקורפוס או בדוקימנט קולקציון שלנו זה שאינדקסנו ו-df זה כמות הדוקימנט שההאטר מופיע בהם אז זה בגדול מה שדיברנו עליו עד עכשיו כמובן שאתם מוזמנים לעשות תרגיל שמממש בעצם מקבל כמה מסמכים אם אתם רוצים אתם יכולים לגנות כמה טבלאות אם זה עוזר לכם בשביל לא לממש ממש את האינדוקס אז לזה שכבר פירקתם את זה למסמכים ויש לכם את הקאונטים שאתם יכולים לגבוה להכין אז לא צריך ממש מסמך ולפרק את הכל ואז על הדבר הזה לעשות בעצם סקור של tf-idf בואו נגיד שיש לנו שתי אפשרויות tf-idf עם לוג או tf-idf עם טרמספור אז יהיה לנו שתי אפשרויות לתרגיל שוב כמו שבוע שעבר נכתוב על המטלה מה האפשרויות במקווה היום או מחר אז זה הרעיון זה כבר אופציה אחת כמו שאמרנו אנחנו יכולים בעצם קצת התעלמנו כרגע מהיכולת להציג את המסמך כווקטור עכשיו פשוט נתנו להם ציון לפי החיתוך הזה שדיברנו עליו אבל בעצם כמו שהראינו המסמכים בתצוגה שלהם הנוכחית אפשר לייצג אותם כווקטורים כמו שדיברנו ואם כבר ווקטורים אז אפשר להשתמש במטריקות מרחק של ווקטורים אז זה כבר דוגמה דומה למה שהראינו קודם פשוט עם שני טרמים שונים במקרה הזה שוב חזרנו למחזות של שייקספיר ונעשה איתכם תרגול של כמה מטריקות של מרחק אוקיי אני לא יודע מי שלמד למידת נכונה בטח מכיר את זה מי שלא אולי אם למדתם KNN או K-Means אז אתם בטח מכירים מי שלא אז עכשיו נתרגל את זה בזריזות גם אם שכן אז תזכרו נגיד שיש לנו פה שתי אובייקטים לצורך העניין אבזורביישן אחד נגיד זה הדוקימנט ואבזורביישן שתיים זה ה-query אוקיי כן נראה את זה עוד רגע אנחנו בדרך כלל לא נשתמש במרחקים האלו אבל כן ברור שאי אפשר ככה אוקיי אתה צודק אוקיי כי אחרת זה יתה לנו אבל שוב השאלה היא מה המטרה שלנו אם המטרה היא פשוט לקחת את שתי הוקטורים ולתת לנו אני אתייחס לכמה רבדים הרמה העקרונית מה שאנחנו רואים עכשיו זה כאילו המימדיות או מספר המילים בווקאברי הוא שלוש זה כמובן לא המצב נגיד שאנחנו מדברים על סדר גולד של עשרת אלפים מילים או מאות אלפים לא חשוב מספר הטרמים הייחודיים שמופיעים בבוקאברי שלנו בוקאברי זה בעצם אנחנו לוקחים את כל המסמכים שמופיעים לנו בדאטה קולקשן בקורפוס לוקחים את הטרמים הייחודיים שלהם וזה מה שהם מרכיב לנו את הבוקאברי אז משתמשים במילה בוקאברי לצורך העניין הגודל שלו מה שאמרנו נגיד הוא עשר תל אפים כמובן שיהיה לנו פה הרבה אחסים אבל זה בסדר כי הרוב המילים לא יופיעו במסמך ולא בשלטה אבל זה לא רק זה, יכול להיות שיש לנו פה צורך לעשות איזה נורמליזציה אבל אנחנו עוזבים את זה לגמרי כרגע, נורמליזציה אם אנחנו כן משתמשים במטריקות מרחק האלו אז נגיד אתם מכירים שיטות של סקיילינג, למדתם? מין-מאקס, סטנדרטיזיישן, דברים כאלה אז בואו נדבר על מין-מאקס, זה נראה לי הכי פשוט אנחנו רוצים בעצם לייצר סקאלה חדשה עבור כל הערכים הישריים של ה... בלמידת מכונה זה פיצ'רים, כאן זה פשוט נגיד קאונטים של מילים או משהו כזה או הציונים השונים, ואז נחליט שהסקאלה השונה שלנו יהיה בין 0 ל-1 המינימום החדש יהפוך ל-0, כנראה שהוא כבר עכשיו 0 והמקסימום מקום שיהיה אולי גם אם אנחנו עושים tf-idf עם לוג אז נגיד שזה 10 המקסימום, אז עכשיו זה יהיה בין 0 ל-1 הכל ולמשל, נדאג לזה גם במסמך וגם בשילטה בקיצור אבל זה... אפשר לעשות את הדבר הזה, אבל בכל מקרה הציונים שלנו לא יראו ככה ברמת הטרם, הם יראו יותר דומה נגיד ל-tf-idf שראינו קודם והטרמים החשובים יותר או הנדירים יותר בעצם יקבלו משקל גבוה יותר וזה בסדר לה אז למה אנחנו משתמשים בציון הזה בסוף? אנחנו משתמשים בו בשביל לעשות רנקינג, אז גם אם כל הציונים הם קטנים מאוד כי בכל זאת מדובר במסמך עצום ובקוורי קטן כזה הקוורי יהיה עוד יותר ספרס ביחס למסמך, אבל השאלה היא בסוף מי יותר מתאים לקוורי לא מה הציון הגבוה, אז יכול להיות שבשביל רנקינג זה ייתן ציון סביר זה התשובה הארוכה לשאלה הקצרה, אז בכל מקרה יש לנו פה מה שכתוב פה זה משפחה מאושרת, מתמטיקאים או מי שחוקר במדעי המחשב אנשים כנראה שהם לא כל כך טובים מחברתית אז הם מציעים משפחות של פונקציות במקום משפחה אמיתית, אז זה משפחת פונקציות שנקראת מניקובסקי שבעצם ההבדל בפונקציות זה ערך של פי, אם אתם שמים לב יש פה שתי פרמטרים של פי והפי הזה הנהוג יכול להיות אחד, שתיים או אינסוף, עבור שתיים מדובר במרחק אוקלידי שאתם מכירים מהנדסת המישור נגיד, מרחק בין שתי נקודות זה בעצם מרחק בין שתי וקטורים במימדיות שתיים, אז מה עושים אנחנו לוקחים את כל המימדים עבור וקטורים x וy וסוכמים את ההפרש הריבועי ביניהם ומוצאים בסוף שורש, זה מרחק אוקלידי אז עכשיו רק בשביל ליישר קו לראות שהבנו נעשה מרחק אוקלידי בין הוקטור 179 לבקטור 11, 21, 4, אז מה יוצא לנו כאן? בואו נחשב את המרחקים הריבועיים אז 11 ו-1 בריבוע, עזרו לי זה מאוד קשה לי, זה 100 וחישבתי כבר ועוד מה? 21-7 בריבוע, 196 ו-4-9 בריבוע זה 25, בסדר? אז אנחנו רוצים בעצם לעשות שורש של הדבר הזה, בסדר? אפשר לחשב במחשבון ולקבל תשובה אני אבטר על זה כרגע, נראה לי גם בשקף הבא יש את התשובה הזאת יותר מורית המוטיבציה אז כן, זה החישובים שלנו וזה יוצא בערך מרחק 17.9 זה המרחק האוקלידי, תשאלו אותי אם זה לא ברור מרחק מנהטל זה אולי המטריקת מרחק הכי פשוטה פשוט מחשבים את ההפרש בערך מוחלט בכל מימד 11-1 בערך מוחלט, 21-7, 4-9, כל זה בערך מוחלט סוכמים מקבלים את המרחק תחשבו שנייה על רנקינג, אז זה בסדר שזה לא מייצג את המרחק עדיין לא נרמלנו, השאלה אם זה נותן לנו יחס סדר בין התוצאות כלומר זה נותן לנו, מרחק גבוה אגב זה טוב לנו או רע לנו, מה עדיף לנו? מרחק גבוה, מרחק נמוך, מרחק נמוך, נכון? כי זה לא שהם יותר דומים ככל שהמרחק נמוך יותר בסדר? אנחנו רוצים query ו-document כמה שיותר דומים כל אחד מהם יוצג ב-vector עכשיו אז דיברנו על מרחק אוקלידי ומרחק מנהטל אם נחשב כאן זה יצא לנו 29 זה פחות משנה התוצאה עצמה אלא איך מחשבים אותה ובסוף יש לנו פונקציה שלישית שאפשר להסתכל נדמה לי בפונקציות מני קורסקי להבין למה זה יוצא מקסימום כש-p שווה לאין סוף אבל לא נוכח את זה כאן בכל מקרה אנחנו לוקחים את ההפרשים בערך מוחלט ולוקחים את ההפרש המקסימלי ההיגיון אינטואיטיבית, שנייה סבבה, ההיגיון אינטואיטיבית הוא ככל שיש לנו איזשהו פקטור שהוא יותר גדול זה מבטא שוני גדול יותר וזה בעצם הפקטור המשמעותי אנחנו לא מחפשים את השינויים הקטנים אלא בעצם שואלים מהו השינוי הגדול ביותר בין ה-vector בין השאילתה, שאילתה מול המסמכים נגיד שעברו איזה התאמה מינימלית בשיטה מסוימת לצורך העניין המינימום זה פשוט כל מילה אפשרית הופיע בהם או לפחות כמות מסוימת של מילים ואז עכשיו רוצים לעשות רנקים פנימי ביניהם מה התוצאות הכי טובות, אז איך בוחנים את זה? למשל מטריקת מרחק ככל שהדיסטנס קטן יותר זה אומר שזה יותר דומה אז תקבל רנק גבוה יותר, זה גם אפשר לעשות אבל מה שמעניין אותנו זה בין ה-query לדוקימנט כי בסוף רוצים את הדוקימנט שהכי דומים ל-query לצורך העניין אז זה מרחק 7x7, במקרה הזה המקסימום פה במימדים באפרש זה המימד השני של 21 ו7 זה הכי שונה ולכן מרחק 7x7 ייתן לנו 14 כל אחד מהמרחקים בעצם יש לו דגש שונה אם תרצו מבחינת מרחק מנהטן אנחנו בעצם סוכמים את ההבדלים בין השאילת על המסמך או בין שתי האובייקטים ולא אכפת לנו אם ההבדל נמצא במקום אחד או שהוא מפוזר בין נגיד טרמים השונים לעומת זאת מרחק אוקלידי ייתן לנו מרחק יותר גבוה כאשר יש לנו מקום מסוים שבו יש את ההפרש המשמעותי בואו נעשה דוגמה, יש לנו נגיד מימד שלושה מימדים גם אז נגיד שיש לנו מרחק של, נתקוף לכם דוגמה מספרית במקום מדבר בעל פה יש לנו שתי אפשרויות, יש לנו פה איזה קווירי שכבר הפכתי אותו לבקטור הפכתי אותו כבר לבקטור שכאן יש לנו נגיד נגיד שלושה מימדים, נעזור כרגע את כל האפסים, זה הקווירי שלנו ודוקימנט 1 יהיה הוקטור 1,1,4 דוקימנט 2 יהיה הוקטור 2,2,2 אז לפי מרחק אוקלידי אנחנו מחשבים הפרש הריבועי בין הוקטורים ולכן דוקימנט 2 מקבל איזה מרחק? מה המרחק בין דוקימנט 2 לקווירי? קווירי 3, ובין דוקימנט 1 לקווירי? 3, נכון כלומר היה לנו פה מאפיין אחד שהיה בדל גדול זה נכון כמובן גם כשמפזרים איזה קצת יותר אבל אם יש לנו מעט מקומות מפרש גדול והרבה מקומות עם קצת הפרש אז בעצם המרחק האוקלידי יגיד לנו שהפרשים קטנים זה פחות משמעותי הפרשים גדולים הוא נותן להם משקל הרבה יותר גבוה מרחק מנתן פשוט סוחם את ההבדלים ולא אכפת לו אם הם יושבים במקום אחד יותר או פחות אלא כמה סך הבדלים יש, אוקיי? לפעמים יש היגיון לשיטה הזאת ולפעמים יש היגיון לשיטה הזאת כל מה שאמרנו עכשיו זה רק שלושת השיטות של פונקציות מניקובסקי אבל מה שיותר מקובל מכל הדבר הזה זה מרחק קוסינוס אוקיי? אז מכירים כולם את פונקציית קוסינוס אני מאמין היא נעה בין מינוס אחד לאחד אוקיי? יש לנו אפילו בשקופית הזאת נראה בניגוד למה שהם למדים בקורסים מאותמטיקה מעלות רגילות ולא רדיאנים אוקיי? דווקא עובדים רדיאנים, סקן יותר קל אולי קצת ויודעים שב-90 מעלות אנחנו... מכירים כולם את מעגל היחידה? כן? הקשר בינו לבין קוסינוס וסינוס? אוקיי, אז טוב אז נעשה את זה ממש מהר אם מכירים אוקיי? אז יש לנו כאן כאשר הקוסינוס מבטאת במעגל יחידה זה אומר שהרדיוס שלנו הוא אחד כן? זה הכוונה מעגל יחידה ואז הקוסינוס הוא בעצם הקורדינטה של x והסינוס זה הקורדינטה של y אוקיי? כאשר הרדיוס נופל על ציר ה-x אז הקוסינוס שלנו הוא אחד והסינוס הוא אפס כי הגובה הוא אפס בניפודה הזאת אוקיי? כאשר אנחנו מדברים על 90 מעלות אז קוסינוס הוא אפס וסינוס הוא אחד והקוסינוס שלנו יהיה מינוס אחד כאן בסדר? ב-180 מעלות אוקיי? אז גם בין 180 ל-360 אז הוא ינוע בין מינוס אחד לאחד אז זה מה שאנחנו רואים כאן בעצם בגרף הזה זה קל להסתכל על זה במעגל היחידה למי שבמקרה לא מכיר את זה עוזר קצת יותר מאשר להבין שאפשר להכפיל את זה בפקט אנחנו עושים כמו דמיון משולשים מדברים על הפרופורציה בין הציר ה-x לבין היתר במשולש אוקיי? אז מה הקשר לענייננו? הקשר הוא האוטפוט של הקוסינוס שהוא בין מינוס אחד לאחד במרחק קוסינוס או דמיון קוסינוס שזה בעצם המשלימים אחד לשני בעצם מודדים לנו דמיון בין וקטורים שהתוצאה היא בין מינוס אחד לאחד עכשיו אם כל הערכים הם אי שליליים כמו במקרה שראינו כאן במקרה שלנו אז האוטפוטים בכלל בין 0 ל-1 כאשר 1 אם אנחנו מדברים על דמיון 1 הוא אומר מאוד דומים 0 אומר מאוד שונים אוקיי? אז זה השיטה שבדרך כלל אנחנו עושים דמיון קוסינוס אז השיטה הכי פרימיטיבית בוא נגיד זה לעשות פשוט דוט פרודקט אוקיי? מה שאנחנו רואים כאן בלי הנרמול אוקיי? דוט פרודקט לקחת את הוקטור של השאילתה בתור נגיד וקטור שורה לחפיר אותו בוקטור עמודה של המסמך קיבלנו את המרחק ביניהם מה הבעיה? הבעיה היא שיש לנו פה שאילתה מאוד קצרה מסמך מאוד גדול וארוך אז כמו שאנחנו עושים משהו דומה מי שזוכר ב-covariant שמחלקים באסטיאותקן של שניהם אז כאן מחלקים בנורמות של שניהם אז כאן יש לנו נורמה אוקלידית L2 שראינו קודם בעצם סוכמים את הערכים של הוקטורים בריבוע ומוחסים בסוף שורש אוקיי? אז זה דמיון פוסינוס בואו נראה איזה דוגמה לדבר הזה הנה כאן זה סתם דמיון בעין נגע ולג לפה אוקיי? יש לנו שני מסמכים כבר עשינו מודל בג אבורדס הפכנו אותם לבקטורים בצורה הזאת אוקיי? עכשיו לצורך העניין זה מסמכים שמותפסקים כולם נגיד זה פוסטים שקשורים לספורט אוקיי? בגלל זה רואים את כל המילים האלה טים, קוד, שוקי אתם יכולים להנחש איפה זה קרה לפי סוג הספורט בייסבול, סאקר, פנלטי מילים שקשורות לספורט במולחים שונים ועכשיו אנחנו רוצים לקחת שתי וקטורים ולמדיון את הדמיון פוסינוס ביניהם אוקיי? אז הנה התזכורות של הנושא שלנו אז אנחנו עושים וקטור אחד כפול הוקטור השני שוב, זה הכל בא מפה ואנחנו מנרמלים ב-L2 אוקיי? אז בואו ננסה לעשות זה כתרגיל לחשב אוקיי? אז מה יצא לנו עם זה היישובים שאנחנו עושים רגע, אתה מחשב כרגע את הדוט פרודקט? אוקיי, אז זה חמש ברמה עקרונית זה אותו אורך כמו נורמה, זה אותו מספר מכפילות חמש כפול, בסדר, חמש כפול שלוש כן, איך אנחנו מחשבים את זה? כן, זה חמש כפול שלוש ועוד אפס וכן הלאה אוקיי? זה בעצם הדוט פרודקט שלנו איך אנחנו מחשבים את הנורמה של X במקרה הזה? לפי L2 כן, אז חמש בריבוע, נכון ועוד אפס ועוד שלוש בריבוע אבל ברמה הסקלרית, ברמה הבודדת כן, אז וכן הלאה ובסוף נוסעים שורש, נכון אוקיי? אותו דבר אנחנו עושים ל-Y ועושים את המכפילות של הנורמות במחנה דוט פרודקט במונה ואנחנו מקבלים פה דמיון שהוא די גדול שימו לב, אם אנחנו נסתכל רגע על הפרטים מה אנחנו רואים את ההבדלים פה בין שניהם כמעט כל טרם שמופיע באחד מהם מופיע בשני אבל... כמעט הכל, נכון זאת אומרת פה זה אחד אז יש שתיים אבל בדרך כלל המספר המופיעים כאן הוא נגיד בערך פי אחת וחצי מפה אוקיי? כאילו דמיון קוסינוס הוא מסתכל על הדמיון באופן כללי ופחות נותן משקל לזה שנגיד אחד יכול להיות פי שתיים יותר יותר מהשני למשל ואז פשוט מכפילים את כל הטרמים כשתיים אוקיי? דמיון של 0.94 שוב, זה עד אחד יכול להגיע זה דמיון די גדול אוקיי? אז יש לנו פה חישוב של דמיון קוסינוס אוקיי? אז אני מציע שהאופציה השנייה של לעשות משהו זה באמת לקחת דמיון קוסינוס כבר אנחנו נגיע לכאן אבל כדי שלא תעשו את שתי האופציות אז אפשר פשוט לקחת כבר טבלה של מסמכים עם ציונים של TF-IDF אוקיי? אפשר למצוא גם אמולטור לדברים האלה שעושה את זה בעצמו כל מיני קלקולייטר או אונליין קלקולייטר שמכניסים לו כל מיני מילים והוא נותן לכם TF-IDF אוקיי? כדוגמה ואז על הדבר הזה מי שאבחר אז כאופציה שנייה לעשות מרחק קוסינוס בין איזה שהיא שאילתה לנגיד שלוש או ארבע מסמכים אוקיי? ואז זה בעצם היא האופציה השנייה כמובן שאם היינו עושים עוד צד אז היינו עושים רנקינג על ידי זה שככל שהציון ככל זה יותר דומה הדמיון קוסינוס גבוה יותר קבל רנק גבוה יותר בסדר? אז דמיון קוסינוס זה כן אם דיברנו קודם על מרחקים וקטורי מוקלידי או צ'ווי שרומנטן שהם מאוד מקובלים במקומות אחרים שעושים מרחק וקטורי אז כנראה דמיון קוסינוס זה הכי פופולרי לצורך החזור מידע אוקיי? כשה וקטורים יצגים מסמכים אז אוקיי, יש משהו שלא עשינו עד עכשיו אוקיי? אז דיברנו על TF-IDF עם כל מיני נרמולים למיניהם דיברנו אפילו על שיטה אחת פשוטה שפשוט סוחמת את הציון ברמת הטרם לפי המטריקות השונות שלמדנו לתת ציון לטרם ושיטה אחרת שמדברת על דמיון וקטורי אוקיי? המודל הוקטורי וקטור ספייס אבל מה שאנחנו נדבר עליו זה שיש לנו משהו שקצת חסר לנו האמת שדיברנו על זה כאן קודם ומה שחסר לנו זה להוסיף פקטור נוסף מה שקורה, בואו נחשוב על שני מסמכים מסמך אחד הוא באורך פסקה נגיד או אפילו משפט שבמקרה מכיל את המילים של השאילתות מסמך שני זה אולי ספר על למידת מכונה אוקיי? שאחד מהפרקים שלו אולי הוא קשור לעזור מידע אבל הוא מדבר על מיליון נושאים אוקיי? אז בעצם מסמך אחד שהוא אם אני רוצה להסתכף אהה אהה לא יודע מה השרשור של כל הסיכומים של הקורסים השונים בתואר ראשון במערכות מידע אוקיי? במסמך הראשון זה משהו המסמך הראשון זה משהו מאוד ממוקד שקשור לשאילתה אוקיי? עכשיו אם השאילתה שיש לנו קשורה למדעי המחשב איכשהו היא כנראה תופיע במסמך הענק הזה של שרשור כל הסיכומים אבל זה לא יהיה ממוקד אז אחד מהדברים שאנחנו רוצים אולי לקחת בחשבון זה להוסיף פקטור נוסף שזה האורך של המסמך אוקיי? בעצם להגיד דבר כזה אנחנו רוצים להתייחס גם לאורך של המסמך להוסיף אותו ולא בתור עוד פקטור לנרמור לצורך העניין אוקיי? אז השיטה הפופולרית ביותר לעשות את הדבר הזה נקראת בשם המאוד אינטואיטיבי BM25 ועדת שמות של חלק מהאנשים היא טובה יותר וחלק היא פחות לא תמיד כולם מוכשרים בשמות כאלה קליטים מה? כן זה זה ראשי תיבות אני אף פעם לא זוכר מה זה אבל זה יהיה כתוב במהוד רגע מה? אפשר להוכיח שזה לא פותר את זה עד הסוף כמו שצריך אני אנסה לבוא עם דוגמה אין לי כאן דוגמה בשלוף אבל אם היה לנו עוד הפסקה הייתי עושה את זה אבל בואו נציג את השיטה רגע רגע אוקיי הראינו כבר דוגמאות כאלו אני best match זה ה BM best match או פשוט לפעמים הוא copy BM אוקיי? אז זה כל הנושא של לחזור מידה התחיל מאמצע המאה הקודמת אז זו שיטה שפותחה יחסית בסוף המילניון הקודם ואנחנו אומרים דבר כזה אוקיי? אז יש לנו פה tf ואנחנו נוסיף פה עוד איזשהו פקטור שאני כרגע בשביל הסקופ של הקורס הזה אני אבטר בדיוק על הסבר של k1 כפרמטר לBM שאפשר לבחור ערכים שונים ונקפוץ ישר לדבר הזה אוקיי? אז מה יש לנו כאן? יש לנו כאן את average document length כן? זה סתם ראשי עקבות AVDL כאילו אנחנו לוקחים את כל המסמכים בקולקשן שלנו ומודדים מה גודל הממוצע של מסמך מסוים ואנחנו מחשבים דבר כזה אנחנו לוקחים את ה document length שלנו על ידי זה שאנחו סוחמים את כל ה-tf'm שלנו בסדר אנחנו סוחמים את הכל ויש לנו ככה אנחנו מחשבים גם לשאר המסמכים בעיקרון ואז יש לנו את היחס הזה המסמך שלנו איך הוא ביחס, מבחינת האורך, ביחס לממוצע הוא יכול להיות גדול מהממוצע, הוא יכול להיות קטן מהממוצע זה הפקטור שאנחנו רואים כאן זה ה-DL חלקי AVDL ואז אנחנו אומרים בעצם דבר כזה אנחנו מחליטים אם להוסיף את זה כפקטור או לא להוסיף את זה כפקטור או כמה לשקלל את הדבר הזה זה בעצם הפקטור B מי שמכיר כל מיני רגואליזציות למיניהם בלמידת מכונה אז יש לנו איזשהו פקטור מקורי שאנחנו משקללים ואז מכניסים פקטור חדש עם איזשהו מקדם שככל שהוא יהיה גדול יותר אז נכניס את הפקטור הזה ברמה יותר משמעותית וככל שהוא יהיה קטן יותר אז נכניס אותו ברמה פחות משמעותית כלומר אם B כאן שווה ל-0 אז נתעלם לגמרי מהנרמול של המסמך אם B שווה ל-1 נתעלם לגמרי מהאורך הרגיל ונתייחס רק לנרמול הזה ונקבע איזשהו ערך בין 0 ל-1 שוב, ככל שהוא גבוה יותר נתייחס רק לערך המנורמל ככל שהוא קטן יותר נתעלם לערך המנורמל זו אפשרות ככה לשקלל בין שני הדברים בין לבין והמשמעויות אז זה הפקטורים השונים ואם עכשיו רוצים להתייחס לכלל הציון אז אנחנו נגיד דבר כזה כאן יש לנו בהתחלה נסמן את הרכיבים השונים מי שמזהה זה IDF IDF של המסמך אוקיי? עכשיו יש לנו לצורך העניין פרמטר מסוים נגיד שלוש, לא חשוב זה נתון לנו כאיזה שהוא היפר פרמטר לפונקציה הזאת כאינפוט לצורך העניין ואנחנו אומרים דבר כזה כאן יש לנו את ה-TF אוקיי? אנחנו מדברים על תר מסוים מתוך השילטה TF וכאן אנחנו מנרמלים אותו לפי מה שראינו קודם אוקיי? בסדר? אז ככל בעצם בוא נניח שהיינו מסתכלים עכשיו בשביל הפשטות רק על הנרמול כאן אוקיי? אז תגידו לי האם זה יהיה המחנה יותר גבוה כאשר אורך המסמך הוא גדול או כשאורך המסמך הוא קטן מה יקרה בעצם כשאורך המסמך הוא קטן מאוד ביחס לעמוצה ומה יקרה כשהוא גדול מאוד בשביל לדמיין אורך העמוצה בוא נתעלם כרגע נגיד ש-K1 שווה 3 אז בוא נראה זהו, אז אם הוא קטן מאוד אז בעצם הדבר הזה שואף למה יש לנו נגיד סתם לצורך המחשה 3 ונגיד שאורך הממוצה הוא 100 אוקיי? אז בעצם זה שווה ל-0.03 הכפלנו את זה ב-3 0.09 ועוד ה-TF המקורי אוקיי? ועוד ה-TF המקורי אוקיי? ולמעלה יהיה כתוב לנו 4 כפול TF כן? 4 TF למעלה אז בעצם אם יש לנו מסמך מאוד קטן אז אז הדבר הזה אוקיי? הוא כבר משמעותית אוקיי? הבנו את הרעיון? אז כאילו אם יש לנו מסמך קטן וזה טים זה מקבל פקטור הרבה יותר גדול זה הרעיון בקרביות אוקיי? הוא נותן פנלטי למסמכים גדולים בעצם יש לנו אז כאן כבר אופציה שלישית למי שרוצה לממש את הוקפי ב-25 ואפשר סתם לקחת איזשהו פרמטר של K1 נגיד שווה נגיד 2 הנה כתוב לנו גם שK1 הוא בין 1.2 ל-2 נגיד 1.5 בסדר? ו-B שווה ל-0.75 אז זה בעצם שני הערכים בשביל לממש מי שרוצה זה הערכים של אחרי מחקר אימפירי אחרי מחקר עם ניסויים ברור שהערכים האלה עובדים הכי קצת אוקיי? והנה כמה דוגמאות של ציון לפי הסכום אם אנחנו עובדים פשוט עם TFIDF רגיל אוקיי? ומה יקרה אם אנחנו עובדים עם ב-25 כאשר K1 שווה ל-2 אוקיי? אז במקרה הזה אנחנו רואים שכנראה מסמך 2 היה קצר יותר וממוקד יותר ולמרות שלפי TFIDF הוא קיבל ציון קצת יותר נמוך מסמך 1 אז אם נשקלל גם את אורך המסמך אז התהפכו היוצרות והוא קיבל ציון קצת יותר גבוה בסדר? הבנו את המוטיבציה ואת הרעיון איך עושים את זה? זה? זה ברמת הטרם לא זה לכל טרם כל I בתוך Q כן, זה ברמת המסמך ברמת המסמך אפשר להתעלם מהסיגמה הזאת ולהגיד שאם מסתכלים רק על זה אם מסתכלים רק על זה אז זה ברמת הטרם בסדר? ניסחנו מזה יותר שקשור למסמך טרם שקשור שהוא חיתוך מהמסמך והקוורי כן אבל אני מסתכל על זה עבור I ששייך ל-Q חיתוך D בסדר? D זה המסמך שאנחנו כרגע נותנים לו ציון אם אין למסמך C אנחנו צריכים בואו ה-TF יהיה שונה והכל יהיה שונה גם ה-IDF יהיה שונה אוקיי בינתיים ברור? אוקיי אז עד עכשיו בואו נעשה איזה סיכום ביניים אנחנו בעצם סיימנו את כל ה-flow המינימלי של איחזור מידע קלאסי דיברנו על איך עושים פרוססינג לשאילטה ולמסמך דיברנו על מודל מאוד מוקשה מודל בוליאני אוקיי? דיברנו על איך שומרים את המידע inverted indexes אם עושים אותם עם מיקום ובלי מיקום בעיקר בלי מיקום או שאנחנו סופרים כמה מופעים יש או שאנחנו לא עושים את זה אוקיי? אז זה הדרך שלנו לשמור ול-index את המסמכים ביחס לטרמים השונים שמופיעים בכל ה-data collection שלנו, בכל הקורפוס מהצד השני דיברנו על השאילטה שיכולה להיות שאילטה בוליאנית ועברנו היום לדבר גם על רנקינג איך אנחנו בדרכים שונות תוך התייחסות לטרמים שמופיעים בשאילטה ובמסמכים יכולים לתת בעצם ניקוד למסמכים המתאימים לשאילטה אוקיי? אז הדרך הראשונה הייתה ניקוד בוליאני לכל טרם, ברמת הטרם אם הוא נמצא, ציון 1 אם הוא לא נמצא, ציון 0 ואז סוכמים כמה אחדים יש וככה עושים את הרנקינג אוקיי? אז דרך שנייה אנחנו פשוט סופרים כמה מופעים יש לכל טרם במסמך כל טרם נסמך וגם השאילטה נלך רגע לסיכום הזה שהיה לנו כאן אולי זה ירוד זה מעולה עיניים אולי זה יעזור לא לקחתי בחשבון שאני צריך לעשות ככה באחורה אוקיי אז דיברנו על הדבר הזה כאן על איך נותנים ציון לטרם דיברנו על ציון הבוליאני ברמת הטרם או אותה term frequency score שסופרים כמה פעמים זה אמור להיות term and document אוקיי? וסופרים לכל טרם כמה פעמים הוא מופיע במסמך מה שאנחנו קוראים TF ואפשר גם לתת ציון לפי מספר המופעים אבל לעשות גידול של הציון לכל טרם בצורה לוגריתמית בצורה הרבה יותר איטית אוקיי? דיברנו גם על IDF ככל שהמונח יהיה יותר נדיר ניתן לו משקל גבוה יותר אוקיי? ושילוב של זה עם ה-TF או ה-log TF אז זה בעצם השיטות הבסיסיות ואז סוכמים את הציונים ברמת הטרמים גם על הדבר הזה בסוף ראינו את ה-Okapi BM25 ש בעצם משקלל גם את אורך המסמך עם הפקטור הנוסף הזה עם שתי הפרמטרים האלו של K1 ושל B אוקיי? נראה אותם פשוט בתור קבועים היפר פרמטר אז לא נכנסנו לעומק של משמעות שלהם אוקיי? אז זה גם לצורך העניין מתחשב ומעניש מצמחים מאוד ארוכים אוקיי? אז דיברנו גם על יכולת למדוד את הקרבה או את המרחק בעזרת דמיון וקטורי או מרחק וקטורי למדנו או חזרנו על כמה מטריקות מרחק וקטור אוקיי? אז אנחנו יודעים גם לעשות רנקים עכשיו אוקיי? Scoring slash Ranking עכשיו מה שאנחנו רוצים לדבר עליו ככה בחלק האחרון של השיעור זה איזשהו מדד לכאורה קצת יותר אמין להצלחה של המימושים שלנו כמה אלגוריתם שלנו מוצלח יותר השיטה לעשות את הדבר הזה ברמה עקרונית היא לבנות מה שנקרא גם בלימדת מכונה איזשהו Data Set איזשהו Benchmark בכל תחום יש לנו איזשהו Benchmark איזשהו אוסף של לצורך העין דוגמאות שמהווים מדד אמין אוקיי? אז בתחום הזה מדובר פה בעצם על איזשהו Collection ומכיל כל מיני מסמכים ואז על ידי ה-Collection הזה אנחנו יכולים למדוד כמה השיטה שלנו מוצלחת אוקיי? אז זה הרעיון ובשביל לעשות את זה שוב יש פה קצת חזרתיות וחיתוך עם לימדת מכונה אז אנחנו נדבר פה נתחיל מלדבר פה על שני מדדים שבו גוהה לימדת מכונה מכירים, עוד רגע נראה את הקשר אלינו, שזה ה-Precision וה-Recurl אוקיי? מה הרעיון? הרעיון הוא שאנחנו צריכים בשביל הדבר הזה זוגות, יש לנו שאילטה ומסמך ותחשבו על בעיה ציווג בינארית שהמחלקה חיובית כאן זה מסמך רלוונטי לשאילטה המחלקה שאילית זה שהמסמך לא רלוונט אוקיי? זה דרך פשטנית מאוד להסתכל על התוצאה יש לנו מדדים יותר מתוחכמים אבל לצורך העניין חזרו לנו אחרי כל הרנקינג כל השיטות הכי מתוחכמות שראינו עד עכשיו, חזרו לנו נגיד עשרה מסמכים בסדר מסוים, עכשיו אנחנו בוחנים כל תוצאה אם היא רלוונטית או לא רלוונטית ובמקום להסתכל על סיווג של אובייקט, אנחנו פשוט מסתכלים לראות אם המסמך הוא חזר או לא הוא חזר אז נגיד שבדאטה קולקשן שלנו יש עשרים מסמכים רלוונטיים ואנחנו החזרנו אולי פחות מעשרים נגיד רק עשרה אוקיי? מתוכם יכול להיות שחלק הם לא רלוונטיים והחזרנו אותם כי השיטה שלנו תורה שיפור אוקיי? וחלק הם החזרנו אוקיי? אז המסמכים שהם רלוונטיים, כן יש תיוג ידני מראש אוקיי, אני לא אכנס פה מאוד לעומק של הדבר הזה אבל יש דרך מקובלות לטפל בדבר הזה לצורך העניין אפשר לשאול גם לפי איזה שילטות זה לא קל יש לך שאלה טובה, זה לא קל לענות עליה, אבל בוא נגיד דבר כזה בוא נגיד שהכנסנו כמות מאוד מאוד גדולה של שילטות, קיימנו עליהן כל מי שמופיע פה עצבא והחלטנו על מאה שילטות טובות אוקיי? עכשיו כל אחד מהם אנחנו נותנים מסמכים שונים והורכים עצבאה ביניהם אז זו שיטה מקובלת בעיקרון על מדידה של תוצאות אנושיות אוקיי? נגיד שאנחנו בוחרים לצורך העניין חמישה אנשים שיצגו אותנו וכל מסמך שחזר צריך להגיד פשוט רלוונטי או לא רלוונטי ואם יש רוב של שלוש מול שתיים אז נחליטים שהוא רלוונטי, פחות מזה לא רלוונטי זו שיטה שנקראת kappa measure למדידה אנושית ובוא נגיד שזה עבר איזשהו סינום כזה וכל מה שהוא רלוונטי זה לא שבן אדם אחד בודד על מה שהוא רלוונטי אלא כן? עשינו משהו פשוט מאוד כרגע בשביל לבחון אז לצורך העניין בוא נגיד שיש לנו כמות מסוימת של מסמכים, יש לנו עכשיו כמות מסוימת של שילטות עכשיו חזרה לשילטות הזאת שילטות אמורים לחזור 20 תוצאות חזרו פחות וגם אלה שחזרו יכול להיות שלא כל מה שחזר מה שמופיע פה כretrieved זה מה שחזר לא כל מה שחזר הוא רלוונטי אוקיי? אז אנחנו מגדירים ככה precision מודד לנו מתוך התוצאות שהחזרו מה אחוז שהם באמת רלוונטיים אוקיי? זאת אומרת מה שיש לנו פה בעמודות הדבר הזה אנחנו קוראים לו confusion matrix, שוב זה משותף ללמידת מכונה אז מה שמופיע בעמודות זה בעצם קובע את הרלוונטיות לפי המודל שלנו, אם זה היה vector space מודל או שזה היה מודל שסכם תוצאות לפי אחת מהמטריקות שדיברנו עליהם אוקיי? או סתם מודל בוליאני לא חשוב, אחד המודלים שדיברנו עליו, אוקיי? אז חזרה תוצאות אם היא חזרה זה אומר שהמודל אומר שהיא רלוונטית אוקיי? אז זה מה שהוא retrieved זה מה שיש לנו בעמודות ומה שמופיע בשאורות לצורך העניין זה התוצאות שה צוות המתייגים האנושי שלנו החליט שהם רלוונטיים, אוקיי? אז מה הם הדוגמאות שהם רלוונטיות מתוך כל אלה שחזרו? רק המשבצת הזאת אוקיי? מתוך כל אלו שחזרו שזה כל העמודה כאן אז זה בעצם מה שיש כאן הרלוונט זה מה שמופיע בתור true positives כאן והretrieve זה סך הכל כל אלה שהוא סימן כרלוונטיות והוא החזיר אותם גם אם לא באמת רלוונטיות אוקיי? אז מה אחוז הדיוק שלנו בעצם מתוך אלה שחזרו? זה precision, recall בעצם זה מדד כיסוי כזה כמה רלוונטיות בעצם מצאתה באופן יחסי אז אם יש 20 תוצאות, מה אחוז של תוצאות שמצאתה שהן באמת שייכות לקבוצה הזאת? אוקיי? אז בעצם אנחנו מודדים כמה תוצאות הוחזרו שהן באמת רלוונטיות זה שוב המשבצת הזאת, חלקי סך כל השורה הזאת בסדר? זה בעצם מה שאנחנו מודדים precision, recall, שוב מי שמכיר כבר יודע באמת דובר אבל עשינו התאמה קטנה ל-IR לכזור מידע אז זה מדד אחד, אם יהיה זמן נעשה אחרי זה גם דוגמאות יש מה, בוא נעשה עכשיו אוקיי? בוא נעשה דוגמאות מספריות אז יש לנו נציב רק בשביל לראות שזה ברור לא כולם פה למדו למיטת נכונה אז גם עם הרוב אני לא יכול להניח ש100% משופשפים אז בוא נעשה איזה דוגמא נגיד שוב זה מה שהמודל החזיר זה מסמכים שהמודל החזיר זה פשוט סופרים כמה מסמכים הוא החזיר אוקיי? ומה שמופיע כאן הוא לא החזיר לא חזרו מה השורה? מה? אה השורה נכון כאן עשות זה הפוך אז בכלל כל מה שאמרתי הפוך רטריב זה מה שחזר אז אם ככה זה הprecision false positive כן כתוב לי false positive אוקיי? אז שורה ראשונה כאן אוקיי? אני רגיל של confusion matrix ספוכה שורה ראשונה זה אומר שכל המסמכים האלה הוחזרו זה חזר שורה השנייה לא חזר אוקיי? אז אנחנו שואלים אתם זוכרים הסתברות מותנת טוב? אוקיי סבבה מה שכתוב פה זה פשוט ההגדרה של הסתברות מותנת זה true positive חלקי true positive ועוד false positive זה מצד אחד ומצד שני יש לנו את את חלקי true positive וחלקי false negative נגיד שחזרו סך הכל 200 מסמכים אוקיי? מתוכם נגיד 150 רלוונטיים יש לנו עוד 50 שחזרו שהם לא רלוונטיים אוקיי? ונגיד שיש לנו סך הכל 500 מסמכים אוקיי? אז אנחנו יודעים כבר להגיד ששאר המסמכים פשוט לא חזרו אז המודל שלנו החליט שהם לא רלוונטיים ונגיד שיש לנו כאן עוד 100 הוא לא החזיר אוקיי? זה לא שכאן צריך להיות 200 אוקיי? אם נסחר את המספרים שיש לנו כאן נקבל את כמות המסמכים שבדקנו שזה 500 אוקיי? בפועל בעולם האמיתי מאוד מאוד קשה לדעת את התשובה לשאלה הזאת כמה באמת מהמסמכים רלוונטיים ולכן מתייחסים אם רוצים למדוד תוצאות אמיתיות יותר למדד של precision מאשר recall והrecall זה מדד שהוא יותר משוארך כזה בגסות אוקיי? אנחנו לא יודעים באמת כמה יש לנו המון המון מסמכים לא יודעים כמה הם באמת רלוונטיים כי לא נעבור על הכל אנחנו מנחשים איזשהו מדד לפי התפ raft של האבר וכן יודעים להגיד מה היה נגיד אחוז הדוגמאות שחזרו ביחס לדוגמאות אחרות או מה הדיוק שיש לנו ביחס ביחס לדוגמאות שחזרו כי לעבור על 200 דוגמאות זה דבר אחד אבל לעבור עכשיו על נגיד 10000 דוגמאות לדעת מה רלוונטי זה כבר בדרך כלל יקר מידי אוקיי בכל מקרה בעיקר למי שלא מיומן האם מישהו מי שרוצים לחשב מה פרוציז'ן והריקבול כאן פרוציז'ן ופרוציז'ן בוב לאיזה מאה? לא אמרת לי אבל מה זה הפרוציז'ן הפרוציז'ן נכון אז זה הפרוציז'ן 0.75 נכון וריקבול אוקיי בסדר מישהו לא הבין את זה אוקיי לפעמים מכניסים פה עוד מדד נוסף אוקיי כן ה-F1 שזה בעצם מוצא הרמוני בין המדדים האלו מי ש אז זה הנוסחה שלו שתי TR TR זה פרוציז'ן וריקבול ו-K פרוציז'ן ועוד ריקבול אוקיי המדד הזה הוא טוב ולא טוב מה הבעיה במדד הזה הבעיה היא שאם יש לנו נגיד הרבה שאילתות ולכל אחד רק תוצאה אחת שבדקנו נגיד אוקיי הרסנו עכשיו אלף שאילתות וחזרה תוצאה בודקים רלוונטי או לא רלוונטי נגיד היום לוקחים רק את הדרורות גבוה ביותר אז זה היה הגיוני להשתמש פה בפרוציז'ן וריקבול בשביל למדוד את הדבר הזה אבל מה קורה בפועל גם אם אנחנו נסתכל על מצב מחמיר שאנחנו מסתכלים רק על דוגמאות ראשונות שחזרו במנוע הפיפוס אנחנו לא נבדוק בדרך כלל רק את התוצאה הראשונה נגיד דילגנו על הפרסמות שהן אולי פחות רלוונטיות ונסתכלנו על תוצאות האמיתיות הראשונות שחזרו אז מה בדיוק פרוציז'ן וריקבול מודד ביחס לכמות של התוצאות כי יש חלק מהתוצאות שהן רלוונטיות וחלק שלא, אז איך נגדיר את זה אז בשביל זה אנחנו צריכים אפשר להשתמש עוד בפרוציז'ן וריקבול אבל בצורה קצת שונה אז יש לנו דבר כזה שמדד שנקרא precision at k מה זאת אומרת? אנחנו מסתכלים על k הרנקים הראשונים אוקיי ואנחנו בודקים לגביהם איך המדדים שלנו משתנים, הנה בוא נסתכל על דוגמה יש לנו k שהוא שווה ל-5, אוקיי הירוקים רלוונטיים והאדומים לא רלוונטיים אוקיי אז אנחנו אומרים מה precision at k הפרסיז'ן שלנו בחמש דוגמאות שחזרו הוא 60% במקרה הזה אוקיי זה לקחנו אותה חמש הציונים הגבוהים ביותר או חמשת המסמכים אם הדירור הוא גבוה ביותר ובחרנו מה הפרסיז'ן נגביהם precision at 3 במקרה הזה היה נותן לנו 66% כן אנחנו יכולים לבדוק ולהעלות ולהעלות ולהעלות את k כל פעם ולמדוד מה הפרסיז'ן בציונים השונים שחזרו כן אז יש לנו כאן 50% בפרסיז'ן at 4 60% וכן הלאה אוקיי אז יש לנו מדדים שונים כאלו ומה שאנחנו הרבה פעמים נעשה זה נעשה ממוצע של precision at k להרבה שאילתות אוקיי זה מה שנקרא mean average precision וגם כאן אנחנו יכולים לעשות גידול של השוני בכמות התשובות שחזרות ולראות איך זה משפיע על ה precision at מדד מסוים כאילו מה יהיה הממוצע בחמש תוצאות, מה יהיה הממוצע בשלוש תוצאות וכן הלאה אוקיי אז זה בעצם יכול לעזור לנו למדדים השונים האלה עכשיו כאן עשינו נגיד ממוצע של k משונים אוקיי יש לנו פה השוואה גם באותו אופן אנחנו יכולים לעשות recall at k אוקיי כמו שעשינו precision אוקיי בודקים את ה-k תוצאות הראשונות ועל זה בודקים recall אוקיי כמובן שיש איזושהי ציפייה שככל שהרנק יהיה יותר גבוה אז התוצאות יהיו יותר רלוונטיות כי אחרת הרנק שלנו לא שווה כלום אם זה לא נכון בכלל אוקיי כי נוכל להביא k שהוא שווה לגודל הקולקשן שלנו אוקיי צריך להגיד אין מסמכים, החזרנו את כולם קיבלנו recall של 100% זה לא מעניין אבל אוקיי אז ככה אנחנו יכולים לעשות ממוצע בין ה-precision ו-recall השונים ולעשות את זה על הרבה שיילדות גם וככה אנחנו מקבלים התוצאות הנה יש לנו average precision של query 1 average precision של query 2 ו-mean average precision שלוקחים את זה על שילדות מרובות עושים את הממוצע של average precision על שילדות וודדות בסדר הבנו את הרעיון אז יש לנו precision at k ואז בוחרים k עם שונים עושים ממוצע בוחרים נגיד עד רנק 10 מעניין אותנו עושים ממוצע על ה-precision מהתוצאה הטובה ביותר עד לתוצאה העשירית יש לנו 10 תוצאות למצא ועושים הרבה שיילדות ועושים ביניהם ממוצע אוקיי אז זה ה-mean average precision שלנו אוקיי לא ניכנס עכשיו למיקרו ומאקרו ארכאדג' אז זה בעצם מדד מרכזי אחד שדיברנו עליו ו... יש לנו עוד הרבה מדדים נדבר על עוד מדדים אחרים בהמשך הקורס ועכשיו אנחנו רוצים לשקלל עוד פקטור נוסף בואו נחשוב רגע על הפלואו שהיה לנו אולי ממש בהתחלה אל תסתכלו לרגע אוקיי אז דיברנו בעצם על כל מיני דברים שקשורים פה בפלואו אוקיי אבל יש פה עוד רכיב נוסף אנחנו עושים פה גם רנקינג אני לא זוכר אם זה זה לא מופיע בפלואו הזה שהצגנו כאן אנחנו עושים גם רנקינג ורק אז מציגים את התוצאות אנחנו לא נציג את כל התוצאות רק את הכי הטובות ביותר דיברנו על דרך למדוד גם את התוצאות לקחת איזה טסט סגור ולבדוק עבור קורצה של קווריז מה היו התוצאות וככה נקבל מדד המין כמה המודל שהיה לנו פה של ההחזור אוקיי ועכשיו אנחנו רוצים לשקלל משהו שהרבה פעמים עושים את התאמה ליוזרים של אותו מנוח היפוס בוא נגיד שאם לא היו משתמשים שונים אז יש לנו צוות של מומחים שבדק מה התוצאות ובזה גמרנו אוקיי יש לנו מודל טוב יותר או טוב פחות ניתנו לו ציון גם אוקיי נגיד לפי מינה וזה המודל הזה מקבל ציון 80 וזה מקבל ציון 60 בסדר יופי עכשיו אנחנו השתמשנו במודל ובפועל אם אתם השתמשתם במנוח היפוס אונליין אז לחלק מהתוצאות יניינו אותכם יותר החלק פחות ויש המון דרכים למדוד את זה אז עכשיו נראה את האולי הפופולרי יותר וכל הנושא הזה נכנס לנו גם את הנושא הזה נרחיב אבל זה שהוא מבוא לפידבק מהמשתמש אוקיי ברמה העקרונית אנחנו יכולים לחשוב על סנאריום מרכזי אחד וזה השימוש של כל אחד מאיתנו נגיד במנוח היפוס אנחנו רוצים שאחרי הרבה שימושים זה יתאים אישית לנו אוקיי נגיד שאתם אני אתן דוגמה איפה זה מתאים נגיד שדיברנו בשיעור הראשון אולי גם שיעור השני הזכרנו על מילים שיש להם כמה משמעויות אוקיי עכשיו חזרו כמה תוצאות שחלק מהן קשורים למשמעות אחת חלק למשמעות שנייה אם נגיד אנחנו מתעניינים במקומות בלא יודע מה זנים של תפוחים אז המילה אפל שקשורה לתפוח תעניין אותנו יותר ואנחנו נקליק הרבה על תוצאות שקשורות לתפוחים אם מעניין אותנו נגיד לעקוב אחרי שווי של מנהיה כי הרווחתם כל כך הרבה כסף בהייטק שאתם חייבים להשקיע את זה בבורסה אז נתעניין נגיד בשווי של מנהיה של אפל ולא באפל כתפוח ונקליק על תוצאות אחרות ואז אנחנו רוצים שיהיה התאמה לעניין שאנחנו איבה אוקיי אז זה פידבק מהמשתמש יש כל מיני דרכים שאתם מכירים על פידבק מהמשתמש כל מיני אפליקציות שמנסים בכוח להגיד שאהבנו את זה אני לא יודע מה ראיתם תוכנית טלוויזיה ומבקשים לכם לעשות לייק ודיסלייק כל מיני כאלה אבל יש דרכים שהן אומנם מרומזות אבל הן הרבה יותר ישירות שמקובלות להוציא את הנתונים האלו והדרך הראשונה היא לבוא ולהסתכל האם נכנסנו לכישור הזה או לא נכנסנו לכישור הזה אם נכנסנו לכישור הזה אז בעצם באיזושהי דרך אמרנו התוצאה הזאת מעניינת אותנו אפשר גם להתווכח ולהגיד כל התוצאות היו גרועות למרות שזה היה גרוע אבל נגיד שזה לא המצב אז ברור שכשאנחנו יש כמות מצוימת של תוצאות הכי טובות ואנחנו נכנסנו לראשונה לשלישית ולשמינית אז הן הכי מעניינות אותנו כן אז בעצם אנחנו מודדים את הפידבק במשתמש על ידי קליקים נכנסנו לכאן נכנסנו לכאן נתנו בעצם את המדד שלנו היה אפשר גם להגיד בצורה ישירה כמו שיש לנו כאן זה סביר זה טוב, כן ממש בצורה ישירה אבל גם בלי לבקש מהמשתמש שאם אתם מכירים את עצמכם אז אפשר לנחש שממש לבקש מהמשתמש לדרג כל פעם את התשובות שהוא מקבל זה זה קצת לא ריאלי בוא נגיד לבקש את זה אבל אם הוא פשוט יכנס לתוצאה כי הוא רוצה עוד מידע או לראות ממש את המשמח זה משהו שהוא פשוט תוך כדי שימוש במנוח היפוס זה משהו שהוא מתבקש שיקרה אז זה בעצם דרך אחת על ידי הקליקים שלנו אוקיי זה כמה קליקים קיבלו הלינקים השונים אוקיי דרך שנייה שהיא דרושת אפשר גם אותה לעשות אבל היא דרושת ציוד מיוחד אפשר אולי לתכנת משהו פרימיטיבי כזה לטלפונים שלנו אבל כנראה לא יתאפשר לעשות דבר כזה בלי הרשעה של המשתמש אם כל אחד יש לו מצלמה בטלפון והוא מסתכל ככה על הטלפון שהוא גולש אז בעצם מה אנחנו מודדים אנחנו מודדים לפי הזווית שעיניים שלנו מסתכלות על איזה מקום בתוצאות בעצם התעניינו יותר אוקיי אז יש לנו כבר מטריקה עשית טיונינג למה שאני אמרתי סבבה אני יודע שלא לא לא לא אמרתי זה בביקורת סבבה לא אני צריך להתנצל ולא אתה גם אני לא אוקיי השיטה הזאת לפעמים מודדים איזה שוב שיטה יותר קלה לעשות את זה זה פשוט לראות איפה החבר שלנו היה או אפילו יותר ישיר ואולי קצת פחות מדויק אבל הרבה יותר ישיר אם עכשיו אנחנו גוללים על דאפנסים לא משנה אם במחשב או בטלפון ויש אזורים שאנחנו לא רואים כי הם לא נמצאים עכשיו במסך שלנו הם מתחתיו או מעליו אז איפה שאנחנו נמצאים עכשיו ואנחנו משקיעים זמן אז זה אזור שאני חושב שיותר מעניין אותה בתוך המסך הארוך שאפשר להציג אז זה בעצם שתי שיטות ישירות לא אמרתי עדיין איך לשקלל אותן לא נכנס לזה כרגע שאנחנו בעצם יכולים לשקלל חליט בעקבי המשתמש אז זה קליק ובעצם אזורים שבהם התמקדנו במסך אז זה בעצם כל אלה נכנסים לתוך הקופסה של ההתנהגות של המשתמש שהיא שווה בעצם אפשר בעצם להכיל אותה על אפליקציות מסוגים שונים או כמעט על כל אפליקציה בעצם או גלישה באינטרנט פשוט לדעת היינו באזור מסוים מעניין אותנו הקלקנו על לינק מסוים אז כנראה שהלינק הזה מעניין אותנו אז כל מיני חברות שמתעסקות בפרסמות למשל אז הקליק רייט כלומר כמות הקליקים היחסית למשל זה נחשב מדד שהוא מאוד נלקח בחשבון כל הזמן מנסים לעשות אופטימיזציה איך לשטוף לנו את המוח קצת יותר בזאת פרסמות אז זה אחת מהשיטות שעושים מגלים להם פשוט על ידי הקליקים אוקיי אז בעצם נסכם את מה שהיה לנו עד עכשיו דיברנו בסך הכל בשיעורים האחרונים על כמה דברים על קודם כל על מה אנחנו מתעסקים בו איזה דאטה מתעסקים פה ביחזור מידע קלאסי שאנחנו מדברים על טקסטים מה זה טקסט איזה פרוססינג אנחנו עושים למסמכים ולשאילטות שלנו איזה הכנות אנחנו עושים איך שומרים את הנתונים איך אנחנו אחרי זה עושים להם יחזור איך אנחנו עושים רנקינג אוקיי אז דיברנו על מודלים בוליאניים לקבל את התוצאות שהם מאוד עם חוקים כבדים כאלו מאוד נוקשים על שקלול הטרמים המשותפים בדרכים מגוונות ועל דמיון וקטורי ואחרי זה דיברנו על הערכה בעזרת בנשמארק של המודל שלנו ועל שקלול של יוצר של המשתמש אוקיי זה היה הנושאים שעד עכשיו פחות או יותר דיברנו עליהם מה הלאה ניתן סשן של כמה דקות עוד רגע קצת אלפנדס למי שלא מכיר ועוד רגע נעשה את זה מבחינת עוד נושאים אז דיברנו אני כאילו מבקש גם קצת את עזרתכם דיברנו על BM25 כאופציה לניקוד דיברנו על TFIDF לפני זה היה עוד משהו אחד לדעתי שהזכרנו את הדברים סבבה אוקיי זה שלושת הדברים שדיברנו עליו מה כן אני חושב גם על תרגילי בית זה נראה לי שלושה שרלוונטיים לממש ו שבוע הבא אנחנו בעצם על שלושת השיעורים שהיה לנו עד עכשיו נעשה כזה בוחן במודל על מה נשאל אז זה יכול להיות על כמה דברים זה יכול להיות על או חישובים פשוטים מהדברים השונים שדיברנו עליהם ולהבין מה זה אומר או ככה סימולציות של שלבים שונים באלגוריתם או משמעות של מושגים שונים נגיד מה המושגים האלו ואלו קשור לפריפרוססינג יהיו כמה רמות של שאלות זה לא אמור להיות משהו מאוד כבד ומאוד ארוך אלא ככה משהו מסכם חלק קצת יותר קשות אבל לא שאלות קשות לפחות זאת לא הכוונה זה פחות או יותר אז אני רגע אפסיק את השיתוף רק מי ש...