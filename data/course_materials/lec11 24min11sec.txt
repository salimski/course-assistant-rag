טוב, ברוכים הבאים להרצאה, טוב, אז שיעור שעבר דיברנו על language models ועל large language models אני מזכיר שבעצם דיברנו על language models עד קודם, הסברנו גם מה זה מודל אפילו עם n-grammים וגם למשל עם tf-idf, ואיך שהנושא הזה של language models הוא עתיק כמה עשרות שנים אחורה, גדל יחד עם החזור מידע ודיברנו שבוע שעבר גם על contextual models, בעצם מה שאנחנו אומרים, המטרה שלנו בlanguage models זה לחזות טקסט יש לנו בעצם, בוא נגיד, מטרה כפולה ב-large language models, זה גם דרך חדשה לייצג טקסט נגיד, התחלנו עם מילים או עם טוקנים ואחרי זה אנחנו מגיעים גם לטקסט, ומשם בכלל ברגע שראינו שאפשר לייצג את הטקסט, אז זה התפתח לעוד מיני עולמות נוספים זה דוגמה של התייחסות לתמונה, אז המודלים של הייצוג הוקטורי לתמונה היה בהתחלה cnn, אחרי זה אנחנו נראה גם מודלים יותר איך בעצם הטכנולוגיה של הטרנספורמרים, שזה הטכנולוגיה המרכזית עד היום בללמים, בעצם חילחלה בסוף גם לכל התחומים, בעצם היום היא השולטת כמעט בכל תחום אז אנחנו נראה בעצם פיתוח של הדבר הזה, יש לנו פה דוגמאות של כל מיני ללמים ואנחנו נראה הרחבה של הדבר הזה עוד מעט אוקיי אז זה היה ללמים לא רק במובן של טקסט, כשדיברנו על contextualize ללמים, הבעיה המרכזית שהיה לנו עם ללמים הכותרות עובק הייתה שהם היו סטטים, לא היה לנו קונטקסט אז מה עושים בשביל לכל זאת לשמור על קונטקסט, אז בשביל זה הרכיבו את הטכנולוגיה של טרנספורמר ונזכיר בקצרה מה זה אומר, זה קצת מסובך אבל נזכיר את ההיילייטס אז בעצם יש לנו ייצוג של שלוש וקטורים, q, k וv, query, key וvalue שאנחנו אומרים דבר כזה בעצם זה המנגנון של ה-self-attention, אז זה מה שאנחנו נסביר עכשיו אז זה הציון שיש לנו בסוף, אנחנו בעצם מדברים על איזשהו כפל וקטורים ומטריצות ועושים את ה-self-attention הזה הרבה פעמים, מה שנקרא multi-head-attention אז מה זה ה-q, k ו-v האלה, אם אנחנו מתחילים מאיזשהו טקסט, כאן יש לנו לא יודע כמה רואים את זה אצלכם ג'ון recently brought, הייתי אומר שזה אמור להיות an apple mobile והדבר הזה עובר דרך wq וwk, עוד רגע נסביר מה זה הדבר הזה אנחנו עושים איזשהו מנגנון של softmax, התפקיד של הsoftmax נגיד באופן כללי לא נדבר בדיוק מתמטית על הדבר הזה כרגע, אבל זה איזושהי פונקציה שלוקחת את ה-output בין q ל-k ובעצם עושה איזשהי סוג של הסתברות עבור כל אחת מהאופציות בעצם ברשתות נוירונים בהתחלה התחילו בכלל עם משהו שלמדנו שנקרא perceptron שבעצם הoutput שם זה דוגמה, זה מינוס 1 או 1 או 0 או 1 output בינארי, משם עברו מהר מאוד מהperceptron לlogistic regression שמשתמש באיזה פונקציה רציפה, היה לזה את היתרונות שלו שלא נחזור עליהם כרגע, הבעיה היא שזה הטעים לoutput בינארי אז בהתחלה גם ברשתות נוירונים השתמשו בהרבה נוירונים עם אותה פונקציית אקטיבציה של sigmoid שבעצם איך נדע כל אחת מהנוירונים בשכבת הפלט ייצג לנו קטגוריה אחרת הכי קל להדגים את זה עם קטגוריות שמייצגות את הקטגוריה באובייקט בתמונה נגיד האם האובייקט הזה זה חתול, כלב, מטוס ושדומה כל מיני רשימה סגורה של קטגוריות וזאת שתקבל את הפסיון הגבוה ביותר היא זאת שאנחנו נבחר וסווג את הדוגמה מה שעשו בsoftmax בעצם זה שינו את זה והתייחסו על הכל כיחידה אחת בעצם אנחנו רוצים לדעת מהי הקטגוריה המסתברת ביותר וזה מה שsoftmax עושה בניגוד להתייחסות של כל אחד מהנוירונים כיחידה בפני עצמה שהיא לא קשורה ליחידות האחרות פשוט כאיזה heuristica בחרו את הנוירון שייצא את הoutput עם הציון הכי גבוה אז בsoftmax מתייחסים להכל כיחידה אחת ונותנים בעצם משהו שהוא מעין פונקציית הסתברות לכל האפשרויות ביחד זה מה שsoftmax עושה, מה הקשר לענייננו אז לענייננו q וk בעצם המטרה שלהם זה להגיד כל אחד מהמילים פה נגיד שכאן אנחנו מדברים על סך הכל קונטקסט של חמישה מילים מילה אחת חמש מילים להגיד כמה כל אחת מהאופציות משפיעה על האופציות האחרות אז בעצם כאילו q זה איזשהו shelter ששואלת כמה כל המילים האחרות משפיעות עליי והתפקיד של k של הq זה לענות על השלטה הזאת ועונים על זה בצורה של כפל בין ה-weights של q ול-weights של k נוצרת לנו איזושהי מטריצה לכל מילה כמה כל המילים האחרות השפיעו ובסוף אנחנו מכניסים את זה לsoftmax והדבר הזה נותן לנו הסתברות של כמה כל מילה השפיעה בין 0 ל-1 סך הכל מקבלים 100% עוד רגע נראה איזה דוגמה לדבר הזה את הדבר הזה אנחנו בעצם מכפילים בעוד וקטור משקלות של value שזה מה שייתן לנו בסוף ההרכבה של המנגנונים האלו זה מה שייתן לנו בסוף את ה-contextual embeddings אז ככה זה נראה יש לנו כאן עוד איזושהי דוגמה Fluffy blue creature roamed the verdan forest אז אנחנו רואים כאן שזה creature מבחינת הסטטיק אמבדינג זה מה שאנחנו עושים ישר בהתחלה של ההשאלה של אותו transformer ואז אנחנו מוסיפים אליו את ה-fluffy ואת ה-blue וזה מה שיוצא לנו בסוף כאילו כמובן שזה רק מדמה שזה fluffy blue creature זה הרכיב את הכל ביחד ואותו דבר כאן יש לנו forest ולעומת זאת verdan forest נותן לנו את מה שיש כאן לא יודע כמה רואים את התמונה שמנגינה על זה אז בעצם כמו שאמרנו אם נרצה ה-query שואל האם יש איזשהו כן כאן בפשטות שואלים האם יש adjective שמתארים נעם adjective זה תואר השם אז זה לא חייב להיות adjective זה בעצם כל המילים בקונטקסט כאן הן משפיעות עליי אז כאן בסוף התהליך לצורך העניין אנחנו רואים שיש לנו השפעה גדולה של המילים האלו על סך הכל זה עדיין לפני שנרמלנו את הכל בנפס לאחד ואמרנו כמה כל מילה משפיעה אוקיי אז זה ההכפלה של ה-k כאן בכל שורה לבין ה-q זה נותן לנו את המשקולות לכל אחד מהמילים כמה שהמילים משפיעות עליו אוקיי לצורך העניין כאן יש לנו תגומה לקריטשר ואנחנו רואים ששני המילים בקונטקסט שקיבלו ציון גבוה זה fluffy וblue מכל שהמילים, לא יודע כמה רואים את זה מקבלים פה ציונים שלילים שבסוף משותכים לאפס שאנחנו בעצם לא ניקח אותם בחשבון בכלל או שנעשה איזה משהו שיקח אותם מעט מאוד בחשבון אוקיי אז זה מה שבסוף יוצא כאן קשה לראות אבל המילה הזאת מקבלת אחרי הפעלה של softmax 0.42 והמילה הזאת מקבלת 0.38 הכל ביחד כולל המילה עצמה של קריטשר לצורך העניין מחושב כחלק מהקונטקסט אם היינו רוצים להסתכל על זה כאיזה הרכבה וקטורית היינו אומרים שקריטשר אחרי שנרכיב עליו את שאר הדברים בסוף נקבל את הדבר הזה שנראה כמו ייצוג הוקטורי שאמור לייצג את הblue fluffy קריטשר אוקיי איך אנחנו עושים את כל התהליך הזה אז יש לנו תהליך שכל פעם חוזה את המילה החדשה לידי המילים הקודמות בעיקר זה עוד מנגנון שצריך לרסן שאנחנו לא, אנחנו רוצים בסוף לייצר מודלים שיש לנו שתי סוגים של מודלים בוא נגיד מודל אחד שמייצג מילים לידי הקונטקסט ומודל אחר שאנחנו התרגלנו לראות אותו אנחנו נתייחס לדבר הזה עוד מעט שהוא יוצר מילים חדשות אז המודלים שיוצרים מילים חדשות יכולות רק לייצרתם מילה חדשה לפי המילים הקודמות ולא לפי כל המילים בקונטקסט אוקיי אחרת זה כאילו רמאות זה לא יעבור טוב אבל המודל שמייצג מה המשמעות של המילה מבחינה וקטורית הוא יכול להסתכל על כל הסביבה אוקיי זה שני סוגים שונים של מודלים ולמה זה רמאות? כי בעצם כשהוא יוצר איזושהי מילה הוא כותב משהו והוא בעצם חוזר מילה ואז כל המילים שהיו עד עכשיו יישמשו למילה הבאה זאת אומרת במציאות אין לו את המילים בהמשך אז בעצם אם הוא עכשיו לומד גם על בסיס ההמשך לא יהיה לו את המידע הזה בפועל אז יקלקל לו את החיזוי של המילים הבאות כשהוא כותב מילה אז תמיד המילים הבאות קשורות להיסטוריה אז זה לא יעבור טוב ככה אז כמו שאנחנו רואים כדי שאפשר אפילו להתחיל לענות על תשובה יש לנו גם טוקנים התהליך של הטוקניזציה מיוחדים כמו למשל, כן זה לכל, יחד עם כל LLM יש לנו את הטוקניזר שלו, המיוחד שלו שהוא יודע לקרוא, אז נגיד לברט ול-GPT יהיו טוקניזרים אחרים ויש לו מלא תתי מודלים אז כאן נגיד יש לנו התחלה וסוף של משפט start of sentence ו-end of sentence לברט עצמו זה משהו קצת אחר נגיד אז יש לנו כל מיני טוקנים מיוחדים שתמיד יהיו ואז אין לנו בעיה או כשאנחנו מתרגמים בין שפות לתרגם את התחלת המשפט פה לתחלת המשפט שם ואז בעצם המילה הראשונה היא בעצם כאילו המילה השנייה כי יש לנו את התחלת המשפט וגם, כלומר הוא לא מתחיל עם אף פעם מ-0 יש לזה עוד משמעויות נוספות של הסיום וגם אפשרויות לקבוע בעצם יש פה הרבה שאלות שנשאלות מה קורה אם אורך טקסט משתנה בשביל זה ייצרו תווים מיוחדים של padding רק בשביל שכל הוקטורים יהיו באותו אורך ולא יהיה בעיה ככה לטפל באורחים שונים או לחזות אורחים שהם לא מגיעים לאורך המקסימלי זו הצורה שבה מתמודדים עם זה משפטים באורחים שונים וכאן אנחנו רואים דוגמה זה לא הדוגמה שאז רציתי להראות לא משנה, יש דוגמה דומה זה פשוט דוגמה של k כפול q אבל יש דוגמה נוספת שמאמנים מודל לתרגם נגיד בשביל תרגום או בשביל משימות שלוקחים רצף של אינטות אחד שנקרא רצף לרצף נגיד לוקחים משפט בשפה אחת ורוצים להמיר אותו לשפה אחרת אז יהיה לנו q בשפה אחת ו-k בשפה אחרת ככה אנחנו בעצם יכולים עוד מה שנקרא cross attention בין שתי שפות, לאמן אותן ביחד ולייצר משפט משפה אחת ושפה אחרת אז זה ככה בצורה פשוטה מה שאנחנו עושים יש לנו למשל בשביל הפשטות שני טוקנים לא יודע כמה זה, לא כל כך יצא טוב זה token a וtoken b לכל אחד מהם יש לצורך העין שלוש weights query, key וvalue אנחנו מתחילים בהכפלה של ה-query ב-key אז יש לנו שני tokenים שתי הכוריז מוכפלים בשני ה-keys וקטור של שני כוריז מוכפל בוקטור של שני keys ויוצא לנו בעצם מטריצה של שתיים על שתיים ובעצם זה מעניין לראות גם שלא היה איזושהי ציפייה שהמילה עצמה היא הכי תשפיעה אבל זה לא תמיד ככה יש מילים שבעצם הם לא הפקטור מרכזי בקביעה של משמעות של עצמם אוקיי, כאן זה מה קורה אם אנחנו מגדילים את זה לחמש מילים אז בואו נראה אם יש לנו כאן דוגמה כזאת אוקיי, אז אנחנו רואים נגיד שמילה apple משפיעה על עצמה ב-20% אוקיי, למה? כי כנראה האמבדינג הסטטי מקודד אולי את המילה תפוח ולא apple בתור חברת אפל ואז מה שגורם לו בעיקר למשמעות זה המוצר אייפון במקרה הזה מקודד אותו למשמעות של חברת אפל אז למרות שהמילה עצמה נמצאת כחלק מההשפעה הסמנטית של עצמה רואים שבדוגמה הספציפית הזאת היא לא הפקטור מרכזי להבין את המשמעות של עצמם אוקיי, אז זה contextual embedding וחלק מהמנגנון הזה לומד שכאילו עוד הגדילו והגדילו את הקונטקסט אבל חלק מהיכולת להתמודד עם קונטקסטים גדולים זה להגיד לא כל המילים חשובות יש כאלה חשובים יותר, יש כאלה חשובים פחות אגב, המנגנון הזה מראה למה קשה כל כך להגדיל את הקונטקסט בגלל שאנחנו בעצם מדברים על פקטור ריבועי לגודל הכלת אנחנו מכפילים בעצם, אנחנו יוצרים פה מטריצה ריבועית ביחס לגודל הכלת של כל מילה כמה היא משפיעה על כל מילה ולכן בעצם אם יש לנו קונטקסט בגודל n אנחנו מדברים על n בריבוע ולכן קשה כל כך להגדיל את הקונטקסט זה משהו שכל הזמן אולי שמעתם שמדברים עליה אז בעצם אמרנו כמה כל מילה חשובה עוד משהו שאנחנו רוצים לקודם זה את המיקום של המילים עצמם זה עוד בתהליך הראשוני אחרי שהוספנו את האמבדינג הסטטי שאתו למדנו, אז אנחנו עכשיו רוצים להוסיף את המיקום שלו בתוך הרצף של התווים אז בשביל זה היום יש לנו מודל כאילו רכיב שלומד את הדבר הזה אבל משהו שעובד לא רע שעשו אותו בהתחלה זה לייצר בעצם להוסיף זה משהו שיוצר, מחברים אותו, עושים חיבור פשוט של מטריצות ושל weights לאמבדינג המקורי, כאן עושים גל אחד של סינוס ואחד של קוסינוס לכל אינדקס איזוגי של המילים נותנים את אחד מהם, אני לא זוכר איזה סינוס ואיזה קוסינוס אז לכל אינדקס אחד נותנים את אחד מהערכים סינוס של אותו ערך ולאינדקסים הזוגיים נגיד את הקוסינוס של אותו ערך וכך בעצם בגלל שיוצרים את זה באוטפוט באותו מימד אפשר פשוט לשבר ביניהם ואז זה משנה קצת בעצם את הערך של האמבדינג הסטטי שהפכנו אותו לקונטקסט של היונג ביינג, אז זה עוד פקטור שדיברנו עליו כאן אנחנו רואים את ההבדל בין עטנשן רגיל לקרוס עטנשן יש הרבה מודלים שעושים קרוס עטנשן, תרגום זה משימה הכי אינטואיטיבית וקלה אבל כמובן זה יכול להיות כל שני רצפים שרוצים ללמוד מרצף אחד לרצף אחר כאן אנחנו רואים אנגלית וצרפתית, אז זה מה שדיברנו עליו זה הקרוס עטנשן הזה שדיברנו עליו אוקיי, אז זה מראה לנו ככה סיכום של התהליך יש לנו כאן את האינפוט, חילקנו אותו לטוקנים אחרי זה עשינו אמבדינג לטוקנים האלו ואוספנו את הפוזישונל אמבדינג ובסופו של דבר קיבלנו את הקונטקסט של אמבדינג זה מה שהתהליך הזה יצר לנו אוקיי, אנחנו עוד נדבר על הפיינטיונינג הזה זה כן משהו שמעניין אותנו אז ברמה עקרונית רק נזכיר בשתי מילים אבל אנחנו נחפור על זאת בהמשך ברמה עקרונית יש עוד כמה דברים שמעניינים אוקיי, אז דיברנו על זה שהמודלים האלו משמשים אותנו לייצוג מילים ואולי לחזות את המילה הבאה, או את המשפט הבא אז בעיקר המילה הבאה זה הייעוד המקורי של אותם אלה אוקיי, אז היום אנחנו יודעים שאלה משתמשים, אחרי זה נראה גם כל מיני משימות להמון המון מטרות למשל רוצים לדעת, נגיד שאנחנו נמצאים בתוך הזה צ'טפוט שתומך בלקוחות ורוצה לדעת אם הלקוח כועס או לא או שהוא נגיד רוצה לתת פידביק חיובי אם הוא כועס צריך לטפל בזה אולי בתחיפות אז בעצם המציאו איזשהו תו מיוחד כאן שאנחנו רואים זה השיטה של ברט, יש כל מיני שיטות זה תו מיוחד שנקרא, טוקן מיוחד שנקרא CLS שזה קיצור של classification שזה בעצם מאשר לכבילנו לקבל את כל הרצף של ה-input ובסוף לתת איזה חיזוי אז בעצם התו המיוחד הזה זה משהו שאנחנו נוסיף למשפטים שלנו כאילו אנחנו נעשה טוקניזציה ואז אותו מודל שלקח אולי למי שיצר אותו חודשים להימן אנחנו קוראים לו pre-trained אוקיי? יש לנו פקטורים, אם נרצה לקחו להם 90% זפויים כאלה ועכשיו אנחנו רוצים להשתמש בהם לא בשביל המשימה המקורית שלהם שאגב גם בשבילה היה לנו כל מיני תווים מיוחדים שכדי לחזות את התו הבאה עשו מה שנקרא self-supervised learning עשו בצורה שהיא קשה לעצמה, אפשר להעביר לא מעט זמן להסביר אותה אבל לצורך העניין החליפו חלק מהמילים בטוקן מיוחד שנקרא mask לצורך העניין לחשוב עליו בתור איזה פילטר כזה, בתור מסכה איך אתם מכירים את זה? מכל מיני תחומים של מדעי המחשב שרוצים למסך נגיד בתקשורת חלק מהIP או משהו כזה אז זה ניסחו חלק מהטוקנים ואז היה צריך לחזות אותם אז ככה השתמשו בזה ברצף המקורי לאימון ובשביל לייצר תשתית בעצם לשימוש באותם אלה למים שאולי לקח קודשים ליצור למשימות אחרות זה גם למשימה המקורית של Next Sentence Prediction השתמשו בזה אבל אנחנו משתמשים באותם תווים מיוחדים שהוקדרו מראש כמו CLS כדי למשל לדעת כמו שאמרנו האם הסנטימנט של הבן אדם שמדבר מול Chatbot לספורט האם הוא חיובי או שלילי לדבר הזה אנחנו לא בעצם בונים מערכת מ-0 אנחנו בעצם מכינים אותה למשימה שלנו ואנחנו בעצם ממנפים את ה-pre-trained וקטורים האלה שיוצרו ה-pre-trained מודל הוא יוצר וקטורים אבל הוא בעצמו מודל ה-pre-trained מודל