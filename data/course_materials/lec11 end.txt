משימה, ואנחנו כבר לוקחים כל מיני משקולות בתוך הרשת מיירונינג ההנפה הזאת, לא בדיוק רשת מיירונינג, נגיד בתוך המודל של ה-LLM שנוצר, בשביל להתחיל מנקודה מאוד מתקדמת למשימה שלנו. לדבר הזה אנחנו נקרא Fine Tuning. אוקיי? אז אנחנו נחזור על זה עוד, אבל זה רכיב מאוד מאוד חשוב, זה משהו מאוד מרכזי, כי זה אומר שנגיד עכשיו אנחנו מאמנים Fine Tuning לפעמים אפילו נגיד לכמה ימים, אבל זה כלום לעומת הכמה חודשים שהם השתגיעו לבנות את הפרי-טרנק מודל.

עוד איזושהי אמירה, שאני לא בטוח אם נחזור עליה עכשיו או לא, אבל יש לנו איזו הבחנה שצריך לעשות בין שני סוגים של מודלים, אולי כן נשתדל להכניס את זה, אבל יש את כל אלה כמו GPT-4 או את Cloud Sonnet 3, וכל המודלים המתקדמים של Gemini 3, כל אלו, שבעצם, כמו שאנחנו נסביר, הם המנוע המרכזי שיושב בתוך מה שאנחנו קוראים לו LLM, אבל בעצם זה Chatbot, שהארכיב המרכזי שלו זה LLM, פשוט Chatbot לכל דבר, היו Chatbots עוד לפני שהמציאו את ה-LLM, שבעצם חברות ענק כמו Google ו-OpenAI, כמו Facebook, כמו Microsoft וכדומה, יצרו לנו מודלים שצריך את המשאבים של חברה גדולה, וגם עם המשאבים האלה לקח להם אולי חודשים לייצר, בדרך כלל המודלים האלה יהיו סגורים, ואנחנו לא נוכל לעשות Fine-Tuning להם, ולמודלים שאו שהם פתחו בכל זאת, או מודלים יותר קטנים שהם היו באופן סורס, להם אנחנו נוכל לעשות Fine-Tuning למשימות שלהם. אוקיי, אז צריך להבחין פה בין מודלים סגורים גדולים מאוד מאוד, שמה שאנחנו נוכל לעשות להם ומולם זה רק פרומפטים, שאתם בטח מכירים כבר בטוח שכולכם השתמשתם באחד מה... מה שנקרא AI ULM, בעצם הכוונה לאחד מהצ'טבוטים של החברות הגדולות, אז בעצם תעשייה שלמה של פרומפט אינג'ינג, איך לייצר את התוצאה הכי גבוהה, אז נשתדל לדבר גם על זה קצת. אוקיי, אז זה וריאציות נוספות שהתפתחו מהמודל, אולי מהראשונים שהיה על טרנספורמר, היה בערך במקביל, את GPT-1 וברט מסרו במקביל, ברט הוא בכבוד פתוח ואפשר לעשות לו גם Fine-Tuning, GPT-1 אולי גם, אני לא בטוח, מתקדמים לא, אבל נראה לי שאחד ושתיים אפשר לעשות להם Fine-Tuning.

אוקיי, אז לזה אנחנו נוסיף שמאז דיברנו על זה, כשיש טכנולוגיה, אגב, זה כדאי לכם לשים לב ופתרונות שיהיה לכם אולי בתעשייה, ביום אחד שתעבדו בחברה שזה יהיה ברלוונטי, אז הרבה פעמים בכלל פריצות דרך באות, כאשר יש כלי אחד שעובד מאוד מאוד טוב למטרה מסוימת, אז בגלל שהוא עובד כל כך טוב לאותה מטרה, אז הרבה פעמים לוקחים את אותה טכנולוגיה, ומתייחסים לאלגוריתם, הם קוראים לזה רדוקציה, כשיש לנו איזה אלגוריתם לפתרון בעיה, ואז אנחנו לוקחים בעיה אחרת, והרכיב של פתרון הבעיה מהשלב הזה הוא פתור כבר, אז כל מה שנשאר לנו לעשות זה להמיר את הכלת שלנו, או לכלת שיודעים לפתור, ומפה ועלה אנחנו נכנסים לזה כבעיה כתורה, אז כשהמודל הראשון שעשה פריצה דרך משמעותית מאוד בדיפ לרנינג היה CNN, שהוא יוצר, ועד היום משתמשים בו המון בעיבוד תמונה, למרות הטרנספורמרים, אז לא החליף לגמרי בעיבוד תמונה את המודלים של CNN, אז הרבה פעמים לקחו נגיד חקר בביולוגיה חישובית למשימות מסוימות, לקחו אינפוטים ביולוגיים, שמייצגים איזה וירוס נגיד, או לא משנה מה, והמירו אותם לתמונה, ואז השתמשו ב-CNN לפתור את הבעיה, אז באותו אופן למרות שהאל אליהם היא מראש, זה מודלי שפה, אוקיי, אז בגלל שזה עובד כל כך טוב, משתמשים עכשיו לכמעט כל תחום אפשרי, שבעצם משתמש בלמידת מכונה, אז ימירו את האינפוט, ואיך עושים את זה נכון לכל דומיין זה גם שאלה, אז ימירו אותו לתצורה הוקטורית, שתעבוד בדומה ל-one hot encoding, שיש לנו כשלב ראשון במילים, אני אזכיר מה זה one hot encoding, יש לנו נגיד שלושים אלף מילים שבחרנו מתוך כל המילים בשפה מסוימת, לפעמים זה יותר, תלוי בכמה המודל שלנו גדול, זה גם אחד מהפקטורים, כמות המילים, אז מה שעושים זה יוצרים עוצר מילים ווקאבלרי, ואז כל מילה שמופיעה בווקאבלרי יש לה מספר סידורי משלה, נגיד מ-0 עד שלושים אלף פחות אחד, ואנחנו נדליק את הביט שמתאים למיקום שלה מתוך השלושים אלף ביטים, זה one hot encoding, זה בעצם אנחנו נותנים שילוב של כאלו, זה מה שמאפשר לנו את האינפוט למנגנון הזה של הטרנספורמר, אז באותו אופן אנחנו נעשה משהו דומה לכל אינפוט שאני רוצה להשתמש במנגנון של הטרנספורמר, כאן אנחנו רואים את האינפוט של, אנחנו רואים מה עושים בעצם בתמונה, כדי להפוך אחד מהמודלים המפורסמים היום, זה נקרא vision transformer, בראשי תיבות פשוט VIT, זה השם של המודל, זה גם המשמעות שלו אבל זה השם של המודל, מה שעושים פה עם התמונה זה מחלקים אותה לפצ'ינג, החלקים האלה שאנחנו רואים לצורך העניין נניח שזו תמונה של 256 פיקסילים על 256 פיקסילים, אז מחלקים אותה נגיד ל-8 על 8, לקוביות כאלו, ויש כל מיני דרכים לייצר בסוף תאר שאנחנו רואים כאן, על ידי נגיד שיטות שאנחנו קוראים להן pooling, עושים איזושהי פעולה על המטריצה, מורידים את המימדים, נגיד עושים ממוצע למשל, mean pooling זה אחד מהדרכים ואחד מהטכניקות, עושים נגיד ממוצע של הפיקסילים ומייצרים ערך אחד, אז בעצם כל פאצ' כזה הוא מה שמייצג לנו, הוא המקבילה שלנו לטוקן לצורך העין, אז מהתמונה, כבר דיברנו על זה שבתמונות, אנחנו אחרי זה משטחים את המטריצה הזאת לווקטור אחד גדול, ואז יש לנו בדומה לרצף של המילים שייצרו ווקטורים, על ידי one-hot encoding בתור input ל-LLM בטכנולוגיה של הטרנספורמר, אז גם כאן ב-vision חילקנו את זה לפאצ'ינג, וכל פאצ' הוא כמו טוקן, ואז יש לנו רצף של טוקנים לפי המיקום שלהם, כאן אנחנו רואים שבעצם יש לנו שלוש מטריצות, כי יש לנו את ה-RGB, אם אני שונו צבע, אז זה הפאצ'ים שלנו, ומשם אנחנו מתחילים את התהליך, שדומה לצורך העניין לתהליך הזה שיש לנו עם משפטים, אז יש לנו כאן את ה-output שמודגם לנו הפעם, תמיד כל כך קל להדגים תמונות, עכשיו אנחנו מדגימים את המנגנון של ה-self-attention, עם מילים כדי להבהיר מה קורה בתמונה, זה מה שיש לנו כאן, ואנחנו רואים שבעין, עושים ויזואליזציה, דווקא ה-CNN נראה יותר ברור מאשר ה-VT, ה-vision transformer שלנו, אבל למשימות שונות זה עובד יותר טוב, למשימות שונות זה עובד יותר טוב, ה-CNN יעבוד יותר טוב עם כל מיני features, זו הבנה ממש של מה שמודל עושה, שלא תמיד יש לנו ב-deep learning, אז הוא משתמש הרבה פעמים על המבניות בתוך התמונה, נגיד edge detection ודברים כאלו, וטקסטורה, גם CNN מטפל בטקסטורה בתמונה, לעומת זאת ב-VAT יש לנו features יותר כלליים, עדיין מגיע לתוצאות יותר טובות. עוד משהו שדיברנו עליו בעיבוד תמונה, בטכניקות שלפני עידן ה-deep learning, זה בעצם דיגיטציה של תמונות של טקסטים, בטכניקה שנקראת OCR אם זה טקסט מול פס, יש לנו גם handwriting recognition, HWR נדמה לי, רשיטיבות, אם זה נכתב, נכתב יד, וזה בעצם מודלים שיקחו טקסטים מהסוג הזה, והתרגמו אותם לסטרינגים, לטקסט שעבר דיגיטציה, אז איך עושים את הדבר הזה? גם כאן יש לנו חלוקה לפצ'ינג, כמו שראינו קודם בתמונה, יש לנו pre-trained encoder, שמתרגם את הדבר הזה באותו אופן שראינו קודם, לבקדורים, ל-embeddings, ואת הדבר הזה יש לנו decoder, שיודע לקחת את הדבר הזה, ולייצר מבקדורים כאלו, בקדורים של טקסט, אז יכול להיות שכאן נגיד יתאים, אותו מנגנון שראינו קודם של cross-encoder, אם אתם זוכרים, בצד אחד יש לנו את הפצ'ינג, ובצד השני, מכל הפצ'ינג האלו, אנחנו מייצרים רצף של אותיות. אוקיי, אז זה בעצם הדרך שבה אנחנו, וזו הדגמה של מודל ספציפי, של TR או CR, קיצור של transformer או CR.

אוקיי, אז ככה זה עובד כאן. מה לגבי אודיו? אז באודיו יש לנו, כמובן אני מדגים פה, מודלים מסוימים ספציפיים, יש עוד מלא מודלים נוספים, וכאן עשיתי קצת ספוילר, למה שיש אחרי זה, שקשור לIR, כי אנחנו בעצם מדברים, על אלה רמים באופן כללי, ובלי פוקוס מיוחד על IR אולי, למרות שהיו פה כמה נגיעות כאלו, אז כאן אנחנו מדברים, על כל מיני אפשרויות, מה שמודגם כאן, זה Speech-to-Speech, יש לנו גם Speech-to-Text, ו-Text-to-Speech, כל מיני מודלים כאלו, אז יש לנו פה איזשהו input, שעובר ASR, זה Automatic Speech-to-Air Recognition, והופך את זה לטקסט, בעצם בפנים, ואז הדיקודר, לצורך העניין, כאן זה כבר מערכת IR, אם נרצה, כאילו אנחנו מתקשרים, לא יודע אם יצא לכם, לדבר עם ה-Chat-GPT, או עם Gemini, או עם אחד המודלים אחרים, אז זה בעצם, יש פה שני שלבים, שאנחנו רואים פה בפנים, שלפעמים עושים מודל אחד שעוטף אותם, ולפעמים שתי מודלים, אז אפילו כמה מודלים, אז יש לנו את הסיגנל שאנחנו אומרים, נגיד בצורך העניין, למודל, המודל הופך את מה שאנחנו אמרנו לטקסט, הוא מייצר מתוך זה טקסט אחר, והוא מוציא באוטפוט ספיץ', אז בעצם היה פה, ספיץ' to טקסט, טקסט to ספיץ', לפנים, אז זה בעצם מה שהיה לנו כאן לפנים, ככה נראית המערכת, בעצם היא מורכבת, מערכת עכשיו יותר פשוטה, של ספיץ', כאן זה טקסט to ספיץ', מורכבת בעצם, לא, יש פה גם וגם, אז יש לנו ספיץ' אנקודר, אוקיי, אז זה השלב הראשון שאנחנו רואים כאן, וזה אנחנו, כמו שאנחנו עוד רגע נראה, זה יהיה דומה מאוד למה שראינו, אינפוט, באודיו, ריקוגנישן שהיה לנו לפני כמה שיעורים, אוקיי, בעצם, אנחנו עושים את אותו תהליך, שעשינו אז, רק בפי לייצר את האינפוט שממנו אנחנו לומדים, אנחנו לא עובדים על הסיגנל המקורי, אלא עושים לו פרוססינג, אוקיי, זה יהיה הפרוססינג שיהיה לנו על המודל, עוד רגע אנחנו נעשה ויזואליזציה של זה, ובפנים יושב לנו אותו large language model, הוא הופך, הוא מייצג את זה כוקטור, ואז יש לנו את הרכיב שבחזרה, מטקסט to ספיץ', שנקרא, והוא קודר, אז הוא בעצם הופך את הטקסט שבפנים נוצר, כתגובה לטקסט שלנו, זה בעצם כבר תהליך של, דומה לאל אל אם כזה שאמרנו כמו, Gmini, ChipGPT וכדומה, והוא מייצר מהדבר הזה, את הסיגנל של הווייס, כאילו זה התשובה שלו, אוקיי, אז באינפוט של הספיץ'נקודר, יש לנו בעצם שני אמבדינג, אמבדינג אחד, שהמטרה שלו זה להבין את מה שאמרנו, ואמבדינג שני שהוא מטפל באקוסטיקה, יש תהליך מרכזי שלא התמקד מבוה הרבה כשדיברנו על אודיו, שצריך לדעת לטפל בכל מיני סוגים של סביבות, יש סביבה שהיא אולפן הקלטות, אידיאלית, ובדרך כלל זה לא ככה, צריך לדעת לבטל את הרעש שיש ברקע, ולדעת להבחין בין הסיגנל המרכזי לבין הסיגנל של הרעש, אז בשביל זה יש לנו את האקוסטיק אמבדינג, ומהדבר הזה, בדרך כלל אנחנו נפסיק כאן, בגלל זה יש לנו את ה-X הזה כאן, זה בעצם לוקח כמו שאולי רואים שם, יש פה את ה-LogMail Spectrogram, בעצם הדבר הזה לצורך העניין נגיד שזה הספסטרום שלנו, לא נזכיר בדיוק מה זה ה-Mail Spectrogram, בסוף אנחנו קצת ירדנו מזה, אבל לצורך העניין יש לנו פה, כמו שאנחנו מכירים, מניפולציה לסיגנל המקורי, וזה כבר דיסקרטי, וזה יהיה בעצם האינפוט של הטרנספורמרט שלנו, ומהדבר הזה לצורך העניין מרכיבים מילים וכדומה, יש לנו כאן שמות של כמה מודלים, פתוחים או סגורים, ואמרנו בשלב הבא, יש לנו Loud Language Model, אלה להם שפתי, כמו למשל Llama של Meta, ועוד כל מיני אופציות נוספות שיש לנו כאן, Llama אם אני לא טועה זה פתוח, אבל זה מודל ענק של Llama, מודל מאוד גדול, ואז עוד שלב נוסף, התאמה של האור, איך עושים את זה אם אתם זוכרים, במודל כשדיברנו על טקסט אמרנו שעושים פדינג, אז גם פה צריך לטפל ברכיב הזה, כשרואים על ווייסטי קצת יותר מורכב, ואז יש לנו את ה-Vcoder, שהוא לוקח את הטקסט שיוצא, והופך אותו בסוף, יש לנו כאן כמה שמות, שאוספתי כאן, זה כבר קשור למשהו שנדבר אחרי הפסקה לפרויקט, מי שיזכר לעבוד עם כזה, משימה כזאת שכוללת אודיו, אין פה ציפייה שתעשו את זה מאפס, אז יש כל מיני מודלים שאפשר להתאים אותם לבעיה שלכם, ויש כמה אפשרויות כאן, כמו ה-Lama Omni, אפשרויות נוספות, Speech, לצורך ה-Speech של אלם, אוקיי, במערכת שאנחנו רואים כאן זה כולל את השלבים השונים, יש לנו את ה-Encoder של ה-Speech, Speech Decoder אנחנו רואים את ה-LLM, ואת ה-Vocoder כאן, אז מערכת שאם אני לא טועה אין טועה עם ה-Lama Omni, אוקיי, אז זה ככה לסיכום, מה עושים ב-Speech, וככה כמה מילים על מודלים של מולטימודל, אם אתם זוכרים גם על זה אנחנו דיברנו, דיברנו, לא הזכרנו בכך את המילה מולטימודל, אבל דיברנו על זה שבעצם יש לנו רצון לשלב בין כמה סוגים של דאטה, זה בעצם בעולם של Deep Learning וב-LLM, מה שעושים היום זה שיש לנו מולטימודליטי, יש לנו מודליטי של טקסט, מודליטי של אימג', מודליטי של ווייס, זה בעצם הוא יודע לטפל בהרבה סוגים, יש לנו פה איזושהי דוגמה פשוטה, שבטח אתם מכירים מודלים כמו דלי וכדומה, שאנחנו רושמים בתור אינפוט טקסטואלי מה אנחנו רוצים, ואז המודל בעצם מייצר לנו תמונות חדשות, שמורכבות על בסיס הקשרים שיש לנו בטקסט, כקשרים שיש לנו ויזואליים. אוקיי, אז מה הרעיון, איך עושים את זה? עושים את זה בצורה כזאת שאנחנו פונים מודלים שלומדים מייצוגים שונים של אותו אובייקט, גם ייצוגים של תמונה וגם ייצוגים נגיד של טקסט, כמו שאנחנו רואים כאן, Red Delicious Apple ויש לנו תמונה של תפוח, ויש לנו כאן עוד דוגמאות נוספות, וככה לומדים לשלב בין כמה סוגים של אינפוטים ובעצם מה שעושים זה מאמנים וקטור אחד, יש כל מיני טכניקות איך לעשות את זה, או להתחיל ממודלים קיימים ולעשות להם פיוז'ן, פיוז'ן זה ככה התחה של וקטור כזה ווקטור כזה, או שמתחילים מאפס עם אינפוטים שמכילים זוגות או יותר מזוגות, של מה נחשב דומה, מה נחשב שונה, ואז נרצה להפריד את השונה ולקרב את הדומה ולייצר ככה תצוגות שונות של אותו אינפוט, על ידי וקטור שמייצג כמה מודליטים.

כאן אנחנו רואים למשל איך טקסט אנקודר לומד את הקשר לאימג' אנקודר, כן ה-T זה לטקסט וה-I זה לאימג', ה-A, C ו-D הם מייצג לנו אפל, צ'ר ודוג. אוקיי, אז אנחנו ממש בונים איזשהו Q ו-K, ככה שיידעו בטרנספורמר להמיר מסוג אחד לסוג השני. אוקיי, אז זה מה שיוצרנו את המולטי מודליטי מודל, אז תחת הסיכום ביניים דיברנו וחזרנו על הטכנולוגיה של הטרנספורמר, בשביל לייצר קונטקצ'ואל לאוט לנגוג' מודל, ודיברנו גם על כל מיני סוגי דאטה אחרים, כולל שילוב של כמה סוגי דאטה, ועכשיו נציג ככה כמה מהמשימות, זה יהיה לנו ככה חלק מזה אנחנו נשתמש לבעות, של LLMים באופן כללי שאנחנו מכירים אם אתם זוכרים, דיברנו על אותו Fine Tuning, שלוקח לנו את ה-Pre-Trained Embedding, שלקח לפעמים חודשים לאמן, לייצר כל מיני משימות חדשות, לא המשימות המקוריות שיצרו אותן, כמו כמה מהם כתובים לנו כאן, זה דווקא בעיקר התמודדות בטקסט, אז יש לנו טקסט ונגיד Code Generation, או דברים יותר קלאסיים, כמו Summarization, דיברנו על Sentiment Analysis, על ההבנה של הסנטמנט של לקוח, שאומר לצ'אטבוט של תמיכה, מערכת של Question and Answering, בעצם בכל צ'אטבוט שאנחנו עובדים איתו היום, נקרא לו AI צ'אטבוט, זו הדרך הנכונה אולי לקרוא לו, הוא בעצם מערכת Question and Answering, מערכת של שאלות ותשובות, אם אתם זוכרים דיברנו על Question and Answering, עוד לפני ה-LLMים, אז בעצם הצ'אטבוט שאנחנו מכירים ועובדים איתו, הוא בשלב הראשון היה מערכת QA, אז מוטיבציה שלנו ל-IR עכשיו, ניקח את כל הדבר הזה ונעביר אותו ל-IR, למה בכלל רצינו מלכתחילה, את ה-Word Embeddings שלנו, רצינו אותם כמודלים כמו BM25 או TF-IDF, נתנו לנו בעצם התאמה מדויקת ועיקר, אם היה לנו מילים שונות, למרות שיש להם משמעות דומה, זה לא יכל לקפוץ לזה, אז בשביל זה הם צירו את הייצור הדחוס, שיודע למצוא Similarity, ואז בעצם אנחנו, במקום להשתמש בצורה הזאת לבקטורים שלנו, אנחנו משתמשים גם לא בשלב הראשון שהיה לנו, שזה היה ה-Word Embeddings הסטאטים, יש לנו גם דברים שמבינים ומתאימים לקונטקסט, זה אותם Contextual Embeddings, דיברנו הרבה עכשיו על הטכנולוגיה של טרנספורמרים שלהם.

אוקיי, אז כמה שאלות בשביל להפוך מערכת כזו של Contextual Embeddings, אז איך נשתמש בהם לעייר? אוקיי, אז הנה כמה שאלות ראשוניות, אחרי זה יהיו לנו עוד שאלות. בעצם מה שאולי היינו רוצים, זה משהו שאנחנו רואים היום, זה לא בדיוק מערכת יחזור שהיה לנו קודם, זה מערכת QA, אנחנו שואלים את ה-Chatbot שאלות, והוא עונה לנו תשובות. אם נרצה, אנחנו, לפחות זה היה ה-Use Case הראשוני שלנו, אנחנו לא רצינו, היום כן רוצים לדעת מקורות, אבל אנחנו לא רצינו לדעת מה הגיע מהעיתון הזה הדיגיטלי, ומה הגיע מהבלוג הזה, אלא אנחנו רוצים תשובה לאיזושהי שאלה.

אוקיי, כלומר יש למודל תפקיד של Fusion למקורות מידע השונים שלו, בשביל לייצר לנו תשובה לשאלה. אוקיי, אז איך עושים את הדבר הזה? יש לנו בעצם מסמכים שיכולים להיות מאוד גדולים, קיטי טקסט מאוד גדולים, ואנחנו דיברנו כרגע על World Embeddings, שיודעים, למרות שהם רגישים לקונטקסט, הם מייצגים מילה אחת. איך נייצג מסמך גדול, או אפילו אם אנחנו מדברים על זה בלוג של כמה עמודים, זה עדיין משהו מאוד מאוד גדול, בדרך כלל בשביל ה-World Embeddings שהצגנו.

גם היום כשאנחנו מדברים על טקסט-אמבדינג, שהם לא רק למילה אחת, עדיין לא מדובר על דברים ענקים כמו מסמך גדול, אז מה עושים? אוקיי, זה עוד אתגר שצריך להתמודד איתו. ואנחנו בסוף רוצים את ה-Vectors שלנו, מאיפה התחלנו, אחת המוטיבציות המרכזיות שלנו, זה גם כשאין לנו התאמה מדויקת, יש לנו נגיד מילים עם משמעות דומה, איך אנחנו יכולים להתייחס לדבר הזה, איך אפשר למנף את הקונטקט של ה-World Embeddings, איך בפועל עושים את זה? אוקיי, מה עושים בפועל? אוקיי, ומה קורה כשיש לנו מיליוני מסמכים, עוד לפני שדיברנו על הפתרון לשאלה השנייה, יש לנו מיליוני מסמכים, מה עושים? מה עושים עם הדבר הזה? אז אנחנו מדבר על זה, ואיך עושים רנקינג לתוצאות? אוקיי, אז הנה תשובות קצרות שאנחנו נרחיב לשאלות, אז איך בונים מערכת QA? אז בשביל זה יש לנו, אמרנו שהצ'טבוטים שאנחנו קוראים להם LLMים הרבה פעמים, הם בעצם, ה-AI צ'טבוט הזה מורכב מיותר מאשר LLM1, ואחד מה-LLMים המרכזיים שלנו, הוא מאומן על לענות תשובות לשאלות. אוקיי, איך הוא עושה את זה? נדבר על זה עוד.

אוקיי, השאלה הבאה שלנו היה, מה עושים עם אותם אינפוטים גדולים שצגנו מסמך? עכשיו אנחנו רוצים לעשות לו ייצוג של World Embedding, אז בשביל לפתור את הבעיה הזאת שהמסמכים, או אפילו איזה שקטע שעשו לו פוסט באיזה בלוג, או לא יודע, ברשת החברתית, הוא גדול מדי, אז בשביל זה יש תהליך חשוב בפרוססינג של האינפוטים שלנו, שנקרא צ'אנקינג. אז אנחנו נדבר על שיטות שונות לעשות צ'אנקינג, ואיך כל הדבר הזה משתלב בדאטאבייס, בשביל למצוא את הדוגמאות שבסוף מעניינים אותנו, או קורובים למה שעניין אותנו. אז כאן יש לנו, אם כולם בטח שמעתם על דאטאבייסס, אז המציאו דבר כזה שנקרא וקטור די בי, וקטור דאטאבייס.

דאטאבייס מיוחד, שכאן עכשיו כבר זה לא דאטאבייס של דאטאבייס, זה נעשה סלק, אני יודע מה, כוכבית, פרם תבלה מסוימת, where name שווה למשהו. כן, את כל הרשומות של שמות שמתחילים במשהו, או נגיד, סלק, כוכבית פרם סטודנט, where יוניברסיטי שווה חיפה נגיד. אוקיי, זה הדרך שבה אנחנו עושים ב-relational db, זה לא מתאים לווקטור די בי, מה קורה בווקטור די בי, איך אנחנו מטפלים את הדבר הזה, כשאנחנו שומרים וקטורים, זה איך אנחנו מוצאים את הדאטא שלנו.

אוקיי, יש לנו וקטורים של world embedding, נגיד 700 פיצ'רים שלא ברור בדיוק מה הכוונה שלהם, איך אנחנו מוצאים מידע דומה. אוקיי, גם אתגרים של ranking ועוד אתגרים נוספים שנדבר עליהם. אז נתחיל מאחת השאלות שיש לנו כאן, של מה עושים עם הדבר הזה, איך בעצם אנחנו עושים, מתאימים את הדבר הזה למערכת של information retrieval, אולי זה עוד לפני כל השאלות האלו.

אז כאן יש כמה גישות, הגישה הראשונה באה ואומרת, יש לנו encoder אחד, שאני לא בטוח כמה רואים את זה כאן, אבל במידה ולא רואים את זה, אז אני אכתוב את זה כאן עוד פעם, בעצם אנחנו מאמנים לתת ציון בסוף, אם אתם זוכרים את אותו תו מיוחד של CLS שהופיע לנו בסוף, אנחנו מעמדים מודל שיש לנו פה כל מיני שאיתות query, ואז יש לנו פה תו מיוחד separator או סוף משפט או משהו כזה, ואז יש לנו את הדוקימנט, אז יש לנו זוגות זוגות של שאילתר ותשובה שחלקם רלוונטיים, חלקם לא רלוונטיים, ואז המודל לומד איך להפיק מהדבר הזה, איך להפיק איזשהו ציון. זה בעצם שיטה ראשונה, מודל יחיד שכולל שאלה ותשובה, או שאלה ומסמך רלוונטי ליתר דיר, או מסמך לא רלוונטי, ואז מפיקים מהדבר הזה ציון ולומדים בצורה הזו, זו הטכנולוגיה, זו אפשרות אחת, כאילו כל האפשרות האלה משתמשים בה, אז קצת מדבר על ההתפתחות בעצם של אפשרויות השונות, אפשרות נוספת זה מה שנקרא by encoder, יש לנו אנקודר כפול, שלוקח בעצם מצד אחד את השאילתה, מצד אחד את המסמך, ולדבר המשותף הזה הוא מייצר לנו ציון, אוקיי, יש לנו ציון של האמבדינג שנוצר לשאילתה ולמסמך, אוקיי, רואים פה אולי ZQ ו-ZD, זה הייצוג הוקטורי שלהם, ואז מה המרפק ביניהם, אוקיי, זה ה-by encoder שלנו, כמובן שכדי לחסוך זמן, אנחנו עושים אנקודינג של כל המסמכים מלך התחילה, וזה חוסך לנו כמובן הרבה זמן, זה ככה זה נראה ה-by encoder, קצת יותר ברור אולי, עוד טכנולוגיה נוספת זה cross encoder, יש קצת התלתל בין ה-by encoder וcross encoder, by encoder זה כידוד וכידוד ואז דמיון, cross encoder זה אנקודר שממיר שאילתה למסמך בעצם, כן, אם אתם זוכרים, דיברנו על cross encoder גם בהקשר של תרגום, אז גם פה רוצים את הקשר בין שאילתה למסמך, אז את זה עושים cross encoder, ובסוף בסוף נדבר על מצב שהוא נגיד חדש יותר, איזושהי טכנולוגיה חדשה יותר, שבא להגיד, אומנם יש לנו את ה-input שלנו ל-contextual world embeddings, יש לנו את ה-processing של-contextual world embeddings, אבל יכולים להיות לנו עוד ייצוגים נוספים, כמו הייצוג שהיה עוד הרבה לפני, ההמצאה של ה-loud language models, שלצורך העניין אנחנו נקרא לו Sparse Vectors, אז Sparse Vectors זה למשל על ידי הייצוג ראשון, יש גם שיטות חדשות איך לייצר Sparse Vectors, אז למשל בעזרת TF-IDF או בעזרת BM25, דיברנו על זה, השיטות האלו, זה ייצג את ה-Sparse Vector, ואיזשהו מודל מיוחד שנקרא Colbert, יודע איך לאחד כמה סוגים של וקטורים ביחד, ולייצר מתוכם איזשהו דמיון משותף, יש כל מיני טכנולוגיות של איך לעשות את זה, בסופו של מקרה, אנחנו לוקחים את כל המסמכים שנוצרו מפה ומשם, אנחנו רואים שיש לנו כאן כמה מודלים, אז ה-Colbert הזה ידע למזג את הכל ביחד, ולייצר לנו את כל המסמכים שיצרו, יש לנו פה איזה סטטיסטיקה של, זה אחת השיטות שבעצם משתמשת ב-Colbert, ועושה פיוז'ן על כמה סוגים של מקורות מידע, Dense ו-Sparse, רואים דווקא שבניסוי הזה, ה-Sparse נתן תוצאות טובות יותר, Sparse זה לא ה-BM25, זה מודל מיוחד שהשתמשו בו ל-Sparse, איפה אגב Sparse, אני אתן דוגמה לאיפה Sparse Vector עדיף, הרבה פעמים על Dense, למשל כשאנחנו בונים מודל גנרי, שהוא לא מומחה באף תחום, ואז יש לנו Sparse Vectors, לקחנו כל מיני מסמכים ויצרנו מהם וקטורים, שהם נגיד בתחום מאוד ספציפי, למשל אני לא יודע מה אסטרונומיה, או חשבונאות, כל מיני תחומים שהמודל הכללי מאוד שלנו, הוא לא כל כך טוב בהם, אז דווקא שימוש במילים מאוד ספציפיות, כשהמילים שלנו הם לא בדיקשונרי של המודל הכללי, אז Sparse Vector יכול להיות יותר טוב מדנס Vector, ובכל זאת אם אנחנו רואים שאנחנו משולבים כל מיני מודלים כאלו, בעזרת קולברט, קולברט יעשה לנו גם רירנקינג, לוקחים פה את ה-BM25, את ה-Dense, שזה לצורך העניין מודל Contextual World Embedding רגיל, את ה-Sparse המודל המיוחד החדש, זה יוצר לנו הרבה פעמים תוצאות יותר טובות באופן משמעותי, כמו שרואים פה. עוד רכיב שיכול להיות שמעניין אותנו, זה לעשות Rewriting ל-Query, בעזרת מודל מיועד לצורך זה, אנחנו מתאימים בעצם את ה-Query המקורי ל-Query חדש, ומאמנים מודל שיידע לעשות את הדבר הזה, ואז הוא יתאים לנו לאותו מערכת נגיד של IR או של Chatbot, בשביל לקבל תוצאות יותר טובות, אז יש לנו כאן דוגמאות לדבר הזה.

אוקיי, אז דיברנו הרבה על אל-למים, אזכרנו את השם רג, למה אנחנו צריכים רג? איפה אנחנו משתמשים פה ברג? אוקיי, אז עוד רגע נבין מה זה רג, אבל נבין מה הבעיה באל-למים. אוקיי, אז הבעיה הזאת שיש לנו באל-למים, ראו אותן ב-AI Chatbots הראשונים. אוקיי, אני מניח שרובכם שמעתם פה חלק מה-Buzzwords, אבל בתור התחלה המודל שלנו, הוא מען על מידע שהוא עדכני עד תאריך מסוים.

אוקיי, אז בואו נניח שהוא יודע לענות בצורה מושלמת על השאלה, שזה גם סימן שאלה גדול, ראינו הרבה פעמים שכדאי אפילו לשלב שיטות ספרסיות כדי לתת תוצאה יותר טובה, אז בואו נגיד שהוא יודע לתת תשובה בצורה מושלמת, אז דוגמה קלאסית שנותנים לבעיה הזאת, זה בעצם ששואלים, מישהו אולי שמתעניין קצת באסטרונומי אולי מכיר את זה, מהו כוכב הלכת עם הכי הרבה ערכים, אז עד שלב מסוים היה ידוע שצדק זה הכוכב שיש לו 60 ומשהו ערכים אם אני לא טועה, והמידע הזה היה נכון עד 2021 לדעתי, משהו כזה, ואז גילו שלשבתא יש יותר ערכים מצדק, אז בעצם אם נשאר ל-LLM שהוא מענה עד ידע עם כמות מסוימת, אז היינו אומרים שהתשובה היא צדק, אבל המידע הוא לא עדכני, אז זה בעיה הראשונה שיש לנו, outdateded information ב-LLM, אז LLM הוא מצוין לדעת, להבין הקשרים, similarity וכל הדברים האלו, אבל המידע שלו חייב להיות מתוחזק כדי שיהיה אמין, זו בעיה הראשונה, ויש כבר פתרון שדיברנו עליו, אז מה הבעיה? בוא נעשה fine-tuning, fine-tuning יכול להיות בשביל משימה מסוימת, אבל הוא יכול גם לקחת את המידע המקורי שלנו ולהוסיף מידע חדש, לעשות את אותה משימה, אוקיי, אנחנו נדבר עוד על מנגינונים כאלו, ספציפית כבר בחלק השני של השיעור הזה, בעצם פתרון אפשרי, הבעיה בפתרון הזה זה שהוא יקר, אם אנחנו עושים re-training מאפס זה בטח מאוד מאוד יקר, אבל יודעים שיש אופציה של fine-tuning שהיא יותר טובה, ואופציה האחרונה זה שמודלים לומדים לתת תשובה, יש כל מיני יוריסטיקות שמנסים לעקוף את הבעיה הזאת, אבל בטח שמעתם על המילה הזיות או hallucinations שמודלים עושים, מודלים לא מאומנים בצורה טובה להגיד אני לא יודע, או אני לא יודעת, אלא נותנים איזושהי תשובה שהיא תשובה שהיא לא קשורה בכלל, כן יש איזה סיפור, יש הרבה סיפורים כאלה, אבל אחד מהסיפורים הידועים, כאילו שהיה איזה hype קצת סביב זה, על איזשהו בן אדם שכמו הרבה דברים אחרים, שומעים על אנשים בדרום קוריאה שלא עלינו נפטרו, בגלל שהם היו כל כך מכורים לגיימינג, שהם שיחקו איזה 48 שעות ברציפות, ולא ישנו ולא אכלו ולא שתו, ואז כן אולי שמעתם על זה, אז כאן מדובר על מישהו שהתמכר לאלה לימים האלו, זה נראה לי שזה היה Chad Gipity במקרה הזה, והוא כן אכל והכל לא מת בסוף, אבל הבן אדם הזה לא הלך לעבוד, והוא בן אדם שהיה סיפור רקע לכל זה, הוא לא למד בדרכים פורמליות, הוא היה לומד עצמאי, והוא חשב שבלי ללמוד הרבה פיזיקה, שהוא ידע להמציא איזשהו סיפור חדש, תיאוריה חדשה פיזיקלית, ופשוט חלק מהדבר הזה, אני מניח שכולכם נתקלתם בזה, בפידבקים החיוביים שה-AI Chat Bots האלה נותנים לנו, אז הוא כל הזמן עודד אותו, והוא הגיע למחקר ככה שתוך חודשיים, הוא רצה לפרסם ממש תיאוריה חדשה, והתברר שהכל זה שטויות, כי האלה אליהם גם, זהרם, הם נתנו פידבקים חיוביים, אבל גם הכל היה אוסף של אזעיות, אוקיי? אז אולי נאסוף כמה דוגמאות לאזעיות, אבל אני מניח שאתם מכירים את הדבר הזה די קטן. אז מה זה הרג? אז הרג בעצם זה משהו שמורכב משלושה חלקים, ה-R זה ל-Retrieval, אוקיי? סתם אנחנו יודעים מה זה אותו Retrieval, ה-A זה ל-Augmentation, שאנחנו נסביר על הרכיבים האלו, ו-Generation ניצור את התשובה בסך. אוקיי? אז הרג בשלבים הראשונים, אנחנו רוצים לקחת מערכת של יחזור מידע, וכבר דיברנו על חלק מהדברים האלה, כן? זה היה אפשר אולי חלק מהם להצמיד לאיזה אלה אליהם, עוד לפני שקראנו לזה רג, אז לא נגיד בדיוק איפה אחד מתחיל והשני נגמר, לצורך העניין, הרג עונה על הדברים האלה, ובעצם משתמש באלה אליהם כמנוע מרכזי בפנים.

אז בעצם הכנה של המידע ליחזור, כולל כמה חלקים, חלק כמובן, זה לא כתוב פה, אבל צריך לדעת לתמוך בכל מיני סוגים של אינפוטים, PDFים, HTMLים, כל מיני סוגים שונים של אינפוטים, ואחרי זה אנחנו רוצים גם לסוגים שונים של דאטה, וגם לסוגים שונים של, כאילו גם דאטה מסוגים שונים כמו PDF, טבלאות ו-HTMLים, וגם דומיינים שונים, מיוצא אופציות, שיטות, גישות שונות, לעשות מה שקרנו לו צ'אנקינג, החלוקה למקטעים הקטנים, ואחרי זה אנחנו נתמקד ב-VectorDB הזה שלנו, איך עושים דאטאבייס עם אותה ממבדינג שלנו. אוקיי? אז נתחיל ב-צ'אנקינג, יש לנו פה בעצם, נגיד, כמה נקודות שנרצה להתייחס אליהן, הראשונה זה כמה צ'אנקים בעצם ניצור מהדאטה שלנו, אז צריך לאזן את כל הגורמים האלה, לא לקחת מעט מדי או לוקחת הרבה מדי, את הרכיב שנוצר מתוך הצ'אנקים האלה, האיכות של ה-Vector. אם ניקח צ'אנק קטן מדי, אז לא יהיה לנו מספיק קונטקסט, אם ניקח צ'אנק גדול מדי, אז הקונטקסט לא יצליח לעמוד, מבחינת המנגנון של ה-Attention, בכל מיני חלקים שהם רחוקים אחד מהשני, כל מיני מילים, טוקנים שהם רחוקים אחד מהשני, ומה המבנה של האינדקסט בסוף, כל הדברים האלה ישפיעו על שיטת הצ'אנקינג, מעבר למה שאמרנו, סוג הדאטא והדומה.

אז חלק ממה שאמרנו כאן אנחנו שואלים, אז סוג ראשון הכי פשוט, נגיד שאם אולי אתם תממשו, אז זה הכי פשוט לעשות את זה, זה Fixed Size Chunking. מה זה אומר Fixed Size Chunking? אנחנו מחליטים על איזשהו גודל, מספר הטוקנים, ואז בגלל שזה נעשה בצורה כל כך גסה, אולי נעשה גם חפיפה מסוימת ביניהם, וזה יהיה הצ'אנקים שלנו מהאינפוט. סוג שני הוא סוג מבני, יש לנו כאן לדוגמה שני כלים של NLP, שאולי יעזרו לנו לעשות את זה בשבילנו, זה בצורה נאיבית מאוד, נגיד חנוכה למשפטים על ידי נקודה, או קצת יותר טוב להשתמש בכלי NLP, שיחלקו את זה למשפטים ולפסקאות.

אז כאן השתמשנו במבניות, לפעמים נגיד PDFים ו-HTMLים, יש לנו כל מיני, ב-HTML נגיד רמזים של מתי מתחילה פסקה חדשה, מעבר שורה וכדומה, יש לנו, נתמקד, שיטות הריקרוסיביות, אולי ההיררכיות כאן יש לנו, חלוקה נגיד לפסקאות, רואים שהפסקה הזאת גדולה מדי, אז נחלק אותה עוד. וזה יכול להיות עדיין בצורה מבנית. יש שיטות שממש מכניסות פנימה את המבנה של הקובץ, אם זה נגיד HTML או PDF, ובונות לכל אחד מהם את הצ'אנקים שלו, וכמובן אפשר לשלב את השיטה הזאת עם כל שאר השיטות האחרות, הנה דוגמה ויזואלית.

אוקיי, כאן זה מתחיל להיות קצת יותר קשה, Semantic Chunking, איך נדע איפה כדאי לנו לחתוך? אז לצורך העניין, גם משימות שהמציאו אותן עוד הרבה לפני ה-LLMים, אבל אפשר גם להשתמש ב-LLM יהודי לצורך זה, שאנחנו רוצים לגלות מתי יש לנו שיפט במשמעות של הטקסט. מתי הטקסט תתחיל לדבר על משהו אחד ופתאום עובר לדבר על משהו אחר, אז כאן ממש מתייחסים למשמעות הסמנטית שלו. אוקיי, וכמובן שכמו לכל דבר, יש גם ל-LLMים שאומנו לעשות Chunking, זה דרך נגיד יקרה ומורכבת, שאנחנו רוצים את זה ברמה דיוק מאוד מאוד גבוהה, זה קצת יותר יקר.

אוקיי, אז זה ה-Contextual Chunking עם ל-LLM. אוקיי, אז הדבר הזה היה למשימה קטנה, מתוכה אפשר לקחת אולי כמה גישות שהן יחסית פשוטות, אז נזכיר בעיקר אולי את ה-Fig Size, את החלוקה למשפטים או מילים, או סליחה, או פסקאות, אולי בעיקר בעזרת כלי NLP, את השינוי הסמנטי, ואת ה-LLM היהודי לצורך זה, ואת כל אלו עם גישות Chunking שבעצם מתאימות לכל אחד מסוגי המסמכים, גישת Chunking משלו, נגיד במקור אחד יש לנו טבלה, אנחנו לא רוצים לה מתייחס כ-Chunking שעשה חלוקה לפסקאות, זה פשוט לא יעבור בצורה טובה. הרבה פעמים הטבלאות נגיד יקבלו שיטת Chunking אחרת מאשר סתם פסקה של טקסט.

אוקיי, אז... אוקיי, דיברנו על הדאטאבייס שאנחנו רוצים לטפל בו, אז מה זה אותו דאטאבייס לבקטורים? בדאטאבייס לבקטורים, המטרה היא ברורה, אנחנו רוצים לשמור את הוקטורים של ה-Embeddings, של ה-Contextual Embeddings של טקסט נגיד, זה לא בהכרח ברמת המילה, לפעמים זה כן יהיה רצף של מילים, אבל לפעמים זה יהיה לכל ה-Chunk Embedding, אוקיי? ואז אנחנו רוצים בצורה יעילה, זה מה שאנחנו נראה עכשיו או אחרי הפסקה, אולי נתחיל עכשיו ואז נמשיך את זה אחרי הפסקה, אז זה איך אנחנו משתמשים בדבר הזה, להחצן ולחפש Embeddings, איך עושים את הדבר הזה, איך המערכת של הרג, אוקיי, משתמשת ב-VectorDB כחלק מהתהליך של ה-Embeddings. אז כאן נגיד גם לאלו שהם לא כל כך עם רקע של מידת מכונה, נזכיר עוד רגע מה זה הדבר הזה, אז Flat Indexing זה דרך אחת, אוקיי? בעצם עוד רגע נסביר מה זה אומר, והדרך המרכזית שאנחנו נשתמש בה זה Approximating, דרך לעשות את זה בצורה משוערכת, לא מדויקת אבל שיכולה לטפל במיליונים או מיליארדים של חתיכות מידע. אוקיי, Flat Indexing בעצם זה שימוש של Algorithm כמו KNN, אז אני אזכיר מה זה KNN למי שלא זוכר, KNN עוד הרבה לפני שמי שלא זוכר הוא Slash, לא כל כך עם רקע של מידת מכונה.

אז יש לנו כל מיני תכונות שיכולות לבטא כל אובייקט שיש בעולם, שאנחנו בסוף הופכים אותם ל-Vector מספרי, או לחילופים אצלנו זה ה-Vector של ה-Root Embedding, אוקיי? ועכשיו אנחנו רוצים לדעת לכל מיני משימות, נגיד למשימה איזה קטגוריה לשייך לאותו Feature Vector, לאותו Vector שמייצג את האובייקט, נגיד Root Embedding, איזה קטגוריה לשייך לו, אז אולי יש לנו אוסף של הגיות של קטגוריות, ידועות שנגיד איזה בן אדם הכין מראש, והוא הצמיד אותם לווקטורים, רואים לתהליך הזה Training, הוא מלמד איזשהו מודל, ששייך בין הוקטורים האלו לבין ה-Target Values, אז ב-KNN לא בדיוק עושים Training, מה עושים? אנחנו מפסים פונקציות שדיברנו על Cosine Similarity ועל משק אוקלידי כשתי מטריקות מרכזיות, יש גם מטריקה נגיד של פשוט כפל וקטורי Dot Product, אוקיי אז נגיד זה שלושת השיטות המרכזיות ביחד, אז אנחנו פשוט רוצים לעבור על כל הוקטורים ששמרנו בוקטור DB, ואנחנו רוצים בצורה מאוד נאיבית, כמו שבטח בקורסי אלגוריתמים למדתם, מה הפתרון הנאיבי? נפרוס את כל האופציות, היה לכם בטח משהו כזה, נפרוס את כל האופציות ונראה מה הכי טוב, אז זה מה שאנחנו עושים כאן, רוצים את התשובות הטובות ביותר, אנחנו לוקחים את ה-Word Embedding שלנו שמייצג את השאילתה, יש לנו Word Embedding שמייצג כל אחד מהצ'אנקינג בתהליך שעשינו של המסמכים, ובוא נמצא עכשיו את ה-K הקרובים ביותר, אחרי שעברנו על כל המיליונים של הדוקימנט ששמרנו בדטאבס, עכשיו השיטה הזאת, מי שאמר לכם אי פעם ש-K אין אלגוריתם לא טוב, אז זה לא נכון, כן אין אלגוריתם מצוין, כמובן שהדאונסייד שלו, החיסרון שלו ברור מאליו שהוא מאוד איטי, עכשיו אנחנו רוצים לראות מה שווה לאובייקטים שחיפשנו, זה ייקח לנו המון זמן, אז בפועל זה לא כל כך ריאלי השיטה הזאת, בוא נגיד שאם אתם בונים איזה מערכת שאמורה לעשות איזה דמו, אז אפשר להתחיל בדבר הזה, אם יש פה נגיד מאות או אלפים בודדים של דוגמאות, אז נגיד איך שהוא זה סביר, מעבר לזה זה כבר יהיה מאוד יקר, אז בשביל זה במערכות של רג, המציאו מה שנקרא ANN, נגיד שהקרדיט מגיע, אני חושב לספוטיפיי שהמציאו את האלגוריתם הראשון, שאני לא יודע אם אנחנו נראה אותו, שנקרא בראשי תיבות, הנוי זה ראשי תיבות של Approximated Neural Neighbors, זה איזה אלגוריתמאי מתלהב אחד שקרא לזה ככה, אז דווקא מאז יש שיטות יותר חדשות שאנחנו נראה עכשיו, אז השיטה הראשונה היא מאוד פשוטה, אני אזכיר שלמדנו אלגוריתם לעשות קלאסטרינג, מה זה קלאסטרינג? זה שוב זה אלגוריתם שקשור ללמידת מכונה, שאני אזכיר אותו שוב במיוחד בשביל אלה שהם פחות עם רקע של למידת מכונה, אז יש לנו ייצוג וקטורי לכל האובייקטים שלנו, המשימה הזאת היא לא משימה רק של נגיד, קשורה בהכרח לזה, אפשר להסתכל עליה כמשימה מתמטית של חלוקה של אובייקטים לקבוצות שאנחנו קוראים להם קלאסטרים, אז כאן קבוצות האלו מוצגות כנקודות במרחב, כאילו יש להן שני מימדים, דלקטורים אמיתיים כמובן יהיו יותר משני מימדים, אז אנחנו אומרים דבר כזה, בואו נחלק אותם לקבוצות, ככה שבכל קלאסטר יהיה לנו אובייקטים קרובים יהיו ביחד, והקרבה מייצגת דמיון, לגבי זה גם היינו דמיון קוסינוס שמודד את הדמיון בין הוקטורים, או מרחק קטן שגם בעצם מייצג דמיון, כמו מרחק אוקלידי קטן, אז אלו שיהיו דומים יהיו באותו קלאסטר, ואלה שיהיו שונים אחד מהשני יהיו בקלאסטרים שונים. אוקיי, אז השיטה אולי הפשוטה ביותר, אבל כמו KNN היא מפתיעה כי היא עדיין בשימוש, נקראת K-Means, ובמיוחד למי שעם פחות רקע אני אזכיר אותה, זו השיטה של K-Means, יש לנו את זה גם בשקפים האחרים כשדיברנו, דיברנו על זה כמה פעמים גם ב-User Feedback ובכל מיני הגשרים אחרים, אוקיי, אז מה שאנחנו עושים זה דבר כזה, נגיד את זה בצורה מאוד נאיבית אבל שמספיקה בשביל מה שצריך לדעת, דבר ראשון, אנחנו מגדירים כמה קבוצות כאלה, כמה קלאסטרים אנחנו רוצים, אוקיי, נגיד שבמקרה הזה אולי נגיד שהיינו מגדירים ארבע קלאסטרים, אוקיי, אז יש לנו הגרלה, כן, של ארבע וקטורים, במקרה הזה, כי K שווה ארבע, אז בתור תהליך של התחול, אנחנו פשוט מגרילים ארבע וקטורים, שכל אחד מהם בעצם הוא הנציג של כל אחד מהקלאסטרים, אוקיי, איך הוא יהיה הנציג שלו בסוף? אז כאן יש איזו הנחה תיאורטית של Algorithm K-Means שאנחנו מראים עכשיו, הנחה תיאורטית שבעצם יש לנו מרכז של המידע, וכל שאר האובייקטים בעצם הם בסוג של התפלגות סביב אותו מרכז, אז המרכזים האלה קראנו או פרוטוטייפס, כן, הארבע וקטורים האלה, זה או סנטרוידס, המרכזים, או פרוטוטייפס, זה כאילו האב טיפוס במפעל שיצרו, אבל המטרה פה של האב טיפוס הוא לא בשביל לייצר את כל האובייקטים בדיוק כמו, אלא לייצר אובייקטים דומים אליו, אז כאילו הרעש הזה הוא לא טעות באמת, אלא זה אמור להיות ככה, אז בעצם האלגוריתם שלנו עד להתכנסות עושה איזושהי לעולה כזאת, שלב ראשון, נקרא לזה שלב שתיים ביחס להתחול, אנחנו בעצם עושים שיוך לסנטרויד הקרוב, איך עושים שיוך? אנחנו בוחרים בתור, האלגוריתם מקבל כאינפוט, נגיד ראינו למשל דמיון וקטורי, בוחרים את הוקטורים הכי דומים לכל אחד מהוקטורים מתוך פרוטוטייפס, אז זה תהליך שהוא פחות יקר, לא משווים כל אובייקט לכל אובייקט, כל וקטור לכל וקטור, משווים רק לפרוטוטייפס, ומשיישים לקלסטר שמיוצג על ידי כל אחד מהפרוטוטייפס, בעצם ככה אנחנו מנישים כאילו שאנחנו יודעים מה הם הקלסטרס, רק זה לעשות שיוך, שיוך של הוקטורים לקלסטרס. בשלב השני אנחנו מנישים שאנחנו יודעים מה הקלסטרס, אבל לא יודעים מה הפרוטוטייפס, וכאילו אנחנו מקבעים את השיוחים ורק עושים ממוצא בשביל לעשות recalculate לקלסטרס, איך עושים את זה? פשוט ממוצע וקטורי של כל האובייקטים ששוויחו בשלב 2 בכל אחד מהמימדים עושים ממוצע וזה מה שקוראים ממוצע וקטורי אז recalculate נראה לי כתבתי פה הרבה דברים כמה פעמים אני מקווה שזה לא טעום כבר center weights כל פעם אנחנו מקרבים קצת את זה וקצת את זה ומגיעים ככה לאיזה מינימום מקומי איזה בעיה אינפיקשה אז אי אפשר להגיע פה לאופטימיזציה גלובלית אבל אופטימיזציה מקומית מצליחים להגיע וככה אנחנו יוצרים את השיוחים שלה אז למה אנחנו מדברים על הדבר הזה? מה הקשר שלו לענייננו? אז ככה לקחנו את הצ'אנקים יצרנו מהם contextual world embeddings בעזרת איזשהו LLM שמבוסס Transformers ומפה ועלה, אגב המטרה של אותם LLMs היא לא לייצר תשובה, היא רק לייצוג אמרנו שיש במערכת של הAI Chatbot לפחות שני LLMs אחד לייצוג, זה המודל לייצוג מודל הרבה יותר פשוט ומודל הרבה יותר מורכב לשאלות ותשובות זה עוד LLM נוסף זה לא מדובר באותו LLM אז אותו LLM שדיברנו שיכול לשלב את ה-BM25 או את ה-TFIDF יחד עם ה-contextual world embeddings כדוגמה למערכת יחסית חדשה, ב-2025 המציאו אותה, השילוב הזה אז אותו אחד נותנים למערכת הזאת איזשהו input, היא עושה clustering ועכשיו הדבר הזה עוזר לנו כי זה לא שאנחנו נשווה רק לפרוטוטייפס אבל לפרוטוטייפס ידעו למקד לנו את החיפוש של וקטורים הקרובים אז איך זה מתבצע? זה מתבצע בצורה הזאת, מריצים clustering עכשיו יש לנו, אנחנו רואים נגיד כמו שאמרנו, ארבעה clusters עכשיו יש לנו query vector, אז אנחנו נחפש לצורך העניין את ה-cluster הרלוונטי, את התוצאה זה שלב ראשון, זה לאיזה cluster הוא שייך זה השלב שעכשיו יש לנו שאיל תא כבר אינדקסנו את כל ה-chunkים שלנו מסמכים חירה כמו chunkים, אינדקסנו את כל ה-chunkים שאמרנו אותם על ה-vector db, עכשיו לוקחים את ה-query שלנו הופכים אותה ל-contextual vector ל-contextual embedding על ידי ה-LLM היהודי ועכשיו מחפשים אותו בדטאבייס, איך עושים את זה? בודקים לאיזה פרוטוטייפ הוא הכי קרוב אז במקום לחפש מיליונים של פרוטוטייפ לצורך העניין נגיד שחיפשנו, לא יודע, שורש נגיד של מיליון למשל יכול להיות, או פחות מזה בדוגמה הזאת יש לנו ארבע שאנחנו משווים זה כמובן הרבה יותר מהיר שלב שני, אנחנו מוכרים את ה-k הקרובים ביותר בתוך ה-cluster הדוגמה הזאת כבר מראה איזושהי בעיה שיש לנו כאן ואז מחזירים את הטובים ביותר זה מתאים למקרה שבו התשובה בעין נראית קרובה מאליה נגיד הוא נמצא קרוב למרכז ניקח את הקרובים ביותר דווקא בדוגמה שיש לנו כאן זה כנראה לא כל כך מתאים אנחנו רואים נגיד ש-d2 אולי בכלל הוא קרוב יותר מאשר d1 ה-vector הקרוב יותר בתוך ה-cluster וה-cluster השני הוא פשוט בגלל שבפנות זה רק הידיעה צנטרויד אז לא בדקנו בכלל את ה-cluster השני זה אחד מהפרמטרים המרכזים של IVF אותו אלגוריתם של משתמש בקלסטרים אז כאן יש לנו איזה פרמטר שנקרא n-probe שאומר לנו כמה קלסטרים אנחנו נשקול תיקח את הקלסטר הברוטוטייב הכי קרוב ואתה מוכנים את זה לפי הקרובה לפרוטוטייב אז באמת במקרה שניקח n-probe שווה 1 אולי זה מצומצם מדי וזה לא יפתור לנו בעיה כמו הבעיה הזאת נפספס פה דוגמאות שהן באמת קרובות אז בשביל זה ניקח n-probe גדול יותר 8 זה ממש גדול אבל זה בא להמחיש כאן לקחנו את ה-8 הקרובים ביותר ונבחר את כל הדוגמאות שלהם זה עדיין תת קבוצה של כל הפרסטרים האפשריים זה יפתור לנו ואמצאנו נגיד גם את ה-d2 אז ככה אנחנו פתרנו את זה זאת הייתה השיטה הראשונה נראה לי שאת ההמשך של ה... ניקח עכשיו נראה לי איזה 25 דקות חצי שהפסקה ואז את ההמשך נעשה אחרי שאלות עד כאן? אני כאילו ברצף שצף כבר נתתי לכם שנייה להשחיל שאלה בסדר, אנחנו נחזור ואם דברים לא יהיו ברורים אז שאלו אחרי זה אין בעיה זה פשוט ירדסי אז עוברתי להקליט ואני מקווה ששומעים לפני שנחזור לחומר אני מזכיר שאנחנו מדברים על וקטור db לפני שנחזור לזה אז כמה מילים נתחיל ביותר קל כאילו אני אחרסם על זה עוד אני מקווה היום בערב ומחר על המטלה האחרונה קודם כל תשימו לב אני לא יודע היו כל מיני שאלות עוד לפני המטלה האחרונה על הצגה של סיכום אני לא יודע אולי אני לא אולי אני יטעתי אבל אין מה להציג את הסיכום פשוט מעלים סיכום צריך להעלות שש מטלות שוטפות מתוך השמונה ואחת מהן כשעברתם שידור בכיתה או כששלחתם סרטון יש עד פתחתי גם את המטלות האחרונות נדמה לי שש, שבע ושמונה למי שחסרים לו מטלות אפשר להגיש עד יום חמישי נתתי עוד כמה ימים לגבי המטלה האחרונה יהיו כמה פורמטים ואני אעלה יותר פרטים בהמשך קודם כל אי אפשר לעשות את זה בקבוצות של כנראה עד ארבעה נפרסם בדיוק פרטים וזה יכלול כמה אפשרויות שבעיקרון בונות מערכת אינטורנט זה יכול להיות או לעשות איזושהי אפליקציה שמשלבת אינפורמציה של יותר מסוג אחד של מידע שמגיד גם טקסט וגם OCR או גם טקסט וגם אודיו או משהו כזה וכמובן אתם לא אמורים לממש משהו מאפס אלא לעשות אינטגרציה של דברים קיימים זה אפשרות ראשונה אפשרות שנייה זה משהו שהוא יהיה יותר להגיד מחקר אולי זה קצת מוגזם אבל המטרה פה יהיה נגיד עכשיו אנחנו לומדים שיטות שונות של אינדקסים או צ'אנקים או בכל מקום שהיו כמה אופציות אנחנו רוצים בעצם לזכור לבעיה הספציפית שאתם תבחרו מה נותן תוצאות יותר טובות ולשבות את התוצאות אז זה אופציה שנייה נגיד קצת יותר מוטט מחקרית אופציה שלישית אנחנו עוד לא למדנו את זה אבל זה יהיה בסוף השיעור יש מה שנקרא אייג'נטק ראג, אייג'נטק ראג זה בעצם אפשרות של שלכם לבוא ולקחת כלים נוספים מעבר לחיפוש למשל אני אתן דוגמה רוצים לבנות איזה אפליקציה שלמשל משלבת אפליקציה של קלנדר ואז אם אנחנו עכשיו רוצים לקחת את המידע אנחנו רוצים בסוף לקבוע מהדבר הזה פגישה ביומן אז הדבר הזה הוא אפליקציה שמשתמשת בכל מיני רכיבים שאחד מהם הוא רכיב של אולי הרכיב המרכזי זה הראג אבל לא ראק ועוד וריאנטים של הדבר הזה זה לדעת נגיד מתי להשתמש בראג ומתי לא זה אולי משהו יותר מומחה כזה של נגיד אם אנחנו שואלים מה מזג האוויר אז אנחנו רוצים להעלות איזה שימוש באיזה API שוב זה אייג'נטק ראג רוצים להשתמש בAPI ולא לתשאל את הוקטור DB שלנו אז יש כל מיני API פיתוחים אני כמובן אפרסם פרטים לכל הדברים האלה אז למשל אם מזג האוויר יש API פיתוח אז אני רוצה לשאול איזה שאלה שיש לה רכיב שהוא לא צריך לתשאל את הראג אז אני פונה לזה API מתאים כמובן דברים לא מורכבים מדי בכל זאת אין פה ציפייה שתבנו איזה חברה ואפליקציה אמיתית ואולי עוד אופציה רביעית האחרונה שאני חושב עליה עכשיו אם יש לכם הצעות נוספות אז אתם מוזמנים להעלות זה לא מומחה הדבר הזה כן זה כאילו בתוך האופציות האלה תבחרו מה שאתם רוצים אבל אז אופציה האחרונה זה לנסות לשים דגש יותר של אפליקציה לבנות סביב זה UI ולבנות שמשתמש ברג שכאילו אתם לא עושים את זה טקסטואלית שמכניסים נגיד אינפוט בקומנט או בטקסט או בקובץ טקסט או משהו כזה אלא עושים UI סביב זה שאתם יכולים ממש לבנות שטלות אוקיי אז כאן פשוט צריך שיהיה מערכת לא מורכבת מדי אבל שהיא end to end אז זה יותר דגש אפליקטיבי אז זה לדעתי שוב אני אשמח לקבל עוד הצעות אבל זה לדעתי ארבע הצעות שפשוט שמים דגשים שונים על דברים שאנחנו לומדים בקורס אם זה הרבוב של סוגים שונים של מידע במולטימדליטי אם זה שילוב של כלים שקשורים לIR וכלים נוספים כן ולא אם אתם מכירים לא כל פעם שאנחנו רואים מסמר אז יש לנו פטיש אז כל בעיה נראית לנו כמו מסמר אז לא כל דבר הוא רג אוקיי לא כל דבר הוא צריך להשתמש ברג אז כמו שאמרנו כדוגמה עם אייג'נטיק איי סוגים שונים של דאטה מערכת שהיא יותר אפליקטיבית end to end ומערכת שהיא יותר מחקרית שבוחנת כל מיני אפשרויות ונותנת את המסקנות כאילו שאנחנו עושים מחקר בתחום הזה אז ככה להגיד סיכום של מה נתן בסוף את הצעות הטובות ביותר איזה קומבינציה ככה נגיד של שיטת אינדקסים בוקטור די בי ואיזה שיטה של צ'אנקים אז זה נגיד משהו שאפשר לחקור דברים כאלה אז זה פחות או יותר לגבי המטע לגבי המבחן אני שנייה לפני זה לגבי המטע אני מפרסם את כל מה שאמרתי בצורה מסודרת היום או מחר לגבי המבחן אז אנחנו בשאיפה היום לסיים את החומר אולי יש למונד כאן שיעור הבא אני אשתדל שלא ובעצם המטרה היא ששיעור הבא אנחנו עושים כבר הכנה למבחן ולא לומדים חומר חדש בלבד אולי איזה שלמונד קטנה אם תהיה אני מקווה שלא ובעצם יש לנו עוד שתי שיעורים עכשיו אני רוצה לגלגל שבכל מקרה שיעור האחרון תהיה בזום אני מקווה שזה בסדר עם כולם כי יום לפני המבחן להגיע לפה נראה לי זה לא ריאלי אם יהיה אפשר להקדים את זה אז אני אעשה את זה זה מראש שאני אומר למי שפה אמרתי זה גם קודם לכמה סטודנטים שיהיו פה בהפסקה שזה לא כנראה יתאים לכולם כל אחד יש את המערכות שלו יש פה כמה מערכות גם מערכת של data science אני חושב יש פה כמה מערכות ובכל מקרה גם אם אתם אותה מערכת אולי יש לכם קורסים שונים אז גם אם רוצים להגיע אולי לא כולם יוכלו להגיע אז אני צריך לראות איך מטעמים את הדבר הזה ואם אני אספיק אני אקדים גם את השיעורות כזאת בזו אז זה ככה מבחינת הכנה למבחן אני שוקל כרגע עם המבחן ובכל מקרה אם זה לא יהיה עם חומר פתוח זה יהיה דף נוסחאות אני שוקל בין שתי האופציות האלה במטרה לא לעשות מבחן קשה אם הוא יהיה עם חומר פתוח אז זה אם זה יתאפשר אני אעשה עם חומר פתוח וככה הנטייה כרגע היא לעשות מבחן אמריקאי ואני מקווה שאני אוכל בימים הקרובים כבר לשלוח שאלות לדוגמה ובכל מקרה שבוע הבא זה יהיה מטרה לעבור על חומרים אני גם אכתוב בהודעה שאם יש נושאים מיוחדים שאתם מרגישים שלא יושבים טוב אז תעלו אותם ואני גם השבוע רוצה לשלוח לכם את הנושאים שצריך ללמוד במבחן היו הרבה נושאים שככה אנחנו תפתחנו אותם בכמה מועדים שונים ואולי לא הולכניס הכל במאה אחוז נושאים שראינו שבסוף זה היה יותר מדי אז אולי אמרנו כבר שהם לא ייכנסו אז אני אעשה לכם גם רשימה מסודרת של זה ומה עוד אתם רוצים לשאול על המבחן שדלתי להתייחס לשאלות ששאלו אותי כבר יש לכם עוד שאלות למישהו אז אני השאיפה היא שלפני שיעור הבא יהיה לכם יותר אינפורמציה בשיעור הבא תהיה שיעור חזרה ובשאיפה גם שיעור הכריז זה מה שאני יכול להגיד לכם על המבחן אז אם אין עוד שאלות אז אני אחזור לחומר מבחינת הדחיפות אני מניח שהמטלה האחרונה היא טיפה יותר דחופה אז אני אנסה לפרסום לה מדי יום אחר ולמבחן בימים הקורוזים בסדר אז context switch חזרנו ודיברנו על וקטור db דיברנו על שיטה שבעצם ממנפת קלאסטרינג מנגנון אפילו אולי לפעמים חלק מהמימושים ישתמשו באלגוריתם כן לא רואים זה נעלם ננסום לרדת הקבלים רואים? של אלגוריתם מאוד פשוט שבמקום כמו בכן להשוות לכל הוקטורים ששמורים בדטאביס שכמובן וקטורים דומים אומרים שיש דמיון בדרך כלל של התוכן בין מה ששאלנו נגיד בשאילתה לבין מה שנמצא בדטאביס אז השיטה הזאת בשביל לא להוריד את הזמן ריצה אנחנו מחפשים רק בוקטורים שנמצאים באותו קלאסטר אוקיי אז דיברנו על זה שיש לנו אפשרות לא לחפש רק בקלאסטר עצמו אלא גם בקלאסטרים קרובים אחרים ויש לנו בשביל זה את הפרמטר של nprop שקובע לנו כמה קלאסטרים הכי קרובים אנחנו רוצים לבדוק בתוכן אופציה שנהיה גם פשוטה יחסית זה שיטה של השינג שבעצם אנחנו אומרים נקראת lsh אז השינג רגיל אנחנו נמניח שכולכם מכירים אוקיי איך משתמשים בהשינג ובמקרה של lsh locally sensitive hashing אנחנו רוצים לעשות בדיוק את ההפך עם מה שאנחנו עושים בשיטות השינג רגיליות שבעצם מה שאנחנו רוצים לעשות כאן זה במקום להוריד את ההתנגשויות בין אובייקטים מה שאנחנו רוצים לעשות בהשינג יש לנו כל מיני אלגוריתמים שמנהלים מה קורה אם יש התנגשויות אם אתם זוכרים בטח למדתם במבנה נתונים או באלגוריתמים מיון פתוח מיון סגור כל מיני אלגוריתמים התנגשויות אז כאן אנחנו רוצים למקסם דווקא את ההתנגשויות אבל לעשות את זה בצורה ממוקדת כך שההתנגשויות יקרו כאשר יש לנו דמיון ואז אנחנו נמפה בעצם לאותם בקטים וקטורים דומים אז עושים את זה בכמה שלבים זה כולל שלב ראשון של שינגלינג, מין-האשינג ו-LSH אז השינגלינג פה מתייחס למידע ספרסי שהוא בלי דנס וקטור ואם המידע שלנו כבר עבר דנס וקטור אז פשוט נתחיל מנקודה אחרת בלי השינגלינג אז מה השינגלינג אומרים? אנחנו רוצים לקחת רצף של K שינגלינג K בעצם אם נרצה N גרם של אותיות אוקיי כאן נגיד לוקחים K שווה 2 והבוקאבלרי מורכב מאותם רצפים של שתי אותיות וזה האינפוט שלנו את האינפוט הזה אנחנו מעבירים לשלב נוסף של מין-האשינג במין-האשינג בעצם עבור כל, לצורך העניין, אנטרי בבוקאבלרי שלנו יהיה לנו one rote encoder שבסוף יהפוך את זה לאיזשהו וקטור שכולל נגיד בשיטה הנאיבית bag of words או שאם אנחנו מדברים על דנס וקטור כמו contextual word embedding אז בעצם לכל מימד יהיה לנו איזה מין-השמי שלו אחרי זה מה שאנחנו עושים זה בעצם להיחד את כל האוטפוטים לאיזשהו signature ככה, זה יהיה הוקטור שלנו שיגיד לנו שיש לנו אוטפוטים כאלה לכל אחד מהמימדים אז בעצם הייחוד של כל המימדים יביא לנו את הוקטור וזה מה שייתן לנו את הוקטורים הדומים בדטאבס אז זה הרעיון של השיטה אנחנו עושים את שלושת השלבים האלו קודם כל להנדק את הכל להאשים שלהם ואז כשמגיע איזשהו וקטור חדש אז אנחנו משתמשים ב-lsh שבסוף נותן לנו איזשהו דמיון לפי כמות הבאקטים ב-hashes הרבים ביותר שבהם היה לנו התאמה אם אנחנו רואים כאן למשל בויזואליזציה הזאת אז יש לנו פה שלוש באקטים זו דוגמה פשוטה והייתה פה התאמה אחת בבאקט השלישי שלהם כל באקט יש לנו את המיניה שלו פשוט אז ככל שיש לנו יותר באקטים יש לנו יותר דמיון וניקח מתופעם את ה-k עם כמות התאמות הגבוהה ביותר ואולי שיטה שהיום אני חושב הנפוצה ביותר נקראת hnsw, אחרי כל navigate small world לא רציתי להציף פה את השקפים אבל השיטה הזאת בעצם גם קיבלה השראה מהסרט של מחפשים את נמון מי שראה את הסרט המצויר הזה אז הרעיון הוא שיש לנו המון המון אובייקטים ואנחנו רוצים לדעת במהירות להגיע לאובייקטים רלוונטיים אז מה עושים? אז אלגוריתם שהביא השראה לשיטה הזאת זה אלגוריתם של skip lists רצית לכם ללמוד את זה בקורס אלגוריתמים או מבנים נתונים? זה נראה ככה יש לנו איזשהו link list מעל link list, מעל link list ככל שהצורה שבה אנחנו בונים את זה כדי שזה יהיה חיפוש בלוג של n בכל מקרה זה שאנחנו בעצם כל אחד מהאיברים שלנו אנחנו מגרילים האם הוא יהיה גם ברמה מעל או לא מגרילים נגיד בהסתברות יוניפורמית של חצי חצי ואז לא ניכנס הרבה לפרטים אבל לצורך העניין אנחנו מתחילים מהשכבה הכי עליונה בחיפוש ואז בשכבה הכי עליונה יש לנו מספר מאוד קטן נגיד של שני איברים נמשיך שם באיבר שהכי קרוב לאיבר שאיתו התחלנו את החיפוש נגיד שבמקרה שלנו האיבר שאנחנו עושים איתו בחיפוש זה הquery זה הvector של הquery ואז בכל פעם שאנחנו מוצאים התאמה הקרובה ביותר אנחנו יורדים רמה אחת למטה וממשיכים את ההתאמה שמה עד שבסוף נגיד המשכנו מכאן ומצאנו שכאן למשל ב-11 הזה הייתה את ההתאמה הגבוהה ביותר כאן זה היה התאמה מדויקת אז מה הקשר בין זה לבין החיפוש שלנו מבחינה וקטורית שוב זה מחולק לרמות שונות אז כאן מתחילים את החיפוש של השאלטה ויורדים ברמה הזאת נגיד שכאן הייתה לנו התאמה הגבוהה ביותר והיה עד מאוד רמה ובסוף אגענו לזה שזה זה האובייקט של מאוד קרובה בואו נראה איך זה נראיה מבחינת שכבות שדומה לשכבות שהיה לנו קודם אז כאן חיפשנו את הנירס נ Aber ודרגה התפתונה ובסוף לדרך כלל זה מחולק לשש שכבות ‫אז קיבלנו כאן את ההתאמה ‫הקרובה ביותר.

‫אז זה בעצם החיפוש מבחינת... ‫מבחינת היצירה של האלגוריתם הזה, ‫אז מיוצר בצורה רנדומית, ‫בצורה דומה של כל אחד מה... ‫אנחנו רואים, יוניפורמית, ‫יוניפ של כל אחד מהערכים, ‫האם הוא יג'ונורט גם לרמה מעל או לא, ‫מה מה שאת אומרת, ‫קיבליסט, וככה אנחנו... ‫זו שיטה שהראתה אפקטיביות גדולה ביותר ‫למציאה רבה של וקטורים קרובים ביותר. ‫בכל זאת, בכל השיטות האלו, ‫ששייכות ל-ANN שלא קשור לרשתות מיירונים, ‫אפוקסינט נאורל נטוורקס, ‫אז בואו נגיד שזה נותן אחוזים מאוד גבוהים, ‫אבל זה לא מאה אחוז כמו לבדוק ‫את כל ה-N וקטורים שנמצאים בדאטאבייסט, ‫אבל כמובן שהטריידרוף לא מאפשר לנו ‫לבדוק את כל האפשרויות. ‫אז עד כאן היה עם ה-VectorDB, ‫יש עוד הרבה שיטות שאפשר לתת עוד.

‫רק תגמנו שיטה אחת של מה שנקרא ‫חיפוש שטוח בעזרת KNN, ‫ושלוש שיטות של אפוקסינט נאורל נטוורקס, ‫ועכשיו אנחנו בעצם קצת נחזור אחורה ‫למה אנחנו רוצים את הרג. ‫מה בעצם ההבדל בין עם או בלי רג? ‫קצת מוטיבציה. ‫אז כאן יש לנו מה קורה ‫כשיש לנו רג, ‫אנחנו רואים ששואלים מיהו איזשהו ‫מנצח בתחרות של 2025.

‫אם נניח שאלה הם לא אומן על 2025, ‫אנחנו רוצים בעצם ‫שיהיה לנו פגישה למידה עדכני ביותר. ‫המידה הזו יכולה להיות אינטרנט, ‫זה יכול להיות גם דאטאבייס, ‫או מקום מידע אחר כמו וקטור ספייס, ‫שהוא רלוונטי במיוחד בתחום שלנו. ‫אז נגיד את המסמכים שאנו אינדקסנו ‫בתוך הווקטור די בי כאן, ‫ואז יש לנו את ה-LLM ‫ויש לנו את הווקטור די בי.

‫הווקטור די בי, בצורך העניין ‫אם אינדקסנו מידע מהאינטרנט, ‫אז הווקטור די בי בעצם ‫הוא אינדוקס שלנו ‫בצורה של שנקים, ‫של קונטקסט של וורד אמבדינג, ‫של שנקים מדפים באינטרנט נגיד, כן? ‫אז עדיין זה מונדקס ‫בתוך רשו דאטאבייס, ‫זו הצורה נגיד שהנקיות עושות את זה. ‫ואחרי זה אנחנו בעצם אמרנו ‫שיש לנו ה-LLM נפרד, נוסף, ‫שהתפקיד שלו בעצם ‫זה לתקשר בתור צ'דבוט, ‫אז בעצם אנחנו שולחים לו את הקוורי ‫ואת הקונטקס ‫שעבר את התהליך של ההתאמה ‫למסמכים שנמצאים בווקטור די בי, ‫את כל הדבר הזה אנחנו שולחים ל-LLM ‫בצורה כזאת שאומרת לו, ‫אתה תמצא לנו את התשובות ‫רק מתוך המסמכים שאנחנו מביאים לך. ‫אוקיי? אז זה בעצם מאוד ממקד אותו ‫ומוריד מאוד את כל הבעיות ‫שהיינו עם ה-LLM ‫שלא כולל מידע עדכני.

‫אוקיי, אז כמו שאמרנו, ‫בפלור של הרג יש לנו שלושה חלקים, ‫השלב של המציאה של ‫הצ'אנקים הרלוונטיים, ‫השלב של היצירה של ה... ‫נגיד, שאילת החדשה הזאת, ‫והשלב של ג'ינרות של התשובה, אוקיי? ‫זה מה שקורה בלי המידע הנוסף שלנו. ‫אם נשאל שאלה עדכנית כזאת, ‫כמו מי זכה באיזושהי תחרות ‫שהתקיימה ב-2025, ‫אז נקבל אחד משתי אפשרויות, ‫אולי קצת יותר, ‫או שנקבל תשובה, נגיד, שגויה, ‫או שלא נקבל תשובה, ‫או שנקבל מה שנקרא הלוסיניישנס. ‫אוקיי? אז מערכת רג, ‫מאפיינית אחת.

‫אז בעצם השלב של ההיחזור ‫מחולק לשלב של אינדקסים ‫ושלב של שאילטה, ‫אז באינדוקס אנחנו לוקחים ‫את כל המסמכים ושומרים אותו ‫בבקטור סטור שלנו, ‫בשאילתה זה יעבור דרך אותו LLM ‫שיצר את הוקטורים בדטאביס שלנו, ‫שעכשיו אנחנו יודעים כבר ‫שלפי אלגוריתמים המתאים ‫אפשר לשמור את זה ‫בכל מיני דרכים, ‫ומחפש אותם בדטאביס שלנו. ‫עכשיו שלב האוגמנטציה ייקח ‫את המסמכים שהתאימו ‫בתוך הוקטור די-בי ‫ומוסיף אותם לשאילתה, אוקיי? ‫אחרי שקיבלו את ההתאמה המקסימלית, ‫ואז הוא שואל את אותו LLM יהודי ‫בצורה נגיד דומה כזאת, ‫בייסט און דה לייטסט אבדנס וכו' וכו', ‫הוא כאילו עונה לנו תשובה, ‫הג'נרטור, על LLM ‫שמשקללת את הקוורי ואת הדוקימנט ‫שמצאנו רק בוקטור די-בי, אוקיי? ‫אז זה, אני אחפש לראות ‫אם יש לי כאן עוד איזה דוגמה לשאילתה. ‫אנחנו רואים שיש לנו כאן כמה צ'אנקים ‫שהתאימו לאותה שאילתה.

‫אוקיי, השוואה בין עם רג לבלי רג. ‫אולי אני אמצא כאן איזשהו פרומפט. ‫בכל מקרה בואו נשווה עם כל מיני מערכות ‫אחרות שאנחנו מכירים של מערכת רג.

‫אז ביחס ל-IR, ‫אז ה-IR מחפש מסמכים ועושה להם רנקינג. ‫ביחס למערכת QA, ‫אז מערכת QA רגילה שלמדנו ‫של שאלות ותשובות, ‫היא פשוט יחסית פרמיטיבית ‫ועכשיו עלינו מאוד את היכולות שלה. ‫דיברנו על LLM רגילים, ‫עוד רגע אני אראה כמה דוגמאות ל-hallucinations, ‫וגם אפשר להשוות את המנגינונים ‫הפנימיים ל-recommendation system שראינו, ‫אבל פה אנחנו עובדים על דברים אחרים, ‫חוץ מההמלצות שיש לנו ב-recommendation system.

‫אז אם נסכם את השלבים שראינו עד עכשיו, ‫ראינו את השלב של תמיכה ‫בכל מיני סוגים של דאטה, של הטעינה, ‫צ'אנקים, קראינו כמה אלגוריתמים, ‫אחרי שיש לנו צ'אנקים, ‫אנחנו עושים אמבדינג בעזרת טקסט אמבדר LLM ‫שמיועד לזה, ואז אנחנו שומרים ‫את המידע ב-VectorDB שלנו, ‫זה ה-VectorStore, ו-VectorDB זה אותו דבר. ‫אוקיי, ואחרי זה, בזמן שאילתה, ‫אנחנו שואלים, ‫מקבלים, הופכים את זה קודם כול ‫ל-Vector, את השאילתה, ‫מחפשים אותה ב-VectorDB, ‫ויוצרים שאילתה חדשה סביב ‫השאילתה המקורית ‫והמידע שאנחנו מצאנו ‫שמתאים ביותר לשאילתה, ‫וה-LLM, שהוא בעצם אומן ‫על זוגות של שאילתות ותשובות, ‫הוא עונה לנו שאלה. ‫זו השאלה שם.

‫הנה, בואו נראה אם יש לנו כאן ‫איזה... ‫סבבה, אז... ‫יש לי פה איזה משהו שאפשר להציג? ‫לא, עוד לא. ‫נמצא עוד רגע איזה... ‫אני רוצה למצוא משהו שממש ‫מראה לנו איך פרומפ נראה, אוקיי? ‫אז יש לנו... ‫עוד מעט נמצא דבר כזה. ‫אוקיי, אז כמה הבדלים ‫ככה בין ה-fine-tuning לרג? ‫לא אמרנו עדיין מה זה ‫כאן-tuning, ‫אז-fine-tuning, הזכרנו את זה קודם, ‫עוד מעט אנחנו נרחיב, ‫זה בעצם אימון מחדש ‫של מודל פלל-לם במקום הרג, ‫אז זה דורש הרבה חומרה וזמן וכסף, ‫וצריך לעשות את זה לעתים טובות, ‫זו המגבלה, ‫אבל המערכת יכולה להיות ‫מערכת של מומחה יחסית.

‫אז כמה דקות, ‫אני רוצה קצת להתמקד ב... ‫אלוסיניישנס, אני רואה ‫שאין עוד הרבה זמן, ‫אז ניתן ככה כמה דוגמאות. ‫יש לנו או אלוסיניישנס ‫שקשורים לעובדות שפשוט לא נכונות, ‫או אלוסיניישנס נוספים, ‫שנקרא להם בצורך העניין, ‫פייסטפלנס, שפשוט גיבוב ‫של דברים, של שטויות. ‫אז אני אדלג כרגע על הסוגים ‫השונים של האלוסיניישנס ‫בגלל החוסר זמן.

‫כמה מילים על פיינט-טיונינג, ‫אז דיברנו על מה זה פיינט-טיונינג. ‫איפה בכל זאת פיינט-טיונינג ‫נרצה להשתמש? ‫אז נרצה להשתמש בפיינט-טיונינג ‫כאשר אנחנו רוצים להתמקד ‫בדומיין ספציפי, כן? ‫מערכת של רג יחד עם LLM, ‫שנעדכן אותו מדי פעם ‫לנושא ספציפי, זה כן יעזור לנו, ‫זה כן יהיה מוצלח. ‫וכדאי להשתמש במודלים קטנים יחסית, ‫למודלים ענקים שיש לנו כמו של ‫GPT-4 או ג'מיני וכדומה, ‫אז אנחנו קוראים להם ‫סמול לנגוויג' מודל, ‫הם לא כל כך קטנים, ‫הם קטנים באופן יחסית.

‫הנה יש לנו כאן דוגמה, נגיד, ‫לתחום של ביולוגיה, ביופרט, ‫לתחום רפואי, של הלסקר, ‫או לכל מיני, נגיד, אי-קומרס. ‫אז נרצה כן להשתמש במודל קטן, ‫שאותו כן זה יהיה לא נורא להתקן, ‫מודל עם מיליונים ולא עם מיליארדים ‫של פרמטרים. ‫אוקיי, אז יש לנו כאן כמה טכניקות ‫של SLM, ‫שאני אדלג על זה עכשיו ‫בעט חוסר הזמן.

‫דבר אחרון שחשוב להגיד ‫זה להתייחס לאג'נטי קראג ‫שעליו דיברנו. ‫אז לא תמיד הכלי הנכון זה ה-VectorDB ‫והמידה שיש לנו ב-LLM, ‫לפעמים נרצה להשתמש בעוד ‫כל מיני כלים נוספים. ‫אז זה בדיוק מה שאנחנו עושים ‫באג'נטי-KLLM ‫ובאג'נטי-KRAG ‫באופן ספציפי לענייננו.

‫אז האג'נט כאן, ‫הוא בעצם מוצא את הכלי הנכון להפעיל. ‫יש לנו רמות שונות ‫שחלק מהן אולי אפילו לא נקראות אג'נט, ‫כמו Workflow, שבו אנחנו מחליטים ‫שלב אחרי שלב, ‫נגיד איך עושים פרוססינג אוטומטי ‫לאימל שלנו, ‫ואז מפעילים לדברים ספציפיים, ‫נגיד סיכום של האימל, ‫הרמה של התראות, ‫קביעה של פגישות ודברים כאלו. ‫בשביל זה יש לנו פלואו ‫שלפעמים מוגדר יותר טוב ‫ולפעמים נותנים ממש לאג'נט ‫שמאומן לצורך זה ‫גישה לכל מיני כלים ‫שהוא בעצמו מאולחליט איזה כלים להשתמש.

‫אז זה האג'נטיק-קראג, ‫הנה כאן יש לו יכולת לאס-אם-אס, ‫לסלק, להיסטוריה של אימלים נגיד, ‫וגם לשאילטות ‫ולמערכת ראג באופן כללי. ‫אוקיי, אז זה האג'נטיק-קראג שלנו, ‫לעומת רגיל שאין לו גישה ‫לכל הדברים האלה. ‫אז היום, עם קצת חששות, כבר פותחים ‫גישה למערכות שיש לנו ‫שמותקנות על המחשב האישי שלנו, ‫ואז נגיד דיברנו על ‫איי-פי-איי של קלנדר נגיד, ‫או איי-פי-איי של מזג אוויר, ‫ואיי-פי-איי אולי של חיפושים, ‫כל מיני איי-פי-איי שונים, ‫עם כלים שמותקנים על המחשב ‫או באינטרנט.

‫יש גם המון כלים חינמיים. ‫אז שילוב של הדבר הזה, ‫יחד עם מערכת ראג, ‫יוצר לנו בעצם את המערכות ‫שאליהם הולכים היום, ‫שנקראות אג'נטיק-ראג, ‫ובעצם מאפשרים יכולות ‫באמת מאוד מאוד מורכבות. ‫אז עד כאן להיום.