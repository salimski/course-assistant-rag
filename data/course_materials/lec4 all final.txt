אז זה היה החלק הראשון, החלק שני זה יהיה שיכול להיות שהחלק השני יגלוש לשיעור הבא, אני אומר את זה כבר, בצער אז החלק השני יהיה על ה-relevance feedback ועל core expansion אוקיי, אז בעצם בחלק הראשון אנחנו לא נקשר את זה לאינפורמיישן רטריבל גם את הקלאסטרים שיש פה אפשר לתת לזה עוד שימושים להחזור מידע ולא שימוש שמובלע פה אחרי זה בסוף בחלק השני, אבל אנחנו כרגע נראה מה זה קלאסטרים באופן כללי למידת מכונה נלמד גם סיבוב בהמשך אז יהיה לנו את הקלאסטרים ואת ה-classification כשתי משימות של למידת מכונה שאנחנו נדבר עליהם, לא נדבר על דברים אחרים וככה ניתן את הרקע למי שאין לו ונחבר את זה ליהר בהמשך אז זה מה שאנחנו עושים עכשיו כן, אני מקליט? כן, בסדר אז ככה הרקע כללית, אינטואיציה כללית למה זה קלאסטרים בכלל מה המטרה של המשימה הזאת לא צליח, אפשר לעשות פה Alt משהו להסיר את זה Hide, אוקיי בסדר, אז לצורך העניין יש לנו משימה כזאת אז כמובן בתור עבודה סטודנט אחרי זה כולכם תהיו בכירים בהייטק אני מדבר כאילו רק בעיניים מונעתם להיות אחראים על שירות לפחות באיזושהי חברה שיש לה נגיד end-users למשל איזו חברת צלולר או משהו אחר תפקיד שלכם קיבלתם, התעקשתם כבר בהתחלה שתקבלו צוות זה לא יהיה באוויר, באמת יהיו מנהלים כי זה חשוב לכם להעל אוקיי אז קיבלתם חמישה מנהלי כוחות ו כדי שזה יהיה אפקטיבי החלטתם לחלק אותם איכשהו לקבוצות אגב זה באמת עובד ככה בהרבה תחומים יש בחברות צלולר נגיד קבוצות של לא יודע מה נגיד גיל סטודנטים וכל מיני קבוצות אחרות, לא יודע מה חצב אחר כן, כל מיני קבוצות מבוגרים יותר ומנסים להתאים לכוחים השונים אז לצורך העניין אנחנו לא אומרים מה המשמעות של כל קבוצה אנחנו רוצים איזשהו אלגוריתם שבאופן אוטומטי יחלק לנו את הקבוצות לפי דמיון של כל מיני תכונות שיש לנו אז כל מי שלא מכיר ללמידת נכונה כבר ראיתם על פיצוגה וקטורית גיא עם תכונות התכונות שאתם ראיתם היו נגיד מילים במודל בגאבורד כל מי שלא מכיר אז ללמידת נכונה אבל ברמה העקרונית יש לנו אובייקטים אם למדתם אובייקט אוריינטד אז בעצם אתם יודעים כמה זה תכונות לצורך העניין יש לכם data members וmethod members או עד שזה לימדו אתכם את זה אז לצורך העניין ה-data members זה צורך העניין זה התכונות של כל אובייקט סוג התכונה מוגדר בקלאס אבל בכל אובייקט יש לו ערכים מסוימים לתכונה וככה בעצם אפשר להגדיר אותו בצורה וקטורית, הערכים לכל התכונות האלו זה ה-vector ולמידת נכונה אנחנו קוראים לזה t-vector כי בסוף רצף של מספרים עם סדר מסוים זה וקטור כמו שנקודה במרחב זה וקטור דיברנו על זה כבר כמה פעמים אז למשל הערך 1, 5, מינוס 1 זה יכול להיות vector ה-vector מהסוג הזה שייך ל-r3 יש לו שלושה מימדים אוקיי אז כל מימד בעצם נתאר איזושהי תכונה תכונה של האובייקט אז במקרה הזה נגיד התכונות יכולות להיות גיל צו ספצי אקונומי נגיד אם מדובר על סטודנטים מה הם למדו בתואר וכולי וכולי כאן כתוב כל מיני השטויות כמו צבע בגדים וגובה לא חשוב נגיד שזה רלוונטי אוקיי אז בעצם אנחנו בלמידת נכונה מבחינים בין כמה סוגי למידה כאן יש רק שניים היום נהיה מאוד שימושי דברים כמו למידת חיזוקים אבל נעזור את זה כרגע אז בלמידה מונחת באמת עשינו עבודה מקדימה לצורך העניין אנחנו יודעים שאנחנו מתייחסים לכל אחד מהאובייקטים שלנו כאיזשהו וואי מסוים למשל הוואי יכול להיות בוגר מערכות מידע באיזו התחיפה זה יכול להיות סוג אחד סוג שני זה בוגר נגיד כולם זה בוגרים סוג שני יכול להיות בוגר מדעי הרוח באוניברסיטת התחיפה או לא יודע אוקיי כאילו יש לנו ארבעה תארים נגיד שהחלטנו כל אחד מהם זה הגיט הקטגוריה הזה בעצם יש לנו מה שנקרא מידע מיטוי הגיחנו אותו מראש המידע הזה הוא נכון ויש לנו כל מיני תכונות שמייצגים את האובייקטים שלנו לגיג סטודנטים שסיימו את התואר ועכשיו רוצים לדעת אם אפשר לחזות את הוואי באמצעות גיג אז הדוגמה שנתתי זה היה דוגמה לסיבוב, ברגע אנחנו לא מדברים על סיבוב אבל יש גם אפשרויות כמובן לחזות כל מיני סוגים של ערכים אחרים זה מאוד פשטני אבל זה ברמה עקרונית מה שמאפיין למידה מונחית שמראש סימנו כל אחד מהדוגמאות עם איזה מטא דאטה שרוצים להבין את הקשר בין התכונות, הפיצ'רים לבין אותו טרגט רג לעומת זאת בלמידה לא מונחית אנחנו מניחים עדיין שיש לנו דרך לייצג את האובייקטים השונים למשל כמו שראינו קודם לגבי השירות לקוחות אבל אין לנו איזשהו מידע נטויג נוסף כמו נגיד באיזה מחלקה אותו סטודנט למד אין לנו את המידע הזה אם יש לנו כל מיני דברים זה ערכים של מאפיינים, אני מדבר כאן באופן טבעי שרוצים ליצור איזשהו חיזוי מסוים אז מה שאנחנו עושים בעזרת למידה לא מונחית זה לא לחזות טרגט ואל יונתון אבל כן מניחים שאותם אוסף דוגמאות שאנחנו קוראים להם בלמידת נכונה דאטה סט אז אותו דאטה סט, אותו אוסף של feature vectors יש להם איזשהו ייצוג של העולם האמיתי כלומר שאם יש לזה התאיה סטטיסטית לטובת נגיד מאפיינים מסוימים או קשר ביניהם אז אפשר למצוא את זה בדייט הסט שלפניהם ולכן אם אין לנו את כל הלקוחות שיש יש לנו נגיד אלף לקוחות עם התכונות השונות שהצלחנו להשיג וגם לא בהכרח כל התכונות אז נוכל למצוא איזושהי תבנית תבניות שונות שלצורך העניין נגיד לבעיה שלנו יחלקו את מי ששייך לאותן קבוצות לחמישה קבוצות ואז הם יצטרפו לקוחות חדשים נגיד לרשת צלולה חדשה שאנחנו מייצגים אז יהיה אפשר לשייך אותם לאותן חמישה קבוצות לפי הדמיון של האובייקט שידא אובייקט הזה דומה לאובייקט הזה יותר מאשר הוא דומה לאובייקטים אחרים אז לקבוצות השונות האלו אנחנו נקרא קלאסטרים כל קבוצה עברית יש שמות כמו פשקול או כל מיני שמות נוספים בדרך כלל מתייחסים בשם באנגלית של קלאסטרים אז זו משימה של איזשהו סדר שאנחנו עושים בתוך ייצוג שלנו של אובייקטים בתוך סוג של עמידה שנקראת עמידה לא מונחת אוקיי אנחנו רוצים בשאיפה שאובייקטים כל אשכול בכל קלאסטר יהיו דומים אחד לשני ויהיו שונים מהאובייקטים שבאשכולות האחרים או אם נגיד את זה אחרת שהאשכולות יהיו שיהיה בהם הומוגניות בתוך האשכול שהם יהיו אובייקטים דומים אחד לשני אבל בין האשכולות אחת לשני יהיו כמה שיותר שונים אוקיי זה בעצם בגדול המטרה של קלאסטרינג ויש לנו כל מיני שאלות שאנחנו יכולים לשאול פה כמו למשל איך נכון לחלק את אנחנו יודעים שכל נקודה במישור במקרה הזה נגיד שציר ה-X וציר ה-Y זה שתי תכונות ערכים של תכונות כלשהם אפשר להמציא תכונות אם זה עוזר לכם אבל אפשר להסתכל על זה אפילו כבעיה מתמטית. עכשיו איך נכון לחלק אותם לקלאסטרינג זו שאלה שאין לה בהכרח תשובה נכונה ובעצם כחלק מגישת הפתעון יכול לתת לנו תשובות שונות אז זאת יכולה להיות חלופה אחת לשלוש אשכולות, שלוש קלאסטרים יכול להיות שזאת חלופה שמתאימה או אולי זאת ובעצם לכל אחד מהם אולי יש פתרון אחר. שאלה שנייה שאנחנו כבר יודעים כי אנחנו מכירים גם מי שלא למד למידת מכונה כבר דיברנו על דמיון קוסינוס אז אנחנו יודעים כבר איך אנחנו מודדים על דמיון בין אובייקטים היה בזמנו איזה טרנד כזה ויראלי הם אהבו להציג סלבריטיז ואת חיות המחמד שלהם ולהראות איך הם דומים וגם התחלתם בטרנד הזה אבל פעם זה היה ברשת הרבה דוגמה הבאה אמורה לייצג את הטרנד הזה אתה שואלים איך אנחנו יודעים בכלל ששתי אובייקטים דומים אחד לשני ואונים באנגלית when you see it you know it כאילו אם אנחנו רואים פה את שתי אלה אנחנו רואים את הדמיון ביניהם אבל בעצם זה לא תשובה טובה כי ככה לא נגדיר למחשב ששתי אובייקטים אלה דומים אנחנו רוצים שזה יעשה באופן אוטומטי אז אפשר לעשות את זה בדרכים שונות כמובן אנחנו מתייחסים לאיזושהי לאחת משתיים או למטריקת מרחק בין שתי וקטורים כמובן שכבר אפשר להסתכל אחורה על הדברים שלמדנו בהחזור מידע ולהגיד שיש אולי עוד שיטות נוספות אבל דיברנו על שיטות של מרחק וקטורי במקרה הזה אנחנו רוצים לעשות מסעור של מרחק וקטורי או להסתכל על דמיון וקטורי דמיון בין אובייקטים שימה של הקלאסטרים זה בין מה לבין מה אנחנו מודדים מרחק אפשר למדוד מרחק בין ה-feature vector, אותו אובייקט, לבין הקלאסטר ואולי אפשר למדוד מרחק בין שתי קלאסטרים אז בין מה למה אנחנו מודדים קלאסטרים גם איך עושים כל אחד מהשימות האלו זו שאלה בפני עצמם אבל בסוף אמרנו, נגיד חילקנו את זה לקבוצות ועכשיו רוצים לשייך איזשהו אובייקט לאחת מהקבוצות אז זו בעצם הגישה הראשונה שאנחנו אמרנו כאן לשייך את האובייקט לקבוצה ופתרונות לא נראה עכשיו מה שנקרא קלאסטרים יראכי אבל בגישה של הפתרון של קלאסטרים יראכי בעצם אנחנו כל הזמן מייחדים קלאסטרים השיטה הנפוצה זה מה שנקרא וואטאם אפ נראה עוד רגע איזושהי שקופית אז קצת מוטיבציה אפליקטיבית באמת בבריף יש לנו כל מיני אפליקציות שזה יכול להיות שימושי אלה הם, גם סוג התוצאה קצת נכנס פה פנימה אז כאן למשל יש לנו מפתכלים על מקורות של עמים שונים ומאיפה הם הגיעו ויש לנו פה איזשהו קלאסטרים יראכי של הדבר הזה זה כבר ממש קשור לענייננו אז יש לנו כאן כל מיני סוגים של דאטה זו דוגמה לדאטה מסוג טקסט אז הדאטה שאפשר לעשות לו קלאסטרים כמובן יכול להיות מכל מיני סוגים קומרי, קטגוריאלי או טקסטואלי נראה עוד מעט דוגמה לסוגי דאטה אחרים אז כאן זה כבר ממש מתקשר לקורי אקספנשן שלנו וזה כבר עונה על אחד מהתוצרים של איך להרחיב את השאילתה אחרי זה אנחנו נדבר מה זה הרחבת שאילתה ולמה רוצים לעשות את זה אז בואו נגיד שיש לנו פה איזשהו שאילתה של אפל היסטורי זאת השאילתה ועכשיו אנחנו רוצים לחשוב שמה בכלל מתכוונים באפ ויש לנו כמה סוגים של אפל האפל המקורי זה היה פשוט תפוח ומי שמכיר את הביטוי תפוח הגדול אז מכיר עוד משמעות וחברת אפל כמובן כולם מכירים אז כאן נגיד אם לוקחים את חברת אפל אז יש פה מגוון רחב של כל מיני מונחים שאולי בעצם זה כולל אותם אולי אם אנחנו אומרים אפל אנחנו מתכוונים לחברות הגדולות והטכנולוגיות שנסחרות בבורסה נגיד של נסטראג אז גם מייקרוספט רלוונטית ורק אפל אחרי זה בטופ פייסנק זה בחמישה טופ פייסנק פי 500 פייסנק פי 5 אם נרצה אז אולי בעצם הכוונה למוצרים של אפל ואז יש לנו את האייפון, אייפוד, אייפד ומק והמייסד של אפל וכו גם יכול להיות הרחבה אז כל הדברים האלה יכולים ככה לקבל כתוצאה או דרך קלאסטר הנה עוד דוגמה גם של טקסט היום לא חושב שזה נמצא בתור מוצר בצורה ישירה אבל בעצם מאחורי הקראים זה מוצר ותיק של גוגל שנקרא גוגל נוז שהיה הרבה רעש על זה כי בעצם כל מי שיוצר תוכן, עדיין היום יש בעיות עם זה בכל העולם כל מי שיוצר תוכן החברות הגדולות בעצם שואבים את המידע שלהם כאן נגיד רצו שכל המידע יהיה זמין לגוגל כדי שימצאו אותם במנוע חיפוש של גוגל וגוגל ניצלו את זה ועשו בעצם חדשות שהן לא תלויות מקור הם פשוט תקחו נגיד את כל החטבות שקשורות לנושא מסוים במקורות שונים והגדו אותם ביחד וזה גם סוג של טקסט קטרינג נוספים אנחנו פה על מידע של תמונות שמשמש ככה קטרינג כאן לעשות משימה בסיסית של כיבוד תמונה שנקרא סגמנטציה חלופה של התמונה לסגמנטים כל מידע שהוא תלוי זמן מקום ערך כאן מידע שהוא תלוי זמן מקום וכאן יש לנו כולה מידע שקשור לביולוגיה חישובית במקרה אתם מכירים אז זה בעצם שקול של מייקרו הרייז אחרי שפתחו בסוף שנות התשעים אני חושב זה היה של המאה שעברה את פרויקט הגנום האנושי אז המון מחקרים גנטיים והמידע הוא מאוד מאוד קשה למצוא בו תבניות הוא מאוד גדול, מאוד סבוך אז למשל אנחנו רוצים בעצם למצוא האם יש סממנים גנטיים שמזהים בצורה מוקדמת מחלות כמו סרטן אז אנחנו בעצם עושים כל מיני משימות כאלו שמחפשים תבניות בתוך המקווארים אוקיי, אז אני מקווה שהבנו את הרעיון גם מי שאין לו רקע מוקדם מה המטרה של קאסטרינג ומוטיבציה זה לא יריד על זה וקצת על איך פותרים את הבעיה הזאת אז יש לנו כמה דרכים, אני לא יודע היום למרות שיש כבר את העונה 5032 של משפחת סינסון אז אני לא יודע כמה מכם מכירים את זה אבל למשל יש כל מיני ככה נקטעים, אפשר להגיד שזה לא חוכמה כי היה להם מיליוני פרקים אז הם בסוף תמצא כל דבר שם אבל למשל הם חזו שדונלד טראמפ יהיה נשיא ארצות הברית זה היה זוי מאוד בזמנו כשהם אמרו את זה זה היה כאילו חצי מליצה הם חזו גם את הקורונה וכל מיני דברים אבל זה אמור להיות סדרה מצוירת בידורית אז יש לנו כמה דרכים לפתור את הבעיה הזאת וזה בעצם גישות שונות לפתרון זה לא רק יוצר אלגוריתמים שונים אז גישה אחת באה ואומרת קצת דומה למי שלמד סיבוב לגישה מאוד נפוצה בסיבוב וקרשיטיקציה זה לבוא ולקח את האובייקטים השונים אז כאן האובייקטים השונים וויזואלית זה דמויות שונות בסינסון ופשוט תחלק את המרחב בדרך כזאת שכל קבוצה תהיה שייכת לפלסטר אחד בלבד יש איזה טכניקה גם שאנחנו עושים את זה אנחנו מייצגים כל קבוצה על ידי איזשהו אב טיפוס אוטוטייפ וזה בעצם הבסיס של אגוריתם קיימינג שאני אראה עוד רגע אני עושה את זה ככה זריז וההנחה שרובכם כן מכירים את זה אבל אם זה מהר מדי אז תסתכלו אני מודע לזה שלא כולם מכירים את זה אז אם זה מהר מדי אני מנסה לענות פה על שתי אילוצים ביחד אז זאת השיטה הראשונה גם בסיבוב אגב יש המון שיטות להסתכל על איך אנחנו משייכים איזושהי קטגוריה מסוימת לאובייקט אז אחת השיטות זה לבוא ולהגיד בלי שום קונוטציות פוליטיות בואו ניקח את השטח כאילו במרחב נחלק אותו עם איזו הפרדה ונגיד מי שבצד הזה של ההפרדה שייך לקטגוריה מסוימת מי שבצד השני שייך לקטגוריה אחרת אז זה הרעיון של מה שנקרא שם הזה שהוא קשה שלא יהיה לו קונוטציה של כל מיני דעות קדומות שהוא נקרא גישה דיסקרמינטיבית מי שמכיר המילה דיסקרמינטיבית זה אפליה אבל אין שום קשר למונח הזה הכוונה היא הפרדה פרדה במרחב לא משמעות אחרת אז אותו דבר כאן, אנחנו מוצאים בעצם אם נרצה איזשהו נקודת סף שעד אליה שייכים לקלסטר מסוים ואימיה היא קלסטר שני ועוד סף נוסף לקלסטר שלישי וכן הלאה זה הרעיון אבל זה נעשה כאן בצורה חבויה לא בצורה ישירה באלגוריתם כמו שנראה עוד מעט אז גם סיבוב שאנחנו נדבר עליהם בהמשך אנחנו נדבר על גישה גנרטיבית ומה גישה דיסקרמינטיבית וכיתרון של בעיות סיבוב אז גישה שנייה היא גישה עיראפית אנחנו כבר עושים קורס שלם של ימידת מכונה אז נתרכז בשיטה הפופולרית יותר גישה אגלומרטיבית bottom-up בהתחלה כל אחד מהפרייקטים שלנו הוא בפני עצמו קלסטר כלומר יש קלסטר שמחיל רק אותו אוקיי, אחרי זה אנחנו בצורה איטרטיבית כל פעם מאחדים שתי קלסטרים עכשיו כבר אין לנו אובייקטים יש קלסטרים כי כל קלסטר מחיל אובייקט אחד כל פעם מאחדים שתי קלסטרים אד giờ בסוף כל הקלסטרים מאוחדים לאובייקט אחד ואז בסה און אז ממאילא נוצר לנו קלסטר אינרכי אז למשל ובדמויות שיש לנו כאן, רואים את זה כן, את המסכים, אז שתי אלה תאומות, הן הכי דומות, ולכן הן מיוחדו קודם אחרי זה יש לנו פה אמא ובת, מי שמכיר ליסא ומרד והן מיוחדו אחר כך ואז כל אלו מאוחדות ביחד ומשפחה כן, כן, משפחה, כדי לשנות, כאן שיותר בסדר אז זה הרעיון, ובסוף בסוף כל האובייקטים מאוחדים ביחד זה הרעיון של קלאסטרים ירחית, כאן נכנס לשאלה ששאלנו בהתחלה בין מה למה אנחנו מודדים דמיון או קרבה או מרחק בין קלאסטרים או בין אובייקט לקלאסטר וכדומה תודה שיטות נוספות יש, כמו שיטות רוסטות חד ספרות שמכיר את ה... הולכים מהסתברות של probability density function, מותר לכם אז ברמה עקרונית, כאן לצורך העניין אם אתם זוכרים את הניקודות הקרובות אחת ושנייה, אז הן צריכות יותר אז נעזוב את זה, אנחנו לא מתמקדים בזה, אנחנו בכל מקרה נתמקד בשיטה הראשונה אז אני אבלג רגע לשיטות הנוספות אז אנחנו נגזור את algorithm k-means מהשיטה הראשונה בעצם אנחנו בעצם, אני מזכיר, המטרות שלנו היו שתיים שיהיה דמיון בתוך קלאסטר והאובייקטים בתוך הקלאסטר יהיו דומים אחד לשני לפי הפיצ'רים, לפי התכונות והיו שונים מהשאר עוד רגע אני אדבר על זה שהוא written k-means מתמקד בעיקר במטרה הראשונה, לא בשנייה הוא דואג לזה שככל הניתן האובייקטים בתוך הקלאסטר יהיו דומים אחד לשני אז ההנחת המוצאה הפיאורטית של... כן יכול לשנות להתקשר לנושא שלו אז למשל אמרנו, אחרי זה נדבר על core-expansion ואנחנו נשתמש גם בצורה ישירה במונח שאנחנו עוד רגע נדבר עליו שהוא של פרוטוטייל וגם נדבר על זה שקלאסטרים משמש לקור-אקספנשן נכנס אבל הוא משמש גם לעוד מטרות זה ככה רק בשיעור היום אה לא, זה היום נראה, בהמשך אולי נראה עוד שימוש אז היה נראה לי יותר נכון ללמד קלאסטרים לפני שנדבר על פרוטוטייל פעם ראשונה אז ההנחת המוצאה של k-means הטבעורטית היא כזאת היא מניחה שבעצם כל קלאסטר הוא בעצם היה מראש בקטגוריה או לא יודעים אותה, היא נשפחה מאיתנו ובעצם כמו באיזה מפעל יצור מה שמייצג את אותה קטגוריה זה אב טיפוס מה שיוצרים מאב טיפוס של מצר, אז יוצרים אב טיפוס בתור אובייקט ועכשיו כל השאר האובייקטים מתוך הקלאסטר הם בעצם סוג של שכפול של אותו אובייקט אבל רק עם ראש, ראש סטטיסטי הראש הסטטיסטי אפשר לראות כאילו עשו פה ספריי בקלאסטרים השונים כאן, ככה זה נראה ובעצם ההנחה היא שהאב טיפוס הוא לא סתם איזשהו אובייקט שהכל ראש ממנו אלא הוא המרכז של הקלאסטר מה הממוצע החיזבאוני לצורך העניין? איך עושים ממוצע של וקטורים? קחים כל אחד מהמימדים ועושים ממוצע קחים את כל הוקטורים בתוך הקלאסטר שים ממוצע בכל אחד מהמימדים, כל אחד מהפיצ'רים ואז אנחנו מקבלים את וקטור הממוצע אנחנו קוראים לזה אז גם אב טיפוס או אוטוטייט וגם הוקטור המרכזי או סנטרויק אב טיפוס זה בעצם ממוצע וקטורי אבל גם אנחנו מניחים הנחה תיאורטית שזה באמת פרוטוטייב של הקלאסטר שהכל זה ראש טיס טיס טיס אז באמת הוא האובייקט האותנטי והכל ככה כמו אם נרצה השוואה ביולוגית אז נגיד שיש לנו בכל שכפול כל מיני כל מיני מוטציות מי שלמד קצת ביולוגיה ורובם הם חסרות משמעות פעם ב זה יכול להיות דבר חיובי ולפעמים זה יכול להיות דבר שלילי אבל אפילו ברמת החיידקים וברמות יותר קטנות וירוסים וכדומה אפילו שם יש כזה היה לנו בעיות קורונה כי היו מוטציות שחלקם היו מסוכנות יותר או מזיקות יותר אז זה הרעיון הנקודות השחורות לא יודע אם אתם יכולים להבחין פה בין הצבעים מייצגו אותנו את הפרוטוטייב של כל אחד מהקלאסטרים אוקיי אז בואו נסתכל רגע על האלגוריתם של קיימינס על המטרה שלו המטרה שלנו זה לעשות שני דברים במקביל דבר ראשון, עבור כמות מסוימת של קלאסטר מוגדרת מראש כנגיד שניים או שלושה שאמרו לנו מראש שאנחנו צריכים למצוא אז אנחנו רוצים למצוא את אותם פרוטוטייב המרכזים של כל אחד מהקלאסטרים ובהגדרה לכל קלאסטר יש רק פרוטוטייב אחד מרכז אחד ואנחנו רוצים במקביל לשייך את הפיצ'ר וקטורס בדטאסט שלנו, כלומר כל אחד מהאובייקטים שיש לנו לקלאסטר המתאים להם ביותר עכשיו אנחנו לא יודעים מראש מהם הפרוטוטייב ומהם השיוכים אבל בעצם אנחנו מנסים לכתוב פה בעיה אינפי קשה, אני כאילו לא רוצה להראות עכשיו את זאת הבעיית האופטימיזציה בעצם שמפתחים אותה אז אנחנו רואים שיש לנו פה שתי בעיות אופטימיזציות במקביל ולכן אי אפשר פשוט יש לך מכיר את המולך הגרדיאנטיסטיופי מי שלא אז לא חשוב אי אפשר פשוט לפתור את זה כבעיית אופטימיזציה רגילה זאת בעיית אינפי קשה ולכן אנחנו עושים איזשהו קירוב לפתרון שהוא מוצא איזשהו מינימום מקומי הרעיון של הקירוב אני אראה עוד רגע בנוסף עכשיו אם אנחנו חוזרים למדידת מרחק אז אני רק אזכיר שדיברנו כבר על כל מיני מטריקות מרחק כולל מרחק קוסינוס שנגיד לענייננו מתאים יותר אולי למחקים האחרים יש גם משקת אדיס דיסטנט שגם עליה דיברנו אז אני מדלג עליה וזו דוגמה שהראינו כבר בדמיון בין שני מסמכים דמיון קוסינוס ועכשיו נראה כבר את האלגוריתם של גיינוס אוקיי בתוך כדי אני אסביר איך בעצם אנחנו מוצאים את שתי המטרות האלה, איך מתמודדים עם הבעיה הזאת של למצוא את האופטימום גם של המרכזים של הסנטרויז וגם של השיוך להשכול זה בעיה אין פיקשה אז אנחנו עושים איזה קירוב שעובד ככה אוקיי בעצם חלוקה הזאת לארבע שלבים זה לא כל כך טוב אולי ארבע זה בעצם התנאי של עלולה ולא שלב רביעי בעצם אנחנו מגיעים להתכנסות או שמגיעים לאיזה תנאי עצירה ואז מפסיקים את האלגוריתם של גיינוס בכל מקרה שלב הראשון ויש כל מיני מטריקות לעשות את זה זה איזשהו שלב שמכיל אלמנט רנדומי אקראי ואנחנו לצות העניין מנחשים את הפרוטוטייפ אוקיי עוד רגע נעשה איזה סימולציה לזה ובשיטה המקורית ממש לקחו את נקרא לזה תחום הגדרה לקחו את הערכים הוולידיים לכל אחד מהפיצ'רים והגרילו שם הערכים וככה הגרילו את הוקטורים הראשונים ולא הוקטורים שבסוף יהיו פרוטוטייפ של אותם פרוטוטייפ הנקודות השחרות האלה אוקיי עכשיו בכל שלב בעצם יש לנו שני דרכים לייצג קלאסטר דרך אחת זה להגיד הקלאסטר מוגדר על ידי הפרוטוטייפ אפשר כל מה שנשאר לעשות זה לראות עם האובייקט שאנחנו רוצים לשייך לקלאסטר השונה זה לאיזה פרוטוטייפ הוא הכי דומה או הכי קרוב אמרנו שזה שקול ואז מצאנו את הפרוטוטייפ שהוא הכי קרוב אליו והכי דומה אליו זה הפרוטוטייפ מייצג קלאסטר אז זה הקלאסטר שלנו שייך מה עשינו במקרה הזה אנחנו שאנחנו יודעים מהם הפרוטוטייפ וכל מה שיש לנו לעשות זה לשייך את האובייקט לקלאסטר המתאים אוקיי אוקיי המטרה השנייה להציג קלאסטר זה על ידי האובייקטים ששייכים אליו נניח רגע שאנחנו יודעים מהם כל האובייקטים ששייכים לקלאסטר אוקיי ומה שהיינו רוצים עכשיו למצוא זה מה היה הפרוטוטייפ שלנו אנחנו יודעים כבר מה קלאסטר אם מישהו גילה לנו מה עם הקלאסטרים, לאיזה קלאסטר כל אחד מהאובייקטים שייך נשארנו רק למצוא את אותו פרוטוטייל עכשיו זה גם משימה פשוטה יחסית מה שנשארנו לעשות זה פשוט לעשות ממוצע על כל המימדים של הוקטורים ששייכים לאותו קלאסטר וסיימנו את המשימה שלנו, זה מה שיש לנו לעשות אז בשתי המקרים שאמרנו, יש לנו שני דרכים לייצג את הקלאסטר ובמקום לקרב את שניהם במקביל, אנחנו כל פעם מקבעים אחד מהם באלגוריתם של k-means ואז מוצאים אותו לפי הגדרה השנייה אז בעצם השלבים האלה שמוגדרים פה, 2-3, זה בעצם מה שמתבצע בכל איתרציה כל איתרציה עוברים על כל אובייקטים בדטאסט ושלב 2 בעצם מניחים שאנחנו יודעים מהם הפרוטוטייפים, יודעים מהם המרכזים ועכשיו כל מה שנשארנו לעשות זה לשייך את הוקטורים השונים לפרוטוטייפ שהם הכי קרובים אליו זה בעצם מקרב אותנו מבחינת האובייקטים לקלאסטרים ושלב 3 עושה את ההפך, הוא מניח שאנחנו יודעים מהם הקלאסטרים השונים לפי האובייקטים שהם מכילים ועכשיו כל מה שנשארנו לעשות זה למצוא את הפרוטוטייפ של כל אחד מהקלאסטרים ומנשיכים ככה, עושים איתרצית כל פעם על כל הדטאסט עד שמגיעים להתכונסות אז נעשה רגע איזה סימולציה ככה של אלגוריתם שלב ראשון נגיד שכאן הגדירו לנו שתי קלאסטרים אז הגרלנו באופן טיפש כזה שתי פרוטוטייפס, הכחול והאדום ועכשיו שלב 2 מניח שאנחנו יודעים מהם הקלאסטרים לפי הפרוטוטייפס פשוט עושים ארגמין, אתם יודעים מה ההבדל בין מינימום לארגמינימום? ארגמין שצי את המינימום אז בעצם אנחנו אומרים שיקפל זה לתת דוגמה ממערך, יש לנו מערך, בוא נכתוב את זה אפילו אה סטארטינג זה יופי, הפייץ' פייץ' לא זוכר שאתם עושים את זה אז נעשה את זה ככה אה בסדר אז יש לנו נגיד מערך כזה אני אמשום את האינדיצים שלו 0, 1, 2, 3 נגיד עכשיו הוא מכיל את הערכים 5, 2, 0, 8 אוקיי אז נגיד שהמערך הזה האינדקס נגיד נקרא לו מאות איי והמערך יהיה מאות איי אז סתם עכשיו אם אני שאל אתכם מה המינימום מעל איי של איי איי אז התשובה תהיה מה 0 כן זה העבר הכי קטן המינימום על המערך זה 0 אוקיי אבל אם אני אשאל מה ארגמין מעל הפרמטר איי של איי איי אז התשובה תהיה מה ובשתיים כי אם נציב איי שווה שתיים נקבל את המינימום אוקיי אז משתמשים בזה הרבה נגיד רוצים לדעת איזה פרמטר יגרום להסתברות הגבוהה ביותר אז עושים ארג מקס כי זה יותר מעניין אותנו אותו פרמטר מאשר הערך של ההסתברות אוקיי אז מה אנחנו רוצים לעשות כאן פחות מעניין אותנו מה המרחק בפרוטוטייפ אם משתמשים בפונקציית מרחק אז רוצים את הארג מינימום אם משתמשים בפונקציית דמיון אז רוצים את הארג מקסימום איזה פרוטוטייפ ייתן לנו את הדמיון הכי גדול לא מה הדמיון אלא לאיזה פרוטוטייפ בשייך אז זה בעצם אם משתמשים נגיד בדמיון קוסינוס אנחנו רוצים הארג מקס אוקיי אז אז אנחנו רוצים בשלב הזה בעצם לבדוק, יש לנו את האיקס הכחול והאדום למה, לאיזה מהם כל אחד מאובייקטים הכי קרוב והתשובה לזה זה הצבעים של הנקודות אלה צבענו באדום הכי קרובים לפרוטוטייפ האדום אלה צבענו בכחול הכי קרובים לפרוטוטייפ הכחול עכשיו נניח שאנחנו לא יודעים מה הפרוטוטייפ ונחשב מחדש את הפרוטוטייפ לפי הממוצע של האדומים והכחולים אוקיי אז זה בעצם השלב הזה זה בעצם הממוצעים או הפרוטוטייפ הסנטרויז החדשים המשיכים כך עד שמגיעים להתכנסים כן זה היה כמובן דוגמת צעצוע מה שנקרא אבל זה הרעיון אמא מה? אז יש בירושאות שנקראות K-Millions יש דבר כזה אבל זה אני חושב שהתשובה לשאלה הזאת היא דומה לתשובה לשאלה למה שהתחילו עם הרשתות החברתיות אז השתמשו בפייסבוק ולא בגוגל פלוס אז התשובה לשאלה הזאת הייתה מספיק טוב והוא היה הראשון אני חושב שזה אותה תשובה כאילו יש יתרונות בחציונים על הממוצעים אבל לרוב הוא עבד בסדר אז אם חייבו להשתמש בו אז השתמשו בו לאו דווקא יותר טוב אולי הוא פחות טוב אני מדלג על פריפרוססינג בעצם כמובן שככל שאנחנו יהיה לנו יותר קלסטר לכל אחד מהם יהיה לנו את הפורטוטייט משלו ואם לצורך העניין לוקחים קלסטר אחד שמכיל את כל האובייקטים אז פשוט נעשה ממוצע ביניהם אוקיי ואז יש לנו את השונות שמבטאת את הפיזור בעצם מה אנחנו מנסים לעשות מינימיזציה כשיש לנו נגיד k שווה 2 של קלסטרים אנחנו מחפשים את הממוצע ביניהם את הפורטוטייט ורוצים למזהר את הפיזור נגיד כשאנחנו מודדים את הפיזור על ידי שונות זה הדרך גם הכי נחוצה חשבו להשתמש בה אז פשוט אנחנו רוצים למזהר את הפיזור המינימלי אוקיי כן זה מה שאנחנו רוצים לחשב פורטוטייט עם מוצע וקטורי והקלסטר מול להיות איכותי על ידי איזה שנעשה מינימיזציה לשונות אני לא אכנס עכשיו לאזורני אני עובר עכשיו ישר לדוגמה הזאת כאן אוקיי זה בעצם שיטה טיפה יותר מתקדמת מהשיטה הראשונה שנתנו וזו דוגמה מאוד מאוד פשוטה בשביל להבין את האלגוריתם עם אפשר אחד בלבל בעצם הכל סקלרי אין פה וקטור במובן הטריוויאלי שמכיל רק מ-1 אוקיי אז יש לנו כאן תשע אובייקטים בדטאסט הזה ואנחנו רוצים למצוא את המרכזים שנתנו לנו שיש לנו שלושה מרכזים אוקיי אז מה שהשיטה הזאת אומרת קוראים לה שיטת פורג'י הגריל פורג'י וקטורי מהדטאסט זה יהיה בעצם ההתחול הראשוני לשלב הראשון של אלגוריתם קיימינס בניגוד לקודם שגרם לו סתם הערכים שהם נחשבים וולידיים אוקיי אז זה שיטה שהיא יותר מוצלחת מבחינת הנפואים שעשו הראו שזה נותן תוצאות יותר טובות וזה איזשהו היגיון מסוים והיא כמובן לא אופטימית, יש שיטות יותר טובות ממנה, אז כאן בדוגמה למשל, הגרענו כפרוטוטייפס את הערכים 1,2,3, שוב אפשר להתייחס לזה כפשוט ערכים כי זה וקטורי מימד 1 אוקיי אז עכשיו עושים איתרציות שבכל אחד עושים את מה שקראנו לו שלב 2 ושלוש, כאן עושים את זה לפי מרחק קוקלידי אבל זה באמת גם דוגמה צעצוע זה לא כל כך משנה אז כמובן שהווקטורים 1,2 ושלוש יישייכו לפרוטוטייפס 1,2 ושלוש, הם הכי קרובים אליה 1,0 ופשוט כל מה שגדול משלוש יישייך לפרוטוטייפס עם מערך שלוש זה שלב שתיים שלב שלוש, נעשה ממוצע בין הערכים השונים אז 1 ו2 הכילו רק אובייקט 1 עכשיו הישארו אותו פרוטוטייפס, לא יהיה בהם שינוי ה... פרוטוטייפס 3 הכיל פה שבעה ערכים נעשה ביניהם ממוצע ויוצא לנו 13 עכשיו נעשה עוד איתרציה נשייך שוב את כל האובייקטים לפרוטוטייפס שהם הכי קרובים אליו אז 1 הייתה כנראה בחירה לא כל כך מוצלחת, נשאר בו רק אובייקט 1 אובייקט 1 לפני עצמו היא מרחק 0 ועכשיו ה... הפרוטוטייפס שהתחיל ב-4 הזז לכיוון 13 אז 3 ו-4 כבר לא הכי קרובים אליו, הם יותר קרובים ל-2 אוקיי אז 2, 3 ו-4 משוייכים לפרוטוטייפס 2 וכל השאר, 13 ומעלה כמובן משוייכים ל-13 עכשיו עושים שוב פעם ייצוא מחדש 1 נשאר 1 הקלאסטר שמכיל את 2, 3, 4 הממוצע בין שלושתם זה 3 והממוצע בין אובייקטים 13, 14 18, 19, 20 זה יוצא בערך 16.8 ואז מגיעים להתכנסות ומסיימים אוקיי אין יותר התכונים אפשריים ו... אפשר להראות גם, אנחנו לא נכנס לזה כרגע, שאפשר להשיג פתרונות יותר טובים אם היינו איפשהו מגיעים לפתרון הזה אז על ידי המדד הזה שנקרא WSSE אז היינו רואים שסך הפיזור בעצם שיש לנו בדטאסט בקלאסטרים השונים הוא קטן יותר וזו המטרה שלנו ב-K-Means בעצם שהאובייקטים יהיו דומים יותר כלומר הפיזור כמה שהוא יותר קטן בכל קלאסטר אז שאלות על כאילו עד כרגע גמרנו בסלב של הקלאסטרים אולי נרחיב אותו קצת בהמשך אם נספיק אולי נעשה גם קלאסטרים יחי אז כאילו גמרנו את החצי הראשון של השיעור החצר שלנו ובחצי השני אני מדלג פה על אפשריות לשיפור, מי שמכיר K-Means++ ומדדים לשיעור קלאסטרים אני מדלג ישר לנושא השני אוקיי, אז הנושא השני אנחנו בעצם מדברים פה על רלוונט פידבק שכבר אתכם מדבר עליו אוקיי, ו בעיקר בעיקר דגש יהיה על קווי אקספנצ'ן, בעצם גם השימוש ברלוונט פידבק יהיה לצורך קווי אקספנצ'ן, אז אם יש עכשיו מסכמים שניים אז זה הזמן להתחלף זה שיעור בליץ אני יודע אבל אולי נחזור לחלק שיעור הבא גם, אז בעצם למה יש לנו מטרה בכלל לעשות קווי אקספנצ'ן עוד רגע נראה מה הקשר לקלאסטרים שדיברנו עכשיו אוקיי, אז הטרה של קווי אקספנצ'ן בעצם הוא להגדיל את הריקול שלנו מדד שכבר דיברנו עליו, מה זה ריקול? ריקול זה מדד שבעצם שואל, בואו נניח בצורה פשטנית שיש לנו מסמכים שחלק רלוונטיים וחלק לא רלוונטיים אוקיי, אז המודל הפזור הוא מצא רק חלק מהמסמכים הרלוונטיים אנחנו עכשיו רוצים להגדיל את כמות המסמכים הרלוונטיים שימצא יותר קל זו הכוונה שריקול מה שהוא מודד זה לצורת העניין מתוך הערכים הרלוונטיים לאותה שאילתה כמה הוא מודד את זה לסך השאילתות אבל כמה מסמכים חזרו שהם באמת רלוונטיים, אנחנו רוצים להגדיל את היחס הזה כן שיהיה כמה שיותר קרוב לאחד, כלומר למאה אחוז אז זה ככה תזכורו אותה לריקול שם אז כבר למדנו כמה סוגים של רלוונט פידבק, ואנחנו לא נתמקדים בזה כמו שאמרתי ולצורך העת בואו נסתכל על מצב כזה שנותן לנו מוטיבציה אוקיי, אז כאן יש לנו כל מיני מסמכים שחלקם מתעסקים בקלבים, חלקם מתעסקים בחתולים והשאילתה הייתה בכלל סוג מסוים של כלב אני לא מכיר את המילה הזאת כשהוא מכיר איך לקרוא בדיוק את הסן הזה של הכלב ק-9 ק-9, ככה אומרים ככה מבקים את זה ק-9 אוקיי יפה, אהבתי אז הק-9 זה זן מסוים של כלב ואם אין לנו שום דרך כן, אנחנו כבר דיברנו ממש בגדול על מילונים כאלה שמדברים על מילים נרדפות או על יחסים אחרים בין מילים הדוגמה המפורסמת בהיסטוריה זה וורדנט לא אכפנו הרבה אבל הזכרנו את זה הוא הופיע בבוחן אבל הק-9 הוא סוג של כלב, אוקיי? זה יחס שקוראים לו בבלשנות הייפרנט כאילו אין פה מילים נרדפות אבל יש פה אחד מתוך קבוצה גדולה יותר, כן? הדוג זה הקבוצה גדולה יותר וק-9 זה סוג של דוג אוקיי? אז זה הסוג של היחס הזה אבל היינו רוצים בעצם להחזר לפחות מסמכים שמכילים דברים קשורים ואז אולי נקבל, אולי למשל המסמך אפילו מדבר על אותו סוג של כלב, ק-9 אבל הוא לא כותב את זה בצורה מפורשת ואז אנחנו מפספסים אז בעצם אם למשל היינו מרחיבים ומביאים בשיטה הזאת מסמכים שכן מכילים את המילה כלב אז היינו מרוויחים, כאן לצורך העניין הוא מודד בשיטה הרגילה אם לא עושים core expansion, שוב רק את המסמכים שמכילים k-9 אבל יכול להיות שיש לנו עוד מסמכים רלוונטיים נוספים בגלל קשר שלהם דווקא למילה כלב, כי k-9 הוא סוג של כלב אוקיי? אז זה ככה הרעיון של השיטה הזאת ואפשר לעשות את זה בכל מיני דרכים אוקיי? אם נגיד שבכלל היינו מחפשים תמונות אז אולי היינו רואים תמונה שיש לה דמיון מבחינת התמונה וכאן אנחנו רואים תשובות לשיטה של new space satellites application איך יקבלו כל מיני תשובות אחרי core expansion זה דוגמא ככה לטקסטים שלא כתובים את הטקסט המקורי בפירוש במסמך ובכל זאת הם רלוונטיים אוקיי? אז הנה ככה המונח המפוזי new space satellites application בואו נניח כרגע שהרחבות השונות נעשים כמו שהתרכזנו עד עכשיו ברמת המילה או ברמת הטרק אוקיי? גם פה יש כל מיני ניואנסים שלא ניכנס אליהם אז הרעיון הוא כזה בואו נניח שוב שיש לנו וכאן נכנס כבר הקשר שלנו לכאמינס סנטרויד זה הפרוטוטייפ זה אותו דבר אז בואו נניח שיש לנו מסמכים רלוונטיים לשילט המסלמת ולא רלוונטיים לשילט אנחנו יודעים מה הם, בואו נגיד שיש לנו בצרך העניין איזשהו benchmark כמו שדיברנו עליו בשביל שערכת האיכות ואנחנו רוצים להגדיל את הריקול הריקול מתייחס למצב שאנחנו יודעים מה רלוונטי ומה לא אוקיי אז יש לנו איזשהו benchmark צריך בעצם הסימון הזה כאן זה וקטור הפרוטוטייפ של כל המסמכים שמופיעים באוסף המסמכים של ה-V צריך איך אנחנו בעצם מגדירים אותו כמו שאמרנו אנחנו עוברים על כל המסמכים, כל הוקטורים ה-V של D בעצם זה צורה וקטורית של מסמך D נגיד למשל שבעזרת מודל באגבות אנחנו מציגים אותו בצורה וקטורית כלומר יש שלב שאנחנו קוראים לו רקטוריזציה של המסמך שהופכים את המסמך לוקטור אז אנחנו ראינו כבר שיטה אחת שיש בו כל מיני וריאנטים או שיטה וחצי וגם דיברנו על ה-positional אינדקסים ששם אנחנו גם מכניסים את המיקום של המילה ונעזוב את ה-positional אינדקסים, יש לנו bag of rules שהם לוקחים את כל הטרמים שלנו וכל אחד מהם הוא בעצם מהווה מימד או feature ודיברנו על דרכים שונות בעצם לתת לו ציון זה בעצם השיטה שבה נציג את המסמכים השונים אז מה שאנחנו עושים זה לוקחים את המסמכים השונים שמופיעים לנו ועושים ביניהם ממוצע ותגורים זה בדיוק הגדרה של הפרוטוטייפ אז הנה דוגמאות של פרוטוטייפ של קבוצות שונות, יש לנו את העיגולים וזה הפרוטוטייפ שלהם את המוריינים ואת האיקסים זה בדיוק מה שראינו קודם בהשקחים של קלאסטר אז אנחנו רוצים להגדיר דבר כזה יש לנו מסמכים רלוונטיים ומסמכים לא רלוונטיים מסמכים רלוונטיים נסמן אותם עם dr מסמכים לא רלוונטיים נסמן אותם עם nr אז בעצם אנחנו אומרים דבר כזה בוא נניח שהרלוונט והנרלוונט היו שתי קלאסטרים ועכשיו לכל אחד מהם יש לו את הפרוטוטייפ שלו בדיוק בצורה שבה הגדרנו אותה אז אנחנו אומרים שאנחנו רוצים לקבל את המסמכים שבעצם במקסימים את הדמיון בין השאילתה לרלוונטים את המרחק בתור ערך בין הדמיון של השאילתה לרלוונטים שמיוצג על ידי הפרוטוטייפ שלהם המרכז שלהם הצנטרות שלהם שוב, מה זה mu dr? אתם מבינים? יש לנו אוסף של מסמכים רלוונטים לשאילתה מסוימת q נגיד השאילתה היא תן לנו לא יודע מה היה קודם עם נאסא אני כבר לא זוכר New Space Satellite Applications זה השאילתה אז אנחנו רוצים לקחת את כל המסמכים ששומנו רלוונטים לשאילתה הזאת ויש להם עכשיו את המרכז שלהם זה הסנטרויד אז אנחנו יכולים למדוד מה המרחק לצורך היה נגיד מרחק קוסינוס בין השאילתה לבין תו סנטרויד למשל למדוד את זה לידי tf-idf, כמו שדיברנו לפני המבחן, אז יהיה השאילתה 1 לצורך tf idf ניקח מהמסמכים שלנו וככה כל אחד מהפיצ'רים יקבל ציון ויש לנו וקטור לשאילתה ווקטור למסמכים השונים הממוצע של הוקטורים הרלוונטיים זה הסנטרויד זה מה שכתוב פה בצד שמאל בצד ימין כתוב לנו שיש לנו גם נמדוד גם את המרחק בין הוקטור של השאילתה אותו הוקטור שהופיע כאן לבין הממוצע, הפרוטוטייפ הסנטרויד של המסמכים הרלוונטיים בסדר? הבנו אותה שני הפרוטוטייפים לפי העייפות והעיניים שלכם או שפשוט נגמרתם בגלל הבוחן זה היה גדול עליכם ואני לא חושב שהסתדרתם בסדר או שלא יודע יש לנו יום הבוחן מה? יום ערוף? בסדר אז בכל מקרה אנחנו רוצים למקסם את הדבר הזה אוקיי? אז איך אנחנו עושים את הרשימה זאת? כן אז בעצם כאן אנחנו אומרים שבדרך כלל אפשר להניח שה קווירי אחרי, רוצים בעצם למצוא קווירי שהוא כבר expanded הגדלנו אותו, ספנו לו עוד כל מיני מילים שלא הופיעו במקור על ידי זה שאנחנו נמקסם את המרחק הזה אוקיי? אז רק על כל מיני הנחות אנחנו יכולים להגיד שהוא שווה לפרוטוטייפ של DR ועוד הפרש ביניהם אוקיי? עכשיו אם אנחנו מפרקים את זה אז בצד שמאל כתוב לנו פשוט את הפרוטוטייפ של DR, של הסמכים הרלוונטיים סליחה, וכאן את הפרש ביניהם זה מה שכתוב כאן אוקיי? אז זה פשוט פירוק של מה שכתוב פה למעלה אוקיי? אז בואו נראה איזושהי הדגמה וקטורית של הדבר הזה אז לצורך העניין, העיגולים פה זה המסמכים הרלוונטיים והאיקס פה הפוך, כן, והאיקסים הם הלא רלוונטיים, אז יש לנו כאן, זה הצנטרויד של הרלוונטיים וזה הצנטרויד של הלא רלוונטיים אוקיי? ו מה שאנחנו רוצים לעשות, זה ליצור בסוף את ה-Q אופטימל, לקחת את ה-Q המקורית, את השאילתה המקורית, וממנה ליצור את השאילתה האופטימלית על ידי המקסום של מה שאמר אוקיי? עכשיו, מה בעצם אנחנו רוצים לעשות? אנחנו רוצים לבוא ולראות איזה מילים אם נוסיף אותם לשאילתה זה ימצא לנו את הדבר הזה עכשיו מה שאנחנו אומרים כאן זה קצת סיבוך של מה שדיברנו כאן כאן, ומה שאנחנו אומרים זה שזה אלגוריתם אחד המקובלים בשביל בעצם לעשות ה-core expansion, אנחנו אומרים יש לנו את השאילתה המקורית אנחנו רוצים עכשיו לקבל שלושה פרמטרים Alpha, Beta ו Gamma, אוקיי? אז לכל תלק פה אנחנו נותנים משקל אחר, אנחנו אומרים לשאילתה המקורית, נקרא לזה Q0 לצורך העניין, אנחנו ניתן משקל Alpha, הוא לא שאנחנו מבטלים אותה לגמרי, אנחנו אומרים בכל זאת אנחנו רוצים כמו שהיא הופיעה, את השאילתה המקורית ואנחנו נותנים משקל Beta לצנטרויד של המסמכים הרלוונטיים ומשקל Gamma לצנטרויד של המסמכים הלא רלוונטיים, ואז בעצם השאילתה החדשה אחרי ה-expansion מורכבת מה-Alpha כפול השאילתה המקורית ועוד Beta שזה הצנטרויד, הפרוטוטייב של המסמכים הרלוונטיים פחות גם הכפול צנטרויד של המסמכים הלא רלוונטיים אוקיי? אז זה בעצם האלגוריתם פה, יש לנו גם ככה, זה יצר לנו וקטור שיש בו גם משקלות חיוביים וגם שליליים בגלל שלק TML כאן, כן? גם היום הוקטורים המקוריים היו כולם חיוביים, בגלל הדבר הזה היו כאן גם וקטורים שליליים יש לנו כאן כמה ערכים מקובלים ו... אנחנו נעים בעצם בין שתי הנחות הנחה ראשונה, ה-1 זה שהמשתמשים שלנו בעצם יודעים יפה מאוד, יודעים היטב מה הם המונחים שמופיעים בדאטה קולקשן שלנו והקורפוס שלנו, והנחה ה-2 זה שה מספכים הרלוונטיים שלנו יכילו מילים שהן דומות למילים שמעניינות אותם אוקיי? כלומר, אם ניקח עוד מילים מהמספכים הרלוונטיים אוקיי? אז נוכל להשאיר בעצם את המינוחים המקוריים ו... ברגע ש... פריקו לא מספיק גדול, בעצם זה מראה שהנחה ה-1 היא לא מספיק טובה. 

הנה דוגמה לשני מונחים שאנחנו נמניח ש... רובכם יודעים שהם אומרים בעצם אותו דבר חוץ מהארץ מותה של האסטרונאוט או הקוסמונאוט כן, האמריקאים במלחמה שהיה להם בזמנו מול הרוסים ולברית המועצות אז זה עוד לא היה רוסיה קראו לזה אסטרונאוט וברית המועצות קראו לזה קוסמונאוט אבל בעצם זה אומר אותו דבר אז איך אנחנו נדע למדוד את זה? כבר למדנו כל מיני מדדים אז על זה אני לא אחזור. אוקיי? אז איך בפועל אנחנו בוחרים כאיזה מילים אז אנחנו רוצים להוסיף אז אפשר לנהוג בשיטה הזאת ולראות מה במקסם זה יכול להיות תהליך מאוד מאוד יקר ויכול להיות שיש הרבה מילים לא רלוונטיות אוקיי? ו... וזה נותן לנו עוד מוטיבציה מיני מוטיבציות רבות אחרות ללמוד על דרכים אחרות לעשות כבר אקספנצ'ן. כמשל אחד מהדרכים האלה יכולים להיות ישירות על ידי קלאסטרינג שמוצא לנו בעצם דמיון בין מילים אבל אנחנו נלמד גם עוד דרכים אחרות שנכנסות לנושא של World Embedding ואת זה נלמד בהמשך. 

אז עד כאן לילה. אני חושב שזה לא היה מהיר מדי כי אני הרגשתי שאני עושה בליטסטאפ זה היה ממש מהיר אוקיי אוקיי פתאום זה דבר רבה אפשר לעשות אני אשמור לא שייך פרופרנטה היא פרופרנטה של המסליטים הרלוונטיים לדבר על העניין אני מוסיף אני יכול לדבר עליך כאחד שיודע מה זה נגד נכון, נכון? כן, זה נגד נכון אוקיי, אז יש לך פה לצורך העניין שני דרכים להבחין בין הקלאסטר שבעצם מיוצג איזו הרלוונטיה ולא בדיוק כך. אנחנו שינו את זה בשיטה של הפטר, עם הקרבה לפרוטוטייפ בעצם, זאת בעצם שתי קטגוריות רלוונט ונאן רלוונט ועכשיו אנחנו אומרים דבר כזה יש לנו בעצם שני דרכים להגיד שאובייקט נשיא המקום לרלוונט או שהוא לא מכיל פיצ'רים שליליים אחרי זה נתן פיצ'רים שליליים ופיצ'רים שהם משייכים אותו למטרה שלילית אז איך נוודא שהוא רחוק מהם יותר? ניתן להם ערך שלילי ואז בעצם אם הוא יתקרב לשם אז סימן שהוא לא רלוונטי אוקיי? ואיפה נתעדף את הפיצ'רים החיוביים זה על ידי ביטויים ששייכים לקורולציה של החיובים זה במקום לעשות את זה כפסיפיקציה בעצם עושים את זה כמחק מקטורי אבל בעצם יכול להיות שהיינו רוצים על ידי כל מיני מידדים שאתה מכיר לאחר מעשה לעשות קלסיפיקציה ואז לראות מה מסביר הכי טוב לשייפות לאותו קטגוריה בסרטים אחרות יש היום את שאפ וכל מיני דרכים אחרות של איך לקרות אם אתה מכיר של הסבר לאחר מעשה שהן מאוד שימושיות ואלה למים ש כמו חופתה שחורה כזאת שאי אפשר לבדור שם אבל זה הרעיון בעיקרון אבל כאילו הרעיון התליהי הברור כי הרגשתי שצפתי אתה נשמח כאן עם החזרה בסדר אני משתדל בכל שיעור לאותו חזרה אבל בסדר אוקיי אז זה לא רק היה רושם אצלי זה משתמש הבנתי אז לכם זה היה אותו את השאלה טוב