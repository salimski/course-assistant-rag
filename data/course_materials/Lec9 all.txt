כבד מדי, אז אנחנו נדבר על חיפוש באינטרנט. עם קריצות קצת אולי מדי פעם על איזה תחום שארי משתנה כל הזמן. אוקיי, אז החלק הראשון זה קצת השפעה קטנה בין מה שלמדנו כפלואו של חיפוש רגיל על מסמכי טקסט, לעומת הפלואו של גם משתמש בשיטות IR-קלאסיות, בנגיד איזשהו מחשב אישי או ארגון קטן או משהו כזה.

אוקיי, אז הבדל מהותי ראשון הוא בעצם בפוקוס של הבעיה. אם בדאטאביס הקטן או בארגון הקטן או במחשב האישי אנחנו מדברים על בעיה של בעצם דלילות של מסמכים, העיקר תמצא לנו, יש לנו כל הזמן לעשות core expansion לעלות את ה-recall, שנתפוס יותר מסמכים, אז עכשיו באינטרנט אפשר להניח שבדרך כלל המידע נמצא שם. אוקיי, ושם הבעיה היא לחפש מה שנקרא לברור את המוץ מן הטבע ולחפש את המידע באמת הרלוונטי, כלומר שם הבעיה היא יותר של precision מאשר של recall, להיות תביא את המסמכים הרלוונטיים.

אוקיי, אז זה בעצם מה שאמרנו ההתאיה ל-precision ב-web search, זאת אומרת recall ו-IR, ואנחנו דיברנו גם על כל מיני מדדים, חלק מהדברים האלה כבר הזכרנו, על כל מיני מדדים כמו precision עד k, כלומר אנחנו לוקחים, זה בעיקר רלוונטי, בוא נגיד דווקא לציבור שמשתמשים ב-web, אז אנחנו מסתכלים על ה-k תוצאות הראשונות, שלא עושים בדרך כלל דבדוף, אלה הם כן ממש רוצים לזכות כל פיסת מידע שנוכל להשיג, אז מעניין אותנו מה שנקרא precision עד k, כלומר נגיד לוקחים 10 תוצאות ראשונות עם k שווה 10 במקרה הזה, ועוברים עליהם לראות איזה מהם רלוונטיים ואיזה לא, אוקיי, אז אם למשל החלטנו על k שווה 10, אז לדבר הזה אנחנו עושים mean average precision, כלומר על שילטות שונות שאנחנו שואלים, אנחנו בודקים בתוך ה-precision עד 10 מה הממוצע של הדבר הזה, אוקיי? ויש לנו פה עוד מדדים נוספים, למשל אנחנו נדבר גם על תוצאות אחרות, נגיד NDCG, Normalized Discounted Cumulative Gain, אנחנו מדברים פה על תוצאות שהן נעים באיזושהי סקאלה מסוימת, זה משהו שמעניין אותנו כי יש חיפושים שיתנו לנו תשובות סופר רלוונטיות, ואז במקרה הזה אנחנו לא נרצה תוצאות שהן ככה בינוניות, אבל בטוח כולכם, לא יודע בשביל איזו עבודה באוניברסיטה או משהו, חיפשת מקורות בנרות שפשוט לא מצאתם ותביאו לי משהו שקשור קצת, ואז תהיו מרוצים במקרים כאלו, אז אנחנו צריכים בעצם את שהיא דרך וה-NDCG זה אחד מהמדדים שעושים את זה, שימדוד לנו אוטפוט שזה לא נכון להסתכל עליו כבינארי, רלוונטי או לא רלוונטי, אלא איזשהו משהו בסקאלה הזאת, בין רלוונטי ללא רלוונטי וזה אחד מהמדדים שפודק את הדבר הזה, הנה כמה דוגמאות ככה בשביל חלק מהמדדים, אז זה רלוונטי מאוד להראות את התוצאות של precision at k, אם ניקח precision at 1, אז יכול להיות שנפשש את התוצאה הראשונה, אבל אם נשקול להגדיל קצת את ה-k אבל לא להגדיל מדי, אז נגיע, כנראה נמצא תוצאות רלוונטיות כבר, בהנחה שאנחנו עושים את התהליך כמו שצריך, ושבאמת קיים. אתה תעשה פייצ'ינג, זה חלק מהשאלה, או שאתה תגיד, מה שלא מופיע פה, טוב, היום גוגל ניתן, בואו נשים בסוגריים רגע את כל ה-ai, אז אני מניח שאתם יכולים לעשות rewind כזה כמה שנים אחורה, שגוגל די נהייתה מונופול, כל מי שעבד בארגונים עם חיפוש אז אולי מכיר את בינג, אם יצא לכם, למה? כי בינג קצת יותר זול ב-api, גוגל יש לו כל מיני הגבלות בכמות בשעה, וכל מיני הגבלות דרקוניות מהצוונות כאלו, שאם אתה ארגון גדול אז אולי הוא יפתח לך את זה, ואם לא, זה לא משתלם לו, אז הוא יחסום אותך, וייתן לך קצב מאוד איטי במחיר מאוד יקר. אז אם נסתכל על זה, אז אין לנו כל כך אלטרנטיבות, אבל בתור משתמש, אם אנחנו רואים שהתוצאות שלנו, אפילו אם זה לא בדיוק חיפוש בגוגל, אלא משהו שהוא יותר domain specific, אם התוצאות לא מתאימות.

אז נבוא אולי לכלי אחר, או שנשקול רק את התוצאות שבעמוד הראשון לכל היותר, לא נתחיל לעבור על עשרים דפים שיש בגוגל, זה כאילו... בדרך כלל יש מאמץ גדול להביא בתוצאות שהן ויזיביליות, אפילו בשביל לעשות page down, להיות התוצאות באמת הכי איכותיות. משקיעים בהן גם את כל המאמצים. אוקיי, אז נדבר עוד מעט גם קצת על PageRank ועל עוד אלגוריתמים אחרים.

אז שם עושים את רוב המאמצים, דיברנו על זה כבר לדעתי, שיש אלגוריתמים שיש לנו כמה מדדים לראות שזה מוצלח, חוץ מהקליקים, שכל בראוזר, כאילו זה עוד אינטרס ככה לבראוזר, לשלוח בעצם תוצאות לגוגל עצמם, ולדעת ממש את המידע הזה למרות שזה ב-client side, מה שנקרא ולא ב-server side. כמה עשו קליקים על הדפים, אם אתם אי פעם, זה בטח גם במונועי חיפוש אחרים, ניסיתם להתיק את הכישור שנמצא בתוצאות חיפוש, אתם תראו שזה לא באמת הכישור של הדף, אלא זה איזשהו כישור של גוגל עצמו, שהוא יפנה אותנו לדף, כי זה בשביל שהם יוכלו בעצם לעקוב אחרי התוצאות ככה יותר טוב, ולדעת מה רלוונטי ומה לא. אז כאילו דבר ראשון שיודעים, זה כבר כאילו קשור גם לחיפוש, אבל לא רק, זה שקליק על התוצאה זה מדע שנחשב מאוד אמין יחסית, לא מספיק אבל נחשב מאוד אמין, אז כאילו זה נותן לנו איזשהו feedback אחד, וfeedback אחר שגם עליו דיברנו, זה בעצם קצת יותר קשה לחישוב, אבל לבדוק כמה זמן באינו באזור מסוים, כל הזמן עמדנו על, עמדנו על אזור מסוים בדיוק, אז כאילו אם לא עשינו scroll down וראינו את התוצאות, אז כנראה אפילו, לא דיברנו עדיין מה זה snippet, התוצאות עצמם אין לנו עדיין, אנחנו לא רואים אותן בכלל כאופציה להסתכל עליהן, אוקיי, אבל, הסניפט של התוצאות כולם מכירים מה זה כאן, כשאנחנו עושים חיפוש, אנחנו מקבלים איזושהי כותרת שהיא עצמה קישור לדף, ולמטה יש כזה סיטוט קטן מתוך הדף, שגם בפנים יש איזה אלגוריתם למה בחרו דווקא אותו, אוקיי, שהוא בעצם נותן לנו איזשהו מידע רלוונטי, שאמור לעבוד מוטיבציה לראות עוד, כן, להיכנס לדף, אז הרבה פעמים אנחנו קוראים את הסניפט שאלה, ואז לפי הרלוונטיות שלהם, בוחרים להיכנס או לא להיכנס לדף, אבל עצם זה שאנחנו עוצרים לקרוא, זה גם איזשהו מדד שאולי הדף הזה היה יותר רלוונטי מהאחר, אבל אם לא דילגנו על זה ככה ממש מהר, אז בכל זאת קצת רלוונטי, אוקיי, אז, אוקיי, הנה סיבה, אם זה לא ברור, אני בטוח שכולנו בתור משתמשים של מנועי חיפוש זה ברור לנו, אבל אם זה לא ברור, אם נגיד חיפשנו פעילה כמו Jaguar, בעברית יאגואר, אז למה התכוונו? זה יכול להיות כל מיני דברים, הנה יש לנו כאן שלוש Senses, שלוש אינטנטים של מה המשתמש התכוון, הוא התכוון לחייה, למכונית, או למערכת הפעלה, מה זה OS, מערכת הפעלה, מה זה מערכת של Jaguar, לא יודע, אולי זה, טוב, לא יודע, אני לא אנחש, אוקיי, עכשיו, לפי מה שהמשתמש בחר, זה רלוונטי עבורו, כמובן שזה דוגמה מאוד קיצונית, וכאן יש לנו שלושה משמעויות, שבאמת אין שיתוך ביניהם, זה Mac OS, אמרתי בטח מובייל, אבל כן, זה גרסה של Mac OS, טוב, זה אומר עליי שאני לא יודע מה זה, שאני, בסדר, אוקיי, אז זה ככה, הקדמה, ששמה לנו כבר אתגרים קצת, ב-Web Search, אוקיי, אז בואו ניכנס קצת יותר, ל-Web Search, וברגע נראה גם כמה רכיבים שיש ב-Web Search, ואין לנו ב, לא הזכרנו את זה עדיין, נכון? אני יודע שלא דילגתי על זה, כי זה כן חשוב, אוקיי, אז אם זה לא יופיע במפורש, אז נציין את זה אחרי זה, אז בעצם אנחנו, בניגוד, לארגון מסוים, שיש לו Database, או קבצים, שאנחנו רוצים להינדקס, או, במחשב האישי, שהכול ברור, מה יש לנו ומה אין לנו, רק צריך להפנות, נגיד, לאיזה דירקטורי ראשי, ורוצים שהספוק הכול משם בצורה עירכית, אז כאן, אנחנו בעצם צריכים להגיע לכל מיני מקומות, קודם כל, מה זה האישיות האלה שאנחנו בעצם רוצים להינדקס? המסמכים האלו, במקום שם של מסמך, אנחנו מדברים כבר על כתובת URL, אז נתייחס רגע קצת לדבר הזה, ואני מניח שרובכם מכירים את זה, אבל נעשה פה ככה כמה אישי רואי קו, אז בהתחלה, בוא נגיד, בתכנון של האינטרנט, ואני באמת לא עושה פה קורס רשתות תקשורת, אבל אנחנו רואים שיש פה איזושהי כתובת, ומניח שרובכם מכירים מה זה אומר IP-אדרס, אבל זה מחיל ארבעה חלקים, אני לא אדבר פה על מסקינג ועל דברים כאלו, אוקיי? 1, 2, 3, 4, כל אחד מהם יכול להיות בין 0 ל-255, וזה בעיקרון לצורך העניין המקבילה של איזושהי כתובת עם ID מדויק לצורך העניין, למרות שכולנו יודעים שיש לנו IP משתנה וכולי, נשים את זה רגע בצד, נגיד שאין IP משתנה, אז כאילו IP זה משהו מדויק שהוא מזהה מחשב מסוים, אוקיי? אבל כרגיל לנו בני אדם מאוד לא נוח לעבוד בצורה הזאת.

אז מה נעשה במקום זה? אנחנו ניתן שמות ובדיוק, כמו שלמדתם בשנה הראשונה בטח בקורס תכנות, ודיברו איתכם על רצה לכם ללמוד שפצי במקרה? אוקיי, אז אתם עוד יותר טוב, אז אתם מכירים שבטח כשלמדתם משתנים, אומרים לנו שמבחינת המחשב בעצם הוא לא עובד עם שמות משתנים, זה רק בשבילנו, הטביעות שאנחנו שמים על הארגזים לצורך העניין, אבל בעצם מדובר על מקומות בזיכרון, שזה אם נרצה, מי שלא זוכר, כמו איזה מדח אחד ארוך הרם, שיש לנו פשוט מספר של כתובת, אבל לנו לא נוח פשוט להשתמש במספרים האלה, אז אנחנו שמים שמות של משתנים שמייצגים את כתובת מסוימת בזיכרון. אוקיי, אז באותו אופן במקום להשתמש ב-IP אדרס אנחנו משתמשים ב-domain name, ואז כל הדבר הזה הוא כבר עולם שעכשיו אנחנו נפרד. אז יש איזשהו רכיב בין כל הרשת של האינטרנט שאחריות שלו זה בעצם למפות בין ה-domain name לבין ה-IP אדרס. 

אוקיי, זה הרכיב הזה נקרא DNS, domain name system, שהוא מקבל domain name והוא מתרגם את זה ל-IP כי בסוף עובדים למטה ב-IP כמו שלא עובדים משתמשים. אם משתנים עובדים כתובות בזיכרון בעצם מתחת למכסה המנוע, מה שנקרא. אוקיי, אז בואו נסתכל רגע על מבנה של URL עכשיו.

מסתכלים על הכתובת הזאת שאנחנו גולשים באיזשהו מקום מסוים אז מה זה הדבר הזה שאנחנו רואים מול העיניים? בואו נפרק את זה לחלקים. שוב, מניח שחלקכם בטח למדתם את זה בקורס רשתות תקשורת או משהו או בכלל יודעים. אז החלק הראשון עד ה-// זה מה שנקרא הפרוטוקול.

אם יצא לכם במקרה אני לא יודע מה עוד להוריד קבצים בפורמט שאני לא יודע כמה משתמשים בו היום של FTP אוקיי, של file transfer protocol או בהתחברות לאיזה שרת מרחוק אולי באיזה SSH או Telnet או משהו כזה אם יצא לכם לעבוד מול שרתים של אמזון נגיד או משהו כזה אולי אז זה פרוטוקול אחר זה לא פרוטוקול של HTTPS HTTP זה פרוטוקול של בעצם אנחנו מבקשים להעביר דברים שאולי עם Hypertext כלומר יוצגו בבראוזר כולם יודעים מה זה בראוזר? דפדפן כמו Chrome כמו Firefox, Edge מה? כאילו אני עדיין שומע אנשים שקוראים לזה או Google או אינטרנט אבל בסדר אני מוודא אז אם פגעתי בכם אז אני מתנצל, זה לא הכוונה אוקיי, אז בעצם הבראוזר מציג לנו תוכן HTML, עוד רגע אני אדבר על זה בקצרה למי שלא מכיר ואז יש לנו את ה-domain name שזה מחולק בעצם לטופ לבל דומי יש לנו כאן את ה-google.com אגב במקומות שהם לא ארצות הברית אז הסוף כאן במקום נקודה קום זה יהיה כמו בארץ נגיד COIL ואני בכלל לא מדבר על מה שנקרא GTLD כן, הטופ לבל דומי נקודה קום זה רק מילה אחת בארצות הברית, זה הדברים הישנים EDU, גוב קום, אז בארץ זה עם סיומת המדינה גם ברוב העולם זה ככה כי זה מה שקורה כשמי שהתחיל את האינטרנט זה היה ארצות הברית אז הם קיבלו כן, אז הטופ לבל דומי ברוב העולם זה דווקא שני מילים אבל עכשיו פתחו מי שמכיר את הנושא של ה-URLים בשפות שונות גם אז ה-GTLD בעצם פתח לנו עולם שלם שעכשיו הרבה סטארטאפים יש להם נקודה AI בסוף נקודה IO וכל מיני סיומות כאלו שהיו הרבה יותר מצומצמים פעם, זה נקרא GTLD, אוקיי אז המילה האחרונה או שתי מילים האחרונים זה ה-TLD או ה-GTLD והדומיין או הטופ לבל דומיין כאן שלנו זה ה-google.com זה בעצם הדומיין של גוגל וכל מה שאנחנו נראה משמאל בנקודה זה מה שנקרא סאב דומיין אוקיי אם יש לנו גוגל מפס אז זה יהיה מפס.google.com אם יהיה לנו למשל גוגל תמונות אז זה יהיה אימג' או אימג'ז.google.com אוקיי ככה זה אם יצא לכם במקרה ללמוד ג'ווה אז אתם מכירים בעצם את הצורה ההיררכית של פקקג'ס זה אותו דבר ככל שזה יותר ספציפי אנחנו מוסיפים מצד שמאל את הרמה היותר ספציפית ו כאן יש לנו מה שנקרא פורט נאמבר הוא יכול בסוף להזין לאיזשהו פורט אוקיי ו בפנים אחרי הסלש הזה של כאן נגיד הפורט נאמבר בדרך כלל יהיה דפונט ואנחנו לא נראה אותו בכלל רק שתכירו שיש דבר כזה אז אחרי הסלש כאן בעצם אנחנו מדברים על איזשהו מחשב לצורך העניין שרת שהוא ממופה לאיזשהו ספרייה באותו שרת כלומר את ההארדיסק שלו אוקיי אז כאן אנחנו קוראים לזה search ואז כל מה שיהיה אחרי הסרץ הזה סלש עוד הכוונה היא שמדובר בסד דירקטורי פשוט באותו מחשב אוקיי אז בעצם מחליטים לחשוף שאיזשהו מקום בארדיסק איזשהו path יהיה נגיש להזנה באינטרנט אוקיי עם הרשאות מסוימות שהחליטו מראש אז זה מה שאנחנו רואים הסרץ בעצם זה ספרייה אוקיי כלומר מן הסתם לא נותנים לערוץ דירקטורי הרשאות מחליטים מאיזשהו ספרייה מסוימת שהיא נגיד איפה שהסלש הזה נגיד זה אני לא יודע מה web הם קראו לזה נגיד בגוגל אנחנו לא רואים את זה, זה ערוץ לצורך ההזנה של השרת ותחתיו תחת הweb הזה יש ספריית סרט אוקיי אז זה בשרת של גוגל נגיד מפודקא עוד חלק ב URL שאנחנו רואים הרבה פעמים יש לנו פה את הסימן שאלה הסימן שאלה זה כבר לא חלק מהשם של הספרייה זה אומר שמדובר פה בURL מסוג שאילתה אוקיי השאילתה מורכבת מחלקים של key וvalue הkey במקרה הזה זה q והvalue במקרה הזה זה paris הkey והvalue מחוברים עם שווה אז q שווה לפרס זה מן הסתם תמצא לי בגוגל maps את פרס, זה מה שרוצים כאן אוקיי ואם היה לנו עוד פרמטרים היינו רואים שזה משורשר עם ampersand אמפרסנד ממעריך שאתם מכירים אוקיי השרת שבעצם מאזין הוא מקבל את השאילתה והתפקיד שלו הוא לעשות לה parsing לדעת מה עושים עם השאילתה בסדר הדבר הזה זה בעצם אפשר גם למצוא את זה אפילו במודל לפעמים תלך לי לאיזשהו אזור עם תגית מסוימת בתוך הדף אוקיי אם אתם מכירים לחמים נגיד בדף של מודל יש כותרות כאלו אז אפשר שב לגשת נגיד לאזור של חלק 2 והוא אמור לדעת לזרוק אותנו ישר לשם אוקיי יש לו איזשהו תגית אם עשו את זה כמו שצריך וזה לדעתי בהדרס אפשר לזרוק אותנו ישר להדרס אם עשו למי את הזדקן אני לא זוכר אם צריך ID בשביל זה או לא אבל אפשר לבדוק אז זה הרעיון של הURL זה המבנה של הURL וכמובן שבתוך הדבר הזה יכול להיות שיושב ממש קובץ ויכול להיות שלא בכל מקרה אנחנו כבר ניגע בזה כשאנחנו בעצם רוצים לעשות את השלב של האינדוקס באינטרנט אנחנו בעצם עובדים עם איזשהו באופן בסיסי ה-HTML שעוד רגע נדבר על קצת מה זה זה מה שהבראוזור מציג ואם נרצה יש להם בלטין או אדונס כאלה של אחד ממחאי הליצוב זה ה-CSS איך יראו אובייקטים שלנו אפשר לעשות דברים מאוד יפים לפעמים זה נמצא בתוך הדף ולפעמים יש לנו בהתחלת ה-HTML ככה קישור ל-CSS חיצוני אז מי שיתעסק נגיד בפרונטה נכיר את ה-CSS טוב אוקיי זה יש הרבה תוכנות שיש להם אינטראקציה להוריד אותי מהדברים אלו אז כאן נותנים לנו דוגמה למשל איך נראה ה-CSS אז הקלאס הזה של פרייס אוקיי אנחנו רוצים שזה כל מה שסימנו אותו כקלאס שווה פרייס יהיה לו צבע ירוק זה מה שהדבר הזה אומר אוקיי וכמובן עושים בדרך כלל דברים הרבה יותר מורכבים מהדבר הזה למה ה-CSS סופר חשוב לנו כשאנחנו מדברים על מידע מהאינטרנט בגלל שיש אינטרס למי שמעצב את האתר לעשות קוד שהוא יכול לתחזק ושיהיה ברור אוקיי אז הוא נותן פה מיני שמות של רכיבים שונים בדף האינטרנט ואז אם אנחנו מעוניינים במקרה להוריד מידע לרכיב עם קלאס מסוים שבנו את הקלאס בגלל עיצוב מסוים אז פה אני רוצה לעשות סקרייפינג לקלאס שווה ככה וככה אוקיי אז זה עוד רגע נדבר על הסקרייפינג אוקיי הרכיב השלישי על html קצת דילגנו ואנחנו נותן לו פוקוס עוד רגע זה להכניס סקריפטינג, client-side בקוד והשפה הכי פופולרית והנגזרות שלה זה javascript שוב יכול להיות מובנת בקוד או כקובץ חיצוני אז כאן אנחנו רואים שיש לנו איזה כפתור פנימי שברגע שלוחצים עליו הוא מריץ איזשהו קוד, אוקיי unclick יש להם events unclick, unover or unup, undown כל מיני כאלה קובייקטים events שיקראו לאיזשהו פונקציה ברגע שהם מופעלים אוקיי אז javascript זה משהו שהוא גם מובנה מאוד חזק ואם אז בעצם כל הדבר הזה שם אותנו בצורה מאוד מאוד שונה ביחס למערכת היער על המחשב מערכת היער על המחשב בעצם היא כוללת את השלבים שאנחנו מכירים שלב של pre-processing אוקיי שבו אנחנו מפרקים את הטקסט למילים שאנחנו עושים עליהם כל מיני נורמליזציות אולי stemming למטיזציה ואנחנו עושים reverse indexים אוקיי זה יכול להיות זה שאני אומר מחשב אישי שוב זה לא דווקא מחשב אישי אבל לא באינטרנט אוקיי וכאן יש לנו בעצם שלב שלם שהוא שלב מאוד מהותי של discovery אוקיי אז מעבר להבדל בהתחלה שאמרנו שבאינטרנט בדרך כלל הבעיה היותר של precision מאשר של recall כלומר לא בעיה למצוא את המידע אלא בעיה למצוא איזה מידע הוא אלוונטי מתוך הערים של המידע שיש ולעומת המחשב האישי או הארגוני שבו אנחנו רק רוצים למצוא את המידע בדרך כלל דגש של recall אז כאן אנחנו מדברים על שלב שלם של discovery בכלל למצוא את המידע והוא כולל בעצם אפשר להגיד שני רכיבים משולבים שזה scraping וcall אוקיי או לפעמים זה נקרא spidering או שמות כאלו מקבילים והם עצמם לוקחים חלק מרכזי גם פה בהרצאה אז הנה כאן זה נקרא spiderloop אגב תודה לג'ימיני שאחרי התכתבות אינסופית עזר לי להכין את המצגת הזאת אוקיי יש לנו איזשהו URL אנחנו עוד מעט נדבר על מה זה robots.txt אוקיי מי שהתעסק בסקרייפינג אולי מכיר את זה קצת זה בעצם הפוליסי של מה אנחנו מרשים לחשוף ומה לא כן אז לצורך העניין אם נתעלם רגע משחקנים ענקים כמו גוגל אז נגיד שיוצר תוכן מסוים רוצה שאנחנו משתמשים נראה את התוכן הזה אבל הוא לא רוצה ש ניקח את כל התוכן בשביל לעקוף אותו אולי או הוא לא רוצה אולי שזה ייתנדק הוא רוצה שיעבור דרך האפליקציה הואבית שלו בשביל להגיע לצורך העניין לתוכן שהוא יצר נדבר על זה עוד פעם עוד מעט אנחנו רוצים בעצם להוריד את הHTML שעוד חייבים עוד להסביר מה זה קצת והרבה פעמים לחלק את הHTML הזה לחלקים הרלוונטיים לחלץ מה שאנחנו רוצים לעשות פרסינג על זה ולחלץ את המידע ודרך הHTML עצמו כולנו גלשנו באינטרנט יש לנו את הכישורים לעוד אפים נוספים באינטרנט אז ממנו אנחנו מגיעים לעוד תוצרים שונים ועכשיו פה הרבה פעמים יש לנו מידע שהוא או לא רלוונטי צריך לדעת אם המידע הזה הוא חשוב או לא חשוב או ממש ספם לכלמים או שהוא מידע שהוא חוזר על עצמו הגענו ללינק הזה ממקומות אחרים למשל אוקיי אולי לא נקרא בדיוק ככה כי היה פה איזושהי הפניה לדף הזה מדובר בעצם באותו תוכן בדיוק אז אנחנו רוצים לגלות את זה ובעצם להינדקס רק מידע שהוא מידע חדש ולא מידע שכבר ראינו אז ומפה לעבור לעוד לינקים חדשים ולהמשיך את התהליך הזה ואז לעשות את התהליך של האינדוקס עם כל השלבים שאנחנו מכירים גם בדומה למה שלמדנו אוקיי אז זה שלבים שונים שיש לנו עוד לפני שאנחנו מגיעים בכלל לפריפרוססינג ולאינדוקס זה נראה דווקא עצה פחות מוצלח קצת איך אנחנו בBFS כולם? זוכרים או צריכים לזה לתזכור תנתנה? זוכרים כולם? כן אז בואו אני אעשה לכם באמת דקה תזכורת אנחנו רוצים לדעת איך אנחנו סורטים אני כבר הסביר מה הקשר לענייננו אז שאלה טובה אני כבר אזכיר את הקשר לענייננו אוקיי אז אני שותף את זה לצורך ההשוותה אוקיי אז תניחו שה-S הזה כאן זה איזה שהוא דף אינטרנט שאנחנו נדבר אחרי זה על איך מוצאים אותו אבל הוא דף נגיד של המחלקה למערכות מידע דף פרשי נגיד באוניברסיטת חיפה שאנחנו יודעים שיש בו הרבה כישורים והוא יביא אותנו לכל מיני דפים אולי של קורסים ושל מרצים עם התחומי המחקר שלהם אולי וכל מיני דברים כאלה אז זה לצורך העניין מה שנקרא סיד זה בעצם נקודת ההתחלה שלנו וכמו ש יש לנו לידים לבעיה אם אתם מכירים את המילה הזאת ואז אנחנו מוצאים ככה נקודות ההתחלה ככה אז כאן בהתחלת החיפוש בגרף של האינטרנט יש לנו את הסידים האלו כמו נגיד אותו דף של מערכות מידע וממנו כל דף יש לנו כל מיני כישורים שהם בעצמם מפנים לעוד כל מיני כישורים וכן הלאה אז זה בעצם הסריקה שלנו עכשיו הרעיון של bfs זה לא להיתקע אם אנחנו נלך אחרי לינק שהיה בדף הראשי נגיד של מערכות מידע אז נגיד שאנחנו נעשה דבר כזה נתחיל פה דווקא כחול יש לי כאן נתחיל אז נתחיל בכישור הזה ועכשיו במקום לבדוק כאן עוד כישורים אז אנחנו נלך לאחד מהכישורים שהיה בדף הזה ואז ממנו נלך לאחד מהכישורים שהיה בדף הזה ונגיע נהייתקע אולי באיזה כישור איזוטרי במקום שיהיה לנו ככה כישורים באותה חשיבות ברמה המקבילה זה הרעיון של למה דווקא להשתמש בbfs ולא נגיד בdfs אז זה מה שאנחנו עושים כאן אז נגיד מהדף הראשי אנחנו נסרוק את כל הכישורים שמופיעים בדף הראשי ומהם נסרוק עוד רמה אחת לעומק עד איזשהו מקום שנחליט שזה מספיק נכון אוקיי אז זה הקשר של הגרפים לענייננו זה בגלל שבעצם האינטרנט זה גרף אחד גדול זה מבנה נתונים שאולי הוא קצת יותר מסובך מ השוואה לדאטאבייס ארגוני או לכמה קבצים שנמצאים בצורת איררכיה פשוטה אז כאן בעצם אנחנו צריכים בשלב הדיסקאברי לעקוב אחרי גרף כזה וגם אנחנו יכולים להניח שהגרף הזה הוא לא קשיר זוכרים את המושגים האלו? גרף קשיר, קשיר חזק קודם נדבר על זה קצת אז אני אזכיר אם אני חזור רגע לכאן אז אם למשל לא היה לנו את הצלע הזה הצלע הזאת כאן איפה שאני שמתי פס אדום נמחוק את הצלע הזאת אוקיי אז בעצם אי אפשר להגיע מ-S ל-R ול-R מחובר ו-V אוקיי, אז כל אחד מהדברים האלו כאן הם אפשר להגיע מכל דף כל דף זה קודקוד לצורך היום בגרף אפשר להגיע, והכישורים זה הקשרתות אפשר להגיע מכל גרף לכל מכל קודקוד לכל הקודקודים האחרים כאן כן, אגב זה זה לא נכון לגמרי הדימוי הזה כאן עוד רגע אני אסביר למה אני מתכוון אלא זה יותר נכון להציג את זה כגרף מכוון לא כגרף לא מכוון נזכיר עוד רגע מה הכוונה אוקיי, אז אי אפשר להגיע מ-S ל-R, אז בעצם מה שאנחנו צריכים זה שיהיה לנו הרבה סידים, הרבה סידים טובים, לא רק סיד אחד הרבה דפים שטוב להתחיל מהם את השריקה של האינטרנט הם אולי מחוברים באיזשהו מקום ובצורה חלשה ואולי לא אוקיי, אז למה זה גרף מכוון? כי בעצם בעמוד ש-S מייצג נגיד שוב דף של מערכות מידע הדף הראשי יש כמה כישורים כלומר הוא מופנה לאחרים אבל הם לא בהכרח מפנים בחזרה אליו אוקיי, כלומר יש משמעות לחץ לפעמים זה חץ דו-כיווני, לפעמים הוא חץ כיווני והכוונה היא שאפשר להגיע מידף מסוים לדף אחר אבל לא בדווקא ההפך אוקיי, אז זה בעצם מדובר פה בגרף מכוון שיש בו מעגלים לא דאגים אם אתם זוכרים מה זה מדובר פה בים אז זה גרף עם מעגלים כלומר סיקלי אני לא יכול להגיד סיקלי עם מעגלים אבל מכוון אז זה אתה חושב נגיד על אתר כמו שוב איזשהו חוג באוניברסיטת חיפה לדוגמה אבל כמו שאנחנו יודעים רוב האינטרנט הוא בכלל לא נגיש בלי דומי ניימס בכלל רוב האינטרנט אוהבים לקרוא לזה שם שהוא שיידי כאילו הכל שם זה מחירה של דברים לא חוקיים הדארקנט אבל הכוונה היא למה הוא דארק כי אי אפשר להגיע אליו דרך האינטרנט עם הדומי ניימס ועם הכישורים וכל הדברים האלו כי הוא בעצם הם לא יצטרפו למשחק הדרך היחידה להגיע אליהם זה דרך מערכות אחרות אז רוב האינטרנט הוא בכלל לא זמין אני לא יודע להגיד אחוזים אבל וגם מה שכן אז הוא מחובר בצורה מאוד גם אם אתה נגיד מסתכל על מעגלים צריך לעבור אולי הרבה כישורים בשביל לחזור בחזרה הרבה פעמים אז לא לכל דף אפשר להגיע ומכל דף יש דבר כזה שנקרא ככה סידים טובים יותר וסידים טובים פחות להתחיל את הסריקה שלנו זה הרעיון אז בואו נסתכל רגע על כמה דוגמאות בואו נראה מה יש לנו כאן אנחנו עושים איזשהו קרולינג כזה ו אוקיי אז אנחנו רואים פה כל מיני סריקה הדוגמה הזאת היא לא מספיק כדאי פשוט להראות לינק קליקים על כישורים ולהגיד שאנחנו מהנדקסים את הדפים האלה הדוגמה הזאת היא גנרית קצת מדי אבל בכל מקרה ביקשתי ממנו לעשות כאילו קוד מאוד מאוד כללי של איך היה נראה אוקיי זה לא באמת נראה ככה בדיוק אבל לצורך העניין יש לנו דבר כזה יש לנו סט של URLים שאנחנו עוברים אליהם נכניסים את זה לאיזשהו Q כדי שזה יהיה BFS ואנחנו לכל דף עושים אולי פרסינג בודקים שאנחנו לא שרקנו יותר מהדפים שאנחנו רוצים מוציאים דף לשריקה מקבלים אותו ועושים לו פרסינג מכירים ביוטיפל סופ? יצא לכם ללמוד קצת סקרייפינג? אוקיי, אז יש מישהו שלא מכיר זה בכלל? לא מכירים בכלל? אוקיי, אז ביוטיפל סופ נועד במקור לXMLים HTML הוא יבנה הוא לא לגמרי תקין אבל הוא יבנה כסוג של XML מי שמכיר? אוקיי, אז בעצם אני חושב שעוד רגע יהיה לנו דוגמה של HTML, אז בעצם הוא יודע לקרוא בצורה בוא נגיד של עץ את המידע וזה מאוד נוח לחפש דרכו אינפורמציה אפשר לחפש בכל מיני דברים כמו של קלאסים ואת הגיות והיררכיות ואת מספר נגיד בליסט אז המספר הרכיב החמישי בליסט וכל מיני דברים כאלו מאוד נוח להשתמש בביוטיפל סופ יש גם כמובן מיליון כלים אחרים אז כאן אנחנו ממשיכים מוציאים את הלינקים וממשיכים לעשות קרולינג, אז כאן פחות מתייחסים לסקרייפינג אלא יותר לקרולינג מי שמכיר HTML יש לנו תגיד בשם A שאם היא מכילה תכונה בשם Hyper Reference Href אז יהיה לה כתובת מאחוריה שאפשר בלחיצה לקפוץ לדף אחר באינטרנט, זה מה שהבראוזור תפקיד של הבראוזור להביא אותנו אז דרך אותו דף אנחנו בעצם עוברים על כל התגיות מהסוג הזה ומוסיפים את כל הדפים ואז ממשיכים לעשות קרולינג זה האלגוריתם של הקרולינג אז בעצם על הגילוי של האתרים השונים דרך הלינקים המעבר על דפים מה-CDים שיש לנו אנחנו עוברים דרך הקרולינג ואז חילוץ של מידעים מעניינים בשביל זה אנחנו עושים סקרייפינג זה בעצם שלב של אקסטרקשן וכאן אפשר לחלק את המידעים שלנו ל סטטים ודינמיים אפשר להגיד שהמידע שלנו בעצם אמרנו שהוא מושפע משלושה חלקים html שכבר מעבור עליו בעצם זה נותן את ההוראות לבראוזור איך להציג את הנתונים שלנו על הדף יש לנו גם CSS שזה נותן הוראות עיצוביות וJavascript שזה נותן כל מיני סקריפטינג בהינתן שעשינו כך וכך נראה ועוד דברים נוספים שלא הצגנו אז אפשר לחזור לכתוב את זה רגע כאן כדי שזה לא יהיה בעל פה רק אז אמרנו דיברנו על html CSS שזה לצורך עיצוב ויש לנו Javascript זה הסקריפטים שלנו סקריפטים בצד הלקוח קליינסייד ולכל הדבר הזה נוסיף בוא נגיד נקרא לזה הרבה לצורך העניין הרבה אתרים באינטרנט מחוברים לאיזשהו שרת בצד השני נגיד שאם אנחנו דוגמה הכי טריוויאלית לזה זה שאם אנחנו פונים לשירות של חיפוש אז כמובן שהוא מחובר לשרתים של מי שעושה לנו את החיפוש האלה אם אנחנו רוצים שאילת את api שתבדוק לנו מה מזהג האוויר אז באתר מסוים שזה מה שהוא תומך בו אז זה מחובר לשרת שמביא לנו את האינפורמציה הזאת אוקיי אז לצורך העניין המידע יכול להיות שהוא נמצא בצורה סטטית בדף נגיד שכך היו עובדים פעם כשאני אומר סטטית אני מתכוון שלושת החלקים אוקיי רק בצד לקוח נגיד שפעם ביום פעם בשבוע מעדכנים את הדף html לדעתי עדיין אפשר לראות ככה דפים מסוימים בהריות זה הטוב בהרים שונות בארץ ואולי גם במסעות ממשלתיים מסוימים שחלק מהמידע לציבור לא נדבר על מידע אישי אז הוא מפורסם בצורה סטטית משנים ממש את הקובץ פעם בכמה זמן אבל יש קובץ שמכיל את כל המידע הזה זה מידע סטטי ומידע סטטי הכלים היחידים שיש לנו לעשות איתם זה לעשות איזה בקשת בקשת גט מסוג http או https יש לנו בפייטון חבילה שנקראת request מבקשים ממנה תני לנו את הדף html והיא פשוט מורידה אלינו את הדף html ואז אנחנו יכולים למצוא בו אם יש בו תוכן מעניין כישורים וקונטנט אחר שמעניין אותה אז זה אחרי שהורדנו את הדף אלינו למחשב בתור טקסט עכשיו אפשר לעבור עליו בעזרת beautiful source לעומת זאת רוב האתרים היום כנראה אני לא יודע להגיד אחוזים מדויקים אבל אני די בטוח שזה כבר הפך לרוב בעצם מיוצרים בצורה דינמית זה אומר שברגע שאנחנו ניגשים לדף מסוים עושים אולי scroll down נפצים על איזשהו כפתור עושים איזושהי פעולה אז בעצם הנהידה נשלח לשרת והשרת על המקום מייצר לנו איזושהי תשובה גם את הדבר הזה אפשר לעשות לו scraping אבל זה כבר דורש הרבה יותר מאמץ ולצורך העניין כלים שמראש פיתחו אותם בשביל לעשות QA לצורך העניין אנחנו חברה, נגיד שאנחנו ביחד כולנו חברה שיש לה איזשהו אתר מסוים אז אנחנו רוצים לבדוק את עצמנו איך האתר נראה לבדוק כל מיני סנריוז, לראות שזה עובד כמו שצריך הזמנים, התצוגה והכל, וחלק מהדברים עושים מדענית אבל רוצים לעשות כל מיני בדיקות כמו בדיקות עומסים אז אנחנו רוצים משהו אוטומטי שיעשה את זה, אז בשביל זה המציאו כלים כמו למשל אולי המפורסם שבהם אבל יש עוד הרבה כלים אחרים זה שלניום, שהוא בעצם מדמה משתמש נגיד תלחץ על הכפתור הזה, תחכה חמש שניות ואז תזוז לשם ותלחץ על זה וכל מיני עושה scroll down ובסוף בסוף מה שקורה בדפים השונים זה שמוצגים בסוף דפי html אולי css וjavascript השלשה הזאת ואז אחרי שיצרנו אותם בצורה דינמית כן לעשות scraping לדבר הזה ואז נהתח את המידע עם Beautiful Circle אז נגיד בסוגריים שאנחנו נכניס לכל הדברים האלו תרגילים זה לא מאוד קשה אז אני אפרץ בקרוב אז זה בעצם השלבים של הcrawling ושל הסקרייפינג בגדול עדיין אז כן הcrawling זה השלב של הגילוי של הדפי html שאנחנו רוצים אחרי זה להנדקס אז איך מגלים אותם? אנחנו עוברים בוא נעשה איזה סימולציה של זה פשוט אולי במקום נראה בסך זה בפנים מה הקידומת של מערכות מידע אם זוכרים במקרה עכשיו שאני לא צריך אוקיי, אז יש כאן כל מיני דברים שהם כנראה כישורים הנה נגיד סטודנטים אוקיי, אנחנו רוצים לדעת אנחנו רואים את זה בעיניים ורוצים רואים את זה, כן? רוצים לדעת מה בעצם אנחנו רואים מבחינת html אז על הדרך ככה גם נראה html אז אני עושה קליק ימני, רוב הבראוזורים תומכים באינספקט הזה לוחץ פה על אינספקט ואז אנחנו ממש רואים את ההיררכיית html שיש לנו בשביל הדבר ואם אתם רואים חלק מההיררכיה הזאת אנחנו רואים דברים כאלה שכאילו זה דברים שנוצרו והם לא היו שם בכך קודם הם נוצרו בעזרת איזשהו סקריפט אוקיי, השווה שווה דולר אפס הזה מרמז לנו זה לא חלק מהhtml עצמו אז זה דברים שאנחנו נרצה קליק מוסלמים בשביל להוריד בשבילנו אם אתם זוכרים שדיברנו קודם אתם רואים שהסטודנטים מיוצג משהו מיוצג כי גדרנו בתוך הcss איזשהו מחלקה בשם Eliminator Icon List Text אוקיי, שיש לה ייצוג מסוים אז את המילה סטודנטים אוכל למצוא שם ואז צריך לראות איפה עוד בדף html יש לנו את הדברים האלו ולחפש קלאסט כזה בדיוק ואז אנחנו יכולים למצוא את המילה סטודנטים אבל ברמה עקרונית כל הדבר הזה אנחנו רואים איפה נמצא הלינק הוא נמצא במחק מעל הספן הזה או במקביל אליו לא, זה מוצג בתור הטקסט של הלינק למי שלא מכיר html יש לנו תגית A ככה נראים תגיות יש לנו נקרא לזה לצורך העניין סוגריים ושולשים זה נראה לכל מי שלמד מתמטיקה כמו קטן וגדול אבל זה בעצם מסמן כאילו סוגריים שמאליים את שם התגית ואז אחרי הרווח אפשר להתחיל לעשות פרמטרים של שם הפרמטר ושווה value אוקיי, ובסוף סוגרים את התגית ואז לרוב התגי התגיות, כאן זה תגית A יש גם תגית סוגרת אז כאן אם התגית הסוגרת של A נקרא לזה סוגריים ושולשים הפעם, סליחה, Slash A אוקיי, ל-A A זה התגית הפותחת אז Slash A זה יהיה הסוגרת L זה רשימה אז LI זה רכיב ברשימה UL זה הרשימה LI, רכיב ברשימה אז Slash LI זה התגית הסוגרת שלה אותו דבר, כאן Span ו- Slash Span אוקיי, אז בתוך הרכיב של ה-Hypertext יש לנו לפעמים תמונה לפעמים טקסט שאנחנו נרצה להציג שבלחיצה עליו אז אנחנו נגיע למקום אחר זה לא בצורך העניין אז כאן יש לנו את הכתובת שאליה אנחנו נלך אוקיי, אפשר עוד רגע לקפוץ לשם וכאן יש לנו את הטקסט שצריך ללחוץ כאילו בשביל להציג אותו אז בואו נניח שכרגע אנחנו מצאנו בצורה הזאת את כל התגיות של Ahrefs שהן מופיעות פה בתוך ה-HTML אוקיי, הכנסנו את כולם לתוך ה-BFS לפיקוס אוקיי, ואז כאן יש לנו נגיד עוד כישורים מוספים אוקיי, אז יש לנו פה רשימה של כל הכישורים שיש פה בדף לצורך העניין אז זה שלב אחד בקרולינג עכשיו יש לנו בדף נסיים את ה... לינקים, ומהם חלק מהזחילה בקרולינג אנחנו מגיעים עכשיו לדף נגיד הראשון שהכנסנו לטור נגיד שזה הדף של הסטודנטים אז לצורך העניין כאילו אנחנו נכנסים לדף הזה ועוד פעם עושים לו get מוציאים ממנו את כל הלינקים מוסיפים אותם לסוף הטור ועכשיו חוזרים בחזרה לדף שהיינו... כאילו לא חוזרים כי שמרנו את כל הלינקים כבר אוקיי אבל כאן יש פה עוד כל מיני לינקים נגיד... לאתר המתעניינים הולכים לבא בתור אז כל הפעולה הזאת שמתחילים מסיד או מכמה סידים עוברים לינקים ככה אחד אחד מוסיפים אותם וככה בעצם אנחנו יוצרים את הגרף של כל התוכן שלנו זה מגביל לעץ שיש לנו בפייל סיסטם נגיד במחשב ששם כבר יש לך את כל הכנצים במקום ידוע אז כאן אתה צריך לייצר את כל הדבר הזה כדי שנוכל להנדקס ברוון רשת האינטרנט משתנה כל הזמן אתה צריך לדעת לתחזק את זה אבל לצורך העניין אם נגיד שרשת האינטרנט היא ייצור סטטי לא משתנה אז אנחנו מתחילים נגיד מכל הסידים שיש לנו ושומרים את כל האינפורמציה ככה ומהנדקסים אותה לא צריך להוריד אלינו בערך את כל התוכן לשמוע את הכל אלא רק את האינדקסים אז אפשר להכנות לדף עצמו אז זו הצורה שבה אנחנו עושים קרולינג בסקרייפינג אנחנו מחליטים איזה מידע רלוונטי בדף אני אתן דוגמה שהיא די יותר פשוטה אולי אני אלך לכולכם מכירים את החנות של צומץ ספרים פשוט להם יש אתר אפילו סטטי שממש נוח לעשות לו סקרייפינג אז אז זה אתר של צומץ ספרים אז אם עכשיו אני הולך נגיד לסניפים אוקיי רואים שיש לנו כאן ממש רשימה יפה כזאת של כל הסניפים של צומץ ספרים אז נגיד שעכשיו אנחנו רוצים לשמור את המידע אנחנו מטיילים בעבודה שלנו נגיד שאנחנו תומכים באיזה משהו שנמצא בכל מקום לא יודע, כלי חבר שהיה שנים אחריי לוגיסטיקה של איזה שהוא מוצר שאפשר למצוא בכל סופר מרקט אז הוא טייל בכל הארץ אז אנשים שגם ככה מטיילים אז הם רוצים מידע נגיד על כל מיני דברים בדרך ככה לקנות אולי נגיד שאנחנו רוצים את המידע על צומץ ספרים, הנה אז אמצנו זה סיפור למה זה רלוונטי בשבילנו אז אנחנו רוצים לשמור את כל האינפורמציה, הדוגמה הזאת אולי לא דווקא בשביל להנדקס אותה אבל היא כן מדגישה את הסקרייפינג ואולי קצת קרולי אוקיי אז אנחנו רוצים את המידע על כל הסניפים אבל גם לכל סניף יש לנו מידע נוסף שיש לנו בסניף שגם אותו אנחנו צריכים להיכנס ולהוציא ואולי משם להגיע לעוד כל מיני אטראקציות שיש ליד או וואטאבר, אוקיי אז למרות שזה מבחינת איררכיה דוגמה לא מאוד מעמיקה אבל זה כן כאילו מדגים את הקטע של איזה סוג של מידע אני רוצה, נגיד שאנחנו כאן באנו מאוד ספציפיים רוצים את הכתובת, שעות פתיחה, טלפון אימייל ולא יודע כל המידע שיש לנו על הסניפים אז איך עושים את זה? אנחנו באים לפה נגיד שוב עושים אינספקט וכאן הם עשו לנו ממש עבודה קלה, כל בראנץ' כל בראנץ', אתם רואים יש לנו בראנץ' נאם, בראנץ' אדרס יש קלאסים ממש לכל דבר בגלל זה זה נוח להביא את זה בתור דוגמה הם נתנו ככה ב ועדת שמות שלהם והCSSים עשו את זה מראש שהוא קל לסקרייפינג אז כל המידע ככה הגיוני ואנחנו יכולים למצוא אותו בקלות בעזרת Beautiful Sort בסדר? מה זה קרולינג וסקרייפינג? זה פחות או יותר ברור? סבבה אוקיי, אז אוקיי, כאן הוא מראה לנו אני צריך אולי לסדר ולהתקן פה את המצגת הוא מראה לנו כאן בעצם קוד לדוגמה של סילניום שזה בשביל הדינמיק סקרייפינג אוקיי, מחכים איזשהו זמן מסוים אגב באופן כללי אם עושים קרולינג ולא רוצים שיחסמו אותנו אז כל נגיד DevOps ממוצע יראה שאותו IP שולח עוד בקשה ועוד בקשה ועוד בקשה במלאכי זמן מאוד קטנים הוא יחסם אותו מהר מאוד כשלא רוצים שיחסמו אותכם אתם חייבים לעשות ככה תנאה בקוד עם time.slip של כמה שניות כדי שלא יקרעו אותכם מיד ויחסמו אותכם אוקיי, זה כאילו באמת דבר מינימלי לעשות אז כאן אנחנו רוצים למצוא איזשהו אלמנט ומה אנחנו עושים פה כנראה שלוקח לתוכן זמן להתאם זה כאילו המידע הדינמי הפשוט ביותר הדוגמה הזאת כאן זה לא מידע אמיתי example.com זה לא אתר אמיתי אבל המידע מיוצר דינמית ולוקח לרגע טעינת הדף איזה כמה שניות עד שהוא ניתן אז אנחנו נחכה, רק אחרי שהוא נוצר אנחנו נחפש אותו בדף ונוריד אותו זה הרעיון אז כל הדבר הזה דיברנו ברמה עקרונית על מה זה פעולה של קרולינג ומה זה פעולה של סקריפינג כחלק מהפעולה של קרולינג בעצם אנחנו רוצים לבדוק שאין לנו דופליקציות או שכמעט ואין לנו דופליקציות אוקיי, אז מה אפשר לעשות אפשר להתחיל מהURL זה הכי פשוט יש לנו שתי דפים שמכנים לאותו URL וכבר אינדקסנו את ה-URL הזה כמו שמרנו אותו אז אנחנו נעשה בעלולה שלנו Continue או נבדוק לפני שאנחנו מאנדקסים את התוכן הזה שלא היינו שם ולפני שאנחנו ממשיכים לעשות קרולינג שאנחנו לא חוזרים לאותו מקום בגרף שכבר ראינו אוקיי, אז זו בדיקה דרך ה-URL עצמו כאן אנחנו מציעים גם לעשות איזושהי בדיקה לפי התוכן איך עודקים לפי התוכן אז כאן יש לנו דוגמה טריוויאלית, אנחנו עושים פונקציית מרחק ביניהם מרכירים מרחק ויקטורי זו הצעה נוספת לעשות את זה עם מרחק שנקרא Jaccard Similarity אוקיי, ואם זה עובר שהוא סך אז אנחנו מחשיבים את זה כמידה שהוא כמעט זהה ואז אנחנו לא מאנדקסים את זה אוקיי, וכאן יש לנו הצעה לעשות אותו דבר אבל בצורה קצת פחות מדויקת אבל הרבה יותר מהירה בעזרת שיטה של מין-האש, אוקיי, איזה שהוא אלגוריתם של מין-האש שעושה אותו דבר אבל בצורה הרבה יותר מהירה לא מדויק כמו Jaccard Similarity שנחשב פונקציה מאוד איכותית לדמיון בין טקסטים אז אם אנחנו מדברים על סקלות גבוהות כמו דפים שלמים באינטרנט כשאנחנו רוצים שהוא יעבור על הרבה מהם אז מין-האש זה שיטה טוב אז מה המבנה של האינטרנט ומה אנחנו צריכים לדעת בקשר לזה אז אם נרצה אז אמרנו שהאינטרנט הוא גרף מכוון גרף מכוון זה אומר שיש לנו קשתות נכנסות וקשתות יוצאות זה ה-in וה-out שיש לנו כאן כלומר האם אנחנו בתור דף מצביעים על דף אחר ואז זה קשת יוצאת או שמישהו אחר מצביע אלינו ואז זה קשת נכנסת אתם זוכרים את המושגים האלה? נגיד שיש לנו כאן v1 וכאן v2 אז אם נסתכל עכשיו מבחינת v2 הקשת הזאת היא קשת נכנסת מבחינת v1 זה קשת יוצאת אז v1 וv2 במקרה הזה זה שני דפים באינטרנט זה האישויות שלנו זה הצמתים או הקודקודים אז הגרף הזה הוא בעצם סוג של גרף scc שזה בעצם גרף של רכיבי קשירות, גרף מכוון של רכיבי קשירות כלומר לצורך העניין יש לנו כאן נגיד הרבה קודקודים נגיד שזה הדף של אולי מערכות מידע כאילו זה לא דף אחד נגיד מערכות מידע בחיפה זה הרבה דפים שהם ביחד בתוך רכיבי קשירות מסוים והם מחוברים בצורה או אפילו אולי את כל אוניברסיטת חיפה נשים פה ואז יש לנו עוד כל מיני אתרים כאילו ברמת גרנולריות כזאת או אחרת שהם מחוברים בצורה אני מדבר שם על מושגים שלא קשורים לגרפים אבל בצורה מאוד חזקה עם הרבה לינקים אחד לשני אז הם יהיו הרכיבי קשירות החזקה שלנו ואז יהיה לנו זה גרף על שמחבר ביניהם בכל זאת נגיד מאוניברסיטה אחת לאוניברסיטאות אחרות או דברים כאלו, יש פחות קישורים אבל עדיין הם בשאיפה נצליח לקשר בין הרכיבים זאת אומרת, עדיין יהיה קשיר אבל עם הרבה פחות קישורים אז בעצם אנחנו נצטרך פה כלי שיידע לקרוא את הדבר הזה ושנעשה בעצם קרולינג עם רכיב שיודע לעבוד עם SCC, אז יש אלגוריתמים שיודעים לעבוד עם סוג כזה של גרף SCC ובעצם יש יכולת ל-DFS זה Strongly Connected סייקלק אני חושב? לא Strongly Connected Components זה כאילו אותו רכיב לא, אבל הרבה דפים הרבה דפים שבמקום קוד קוד, כל הקוד קודים שלהם שקשורים אחד לשני שם ניקח יישוט אחת שקולטת את כל הוא קשיר חזק כמו שנקרא, אפשר להגיע מכל דף לכל דף אחר וכל הדבר הזה זה כאילו קוד קוד אבל בעצם מדובר באיזשהו רכיב קשירות חזק כזה שאפשר להגיע מכל מקום לכל מקום Invert זה פשוט סיבוב של הקשתות ה-Invert עצמם זה לצורך העניין נגיד קשת אחת, יכול להיות קשת אחת לתוך הקבוצה או כמה קשתות מתוך הקבוצה או כמה קשתות שיוצאות מהקבוצה זה ה-Invert, אז זה לדעתי עוד רגע איזה שקף פיזואלי לדבר הזה קצת יותר יפה ויש אלגוריתם שמשתמש ב-DFS אולי אני אדביק את זה אחרי זה למצגת בשביל לעבור על אותו גרף SCC אוקיי עכשיו עוד רגע נגיע לזה שבעצם באינטרנט זה לא נכון סתם להגיד שיש לנו גרף רגיל בחיפוש ו-BFS או-DFS אלא יש לנו גרף ממושכל עוד רגע נראה לפי מה נקבע המשקולות בעצם אנחנו אומרים שהישויות שלנו גם הקישורים וגם הדפים לא כולם שוות כמו בחיים עצמם אנחנו מספרים לעצמנו כל מיני סיפורים שכולנו שווים ואנחנו יודעים שנגיד בעלי הכוח והממון יכולים להגיע לטיפול רפואי יותר טוב יש להם קשרים והכל אם כל אחד מאיתנו לוקח הלוואה והוא לא יכול להחזיר אז יקלו לו את הבית אבל אם איזה מישהו שהוא מאוד מבוסס לוקח הלוואה והוא לא יכול להחזיר אותה אז עושים לו הקלות בלוואה מי שמכיר את המושג תספורת אז יש ביש זה לא נגיד שזה אותו דבר בדפים באינטרנט אבל יש לנו כאילו גרף ממושכל ובשביל הדברים האלו יש לנו אלגוריתמים כמו אלגוריתם של דיאקסטרה אם למדתם בגרפים של בעצם מציאת מסתולים בגרפים ממושכלים או של אלגוריתם של פרים יש הרבה אלגוריתמים שעושים את זה בקיצור גם אלגוריתמים יוריסטיים שלא עוברים ממש על הכל כמו איי סטאר גם דברים כאלו יש אז בעצם אנחנו הגיוני שאנחנו נעבור לאינטרנט באלגוריתם שמיועד לגרפים מושכלות זה הרעיון וכשאנחנו רואים מושכלות עוד רגע אנחנו נגיע לדבר הזה אבל יש לנו נעבור נעבור דלג רגע על המפ רידוס ונגיע רגע לאלגוריתם כמו הפייג'ראנק אוקיי אז אלגוריתם כמו הפייג'ראנק יש לנו גם אותו וגם את ה-HEATS שני אלגוריתמים נתחיל דווקא עם הרעיון של ה-HEATS שאולי הוא טיפה יותר פשוט אז יש לנו סיווג לשני קבוצות של URLים יש לנו HEATS ויש לנו ה-HUBS ויש לנו AUTHORITIES ובעצם ה-HUBS כמו ה-HUB שאתם בטח מכירים אולי באינטרנט למי שיש לו ראוטר שמחובר להרבה קווים ככה זה ה-HUB אוקיי אז לצורך העניין זה דף שיש לו רשימה טובה יותר או פחות יש לו רשימה להרבה דפים אחרים והוא התפקיד שלו זה המחלף הגדול הזה שמשנה להרבה כבישים זה התפקיד של ה-HUB ו-AUTHORITIES התפקיד שלו זה לא להפנות להרבה דפים אחרים אלא להכיל מידע איכותי אז אנחנו רוצים לשלב את שני הדברים האלה האלגוריתם של HEATS ישירו תוקף את שני האלמנטים האלה בתור HUBS טובים ו-AUTHORITIES טובים אולי ניכנס לזה עוד טיפה האלגוריתם שגוגל בזמנו עשתה מהפכה איתו אולי אחד השינויים הגדולים ביחס לכל מנועי החיפוש הקודמים היא שם ב-98 לא זוכר, 86 לא יודע זה PageRank הוא אלגוריתם שבעצם שם את גוגל על המפה והוא אומר דבר כזה, יש לנו את ה-TFIDF שאנחנו מכירים, בהתחלה כל הדפים, אינדקסות כל המנועי חיפוש אינדקסות הדפים רק בגלל איך קוראים לזה בגלל התאמה לתוכן בהנחה שיש מודלים טובים יותר ופחות אבל נגיד באגה וורדס עם ה-TFIDF עבד בצורה סבירה מבחינתו, זה הכי טוב שהצליחו להשיג בזמנו אז גוגל אמרו בעצם שיש לנו עוד מדדים אחרים, ואחד מהמדדים האחרים שיש לנו כאן זה כמה בעצם דף מסוים לצורך העניין נגיד הוא פופולרי איך אנחנו מודדים את הדבר, אז יש לנו כמה שיטות למדוד את זה קודם כל בזה שיש מנוע חיפוש שמפנה אותנו לכל מיני דפים זה כבר נהיה מדד מאוד חזק של כמה קליקים עושים לפה, אז כבר הדף עצמו לא רק בחיפוש אלא מבחינת הפופולריות של הדף היא הפופולרית כי נכנסים דרך המנוע החיפוש יש להם גם דרכים למדוד את זה ככה וגם על כמה דפים בעצם שאנחנו עשינו להם כבר סקרייפינג וקרולינג מפנים לאותו דף דף שהרבה דפים מפנים אליו הוא לא דומה לדף שקצת דפים מפנים אליו וקשה להגיע אליו לצורך העניין יש דברים שנגיד אוניברסיטת סטנפורד נקשבת המקום בשבילה ואז כמעט כל אתר אקדמי שנקשב רציני ומלמד את אותו תחום שמלמדים באוניברסיטת סטנפורד או ב-MIT נגיד אז הוא יפנה בהכרח לאוניברסיטת סטנפורד נגיד הקורסה גדול של IR IR Classy נלמד בסטנפורד זה מומחה בשם קריס מנינג שהוא זה שהיה מהראשונים גם בעיבוד שפה וגם בתחום של IR אז כמעט כולם לקחו את הקורס שלו אולי קצת עשו לו לא לקחו את כל החומרים או שעשו עיבוד קצת אבל ממש עשו קורס על בסיס הקורס של IR של מנינג כמו שהיום הרבה לימדו למידת מכונה לפי מי שמכיר אני לא יודע הקורס של אנדרו אינג שנחשב אוטוריטה בתחום הזה כולם לימדו קורס כמו של אנדרו אינג ולמידת מכונה כאילו ברגע שמשהו תופס אז כולם הולכים אז נראה את זה גם איזו ביליות של הדבר זה גם להפניות באינטרס אז זה מה שאנחנו מדברים בעצם כאן בעצם אנחנו נשאל מה ההסתברות שנגיע אם בצורה אקראית כאילו כן אנחנו נוחתים על דף מסריים אז כידוע לכם הפונקציה randomize אני מניח שאתם מכירים אותה אז בעצם היא באה יחד עם הסתברות מסריות בדרך כלל יש לנו שתי אופציות להסתברות הזאת אבל לאו דווקא אפשר גם בעצמנו להגיד שזה הסתברות בדרך כלל יש בכל שפת תכנות לפחות שתי הסתברויות אחת זה הסתברות אחידה כלומר לכל ערך יש הסתברות זהה בהנחה שכל הערכים שונים ואופציה שנייה זה לעשות איזה התפלגות נורמלית כמו שמכיר אז זה שתי הערצות הבסיסיות שיש בכל שפת תכנות אבל לצורך העניין אספנו עשינו סוג של היסטוגרמה כזאת כמה נחתו לכאן כמה נחתו כאן ואז אספנו מספיק סאמפל מספיק גדול שמייצג את העולם האמיתי אז אנחנו יודעים מה הסיכוי שבאופן אקראי נחתו על דף מסוים ואז הדבר הזה הוא מדד בשבילנו כמה אנחנו מניחים ש פופולארי אותו עמוד ואנחנו משקלים את הדבר הזה מעבר ל-TFIDF שלו אוקיי אז הנה איזושהי דוגמה יש לנו גרף A שמפנה לסליחה נוד A מפנה לנוד B כלומר A ו-B זה דפים לצורך העניין אז ההסתברות של קליקינג היא 0.85 וההסתברות המשלימה היא 0.15 אז אומרים לנו כאן איך לחשב את ההסתברות הכוללת מכל הדפים שאנחנו נגיע לאותו דף נוסף שדיברנו על עוד שלוש דקות אז אני אזדרז. יש לנו כאן עוד דוגמה נוספת גם לאותורטיז שלנו ולהאב אנחנו רואים שזה משהו די פשוט של פשוט לספור את כמות ההפניות לאותורטיז מההאב ואז ככה האב נחשב איכותי ואותו דבר הפוך זה רלציה שהיא מחוברת אחת לשניה אז שתי מילים על אלגוריתם של סיווג של נאיב בייס היום אנחנו משתמשים הרבה ב-AI ונאיב בייס בעצם היה אחד מהאלגוריתמים הראשונים של עמידת מכונה שנחשב אלגוריתם גנרטיבי. מה זה אומר אלגוריתם גנרטיבי? יש לזה הרבה הגדעות אבל לרמה עקרונית הוא יכול לייצר תוכן. 

אנחנו מדברים פה בעצם על משהו שמאפיין את האובייקטים שלנו ולא רק אומר לנו הדבר הזה הוא נגיד סוסו זברה הוא נמצא קצת יותר פה קצת יותר שם מאיזה צד של הגבול אז כאן אנחנו ממש מאפיינים את ההסתברות להיות חלק כמותה קבוצה אז יש פה את חוק בייס והנחה נאיבית שהאובייקטים שלנו לא קשורים אחד לשני אז באופן מאוד קל עוד אחד מהאבני בניין בקרולינג שלנו זה לדעת אם התוכן שלנו ספם או לא ספם. מה שמופיע לנו כאן זה דוגמה לאיזשהו מודל באגה וורדס כזה פשוט בצורכיה נגיד עם tf-idf ואנחנו בעצם על ידי הסתברויות שלנו למילים השונות יודעים להגיד האם אותו דף הוא תוכן איכותי או שהוא סתם זבל שנגיד למשל רצו לקדם בשיטה של לעשות מרקטינג הוא לא באמת מדבר על התוכן הזה יש ממש שיטה שנקראת seo search engine optimization שאנשי מרקטינג מכניסים נגיד כל מיני מילים בצבע הרקע שלא רואים אותם אבל מנועי החיפוש מתייחסים לזה כטקסטי כל דבר ואז ימצאו אותם. אז כאן אנחנו רוצים להבחין בין שני הדברים האלו אז יש לנו פה דוגמה שבעצם אומרת לנו שבמקרה הספציפי הזה יש לנו את המילה money ואת ההסתברות לזה ובסוף אנחנו מגיעים לזה שהסיכוי הגבוה יותר הוא שמדובר בדף שהוא ספם הוא קיבל ציון גבוה יותר לצורך העניין זה מעין הסתברות כאן מי שמכיר נאית בייס מכיר את למה אני אומר מעין ואפילו בהנחת חוסר התלות זה לא בדיוק הסתברות, נקרא לזה ציון לצורך העניין אוקיי, אז בזאת פחות או יותר אנחנו ממצאים כאן את כל העולם של Web Search, כמובן שאפשר לדבר עליו עוד הרבה מאוד אנחנו בסיכום דיברנו עוד על כמה דברים שאנחנו צריכים להבין בהם קודם כל להבין מה זה העולם של ה-URLים להבין קצת על ה-HTML איך הם נראים איך אנחנו עושים סקרייפינג של תוכן דינמי או סטטי משם אוקיי, בכלל להבין איך כל הרשת נויה, כל התחום הזה של הניתוח של כל הקשרים באינטרנט נקרא Link Analysis אולי לא נתנו לו את הכותרת הזאת אבל דיברנו על אלגוריתמים כמו אלגוריתם של PageRank שבעצם ייתן לנו משקולות שונות לפי הפופולריות לדפים שונים אוקיי, או באלגוריתם של Heats עם Authorities והHubs שלפי זה אנחנו נתייחס לתוכן ואז מעבר להתאמה או כרכיב נוסף מעבר להתאמה של הטקסט והתוכן נותנים גם לרכיבים האלו רכיב משמעותי ברנקינג ובהחזרה של תוצאות מסוימות ועל כל מיני אתגרים שיש לנו במהלך הדרך האתגר של להתייחס למידה אם הוא מורשה או לא מורשה מה שקראנו לו רובוטס TXT לא יודע כמה הראינו את זה אבל זה אותם מידעים, לא משנה כרגע, אותו דומיין יגיד לנו שהוא לא רוצה שנעשה לו סקרייפינג וקרואולינג מדפים מסוימים אז אנחנו נכבד את זה לפי המיקום בקובץ יש לו מיקום תחת לדומיין נגיד google.com slash robots.txt שם זה יפה אז לפני שעושים סקרייפינג לאותו דף אנחנו נבדוק אם זה מותר גם בשביל להיות עוגנים וגם בשביל שלא יחסמו אותנו ואלגוריתם שונים בגלישה של הגרף, טיפול בספאם טיפול בכפירויות כל האתגרים האלה נוספים לנו ל Web Search עד כאן כאילו רציתי לעשות משהו לא כבד מדי אחר