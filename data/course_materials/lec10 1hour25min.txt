טוב, מה שאנחנו נעשה עכשיו זה דבר כזה, אנחנו מתחילים. קודם כל נעשה חזרונת על חלק ב' של שיעור שעבר, בגלל הבוחן שהיה אז גם אלה שהצליחו לשרוד ולהישאר היה קשה להתרכז קצת, אז אני אעשה חזרה קצרה. נפרסם במהלך השבוע, היו פשוט עוד כמה נבחנים באיחור.

מקווה שביום יומיים אקבור ביום מישהו כזה. אוקיי, אז מה שעשינו בשבוע שעבר זה בעצם הרחבה של הרזור מידע עבור הווב, אוקיי? אז יש לנו עדיין את אותו נגיד מנוע בסיסי שיש לנו בארגון או במחשב הפרטי, הדגשים פה טיפה שונים בגלל שההנחה היא שבווב לא חוסר מידע, ובעצם אנחנו צריכים להגיע למידע המדויק, כלומר יש לנו דגש של Precision, ומה דגש של Recall? להשיג בך את המידע בארגון או במחשב הפרטי, אז זה בעצם מהבדלים, אחד מההבדלים המרכזיים, כאשר שוב עוד דגש אחר זה טביעה של מידע ב-Web Search, דלילות של מידע בקלאסיקה AR הרגיל. אוקיי, אז יש לנו עוד כל מיני מדדים אחרים למדוד, לא נעבור עכשיו על כל מיני דברים כמו Precision עד K, ו-Mean Average Precision וכל מיני דברים כאלו שפה דיברנו עליהם בעבר, דיברנו קצת על איך לנתח כתובת URL, אז זה שהתחלנו בעצם את הכתובת הבסיסית מ-IP Address, שמורכב בעצם מסדרה של 4 מספרים, 0 עד 255, בעצם זה הכתובת הבסיסית, אבל בגלל שלנו לא נוח לעבוד עם כתובות כאלו, אז יש לנו Domain Names, ויש לנו רכיב בראוטרים נקרא לזה לצורך העניין, ברשת של האינטרנט כל מיני נתבים, שהתפקיד שלהם זה להעביר פאקטינג ממקום למקום, אז הם אחראים בעצם, צריך להפעיל את ה-Domain Name Service, את ה-DNS, שאחראי למפות בין ה-Domain Name ל-IP, דיברנו על Top Level Domains, והיום שיש לנו בעצם GTLDs, על דומיינים, סאב דומיינים, כמו למשל, בלוג.site.com הוא סאב דומיין של site.com, וככל שהוא, יש לו תחילית ארוכה יותר, זה יותר ספציפי, יש לנו את הפרוטוקולים, שעיקר הפרוטוקולים שלנו זה פרוטוקולי HTTP, או HTTP Secured, זה ההבדל בין HTTP ל-HTTPS, לכאורה שהוא מוצפן, את ה-Domain Name שמורכב מסאב דומיינס, כבר דיברנו עליו, לפעמים יש לנו גם פורטים, ואז מתחת לזה יכול להיות לנו שכל הדבר הזה מנופל לאיזשהו סרבר, שמאזין לאיזושהי פתיקייה ספציפית, ואם מחליטים לחשוף תוכן של סאב דירקטוריז, אז בעצם אנחנו יכולים הרבה פעמים לראות פה איזשהו סאב דירקטורי, או אפילו היררכי, ולפעמים זה גם מכיל קווריז, אז אנחנו נראה פה זוגות של פרמטר שווה ל-value בקווריז, והתפקיד של השרת זה בעצם לפרסס את זה ולחלק את זה לפרמטרים ולתת ריספונס מתאים לקוורי, ולפעמים אנחנו גם רואים בתוך ה-URL מקומים עם סולמית ככה בתוך הדף ספציפית, הנה ככה עוד דוגמה, יש לנו פה maps.google.com, אוקיי, אז יש לנו פה את הדומיין עם ה-TLD בסוף, הנקודה com זה ה-TLD, ה-search זה ה-path, אותו כל כמובן זה HTTPS, ה-center בסוף זה ה-zone בתוך הדף, maps זה subdomain, וכאן ה-query שאנחנו הרבה פעמים רואים זה q, זה בעצם אחת ממילות המפתח שחיפשנו זה פארס, אוקיי אז q שווה פארס, הרבה פעמים זה משורשר, יש לנו עוד פרמטרים על שפה וכולי וכולי.

אז מה אנחנו רוצים בעצם לחלץ מהאינטרנט? אז יש לנו ברמה עקרונית מידעים שונים, בין מידעים שעובדים דרך איזה Web API והם יותר structured, אנחנו רואים כל מיני JSONים, אולי נדבר על זה בהמשך, שחוזרים, ורוב המידע הוא מידע של HTMLים, אבל כמו שאמרנו גם המידע ה-HTML היא מורכב מכל מיני רכיבים, ובעצם כל HTML בסיסי מכיל עוד רכיבים שונים שכבר מזכיר אותם, אז יש לנו מידע שהוא נחשב מובני למחצה, semi-structured, יש לנו יחד עם אותו HTML, כמו שאמרנו יש לנו רכיבי עיצוב, שמאוד עוזרים לנו, הראינו דוגמה עם צומץ ספרים למשל, מאוד עוזרים לנו למצוא את האינפורמציה שמעניינת אותנו, ה-CSSים, כי יש להם כל מיני קלאסים של עיצוב, אז עוזר לנו למצוא את המידע שמעניין, שבדרך כלל מחולק לקלאסים שונים באזורים שונים. יש לנו Javascripts, צריך לדעת גם להתחשב בהם, וכל המידע הזה, כל הדברים האלה הם ב-client-side, אבל יכול להיות גם סקריפטים שפונים ל-server-side, ובעצם דיברנו גם על מידע סטאטי ומידע דינמי. אוקיי, כך נראה דף HTML, יש לנו מבנה שהוא מאוד דומה למבנה של XML מי שמכיר, תגיעות פותחות ותגיעות סוגרות, זה יהיה אותו שם של תגית עם סלש בסוף, זה יהיה תגית הסוגרת, ואחרי שם התגית בהתחלה, נכנע זה לצורך העניין מסוגריים משולשים, למרות שזה סימן כזה של קטן וגדול, אז אחרי הסוגריים הפותחים ושם התג, יכולים לפעמים לבוא כל מיני פרמטרים, אחד מהפרמטרים שמעניין אותנו ביותר, זה הפרמטר של לינקים לדפים אחרים, אז יש לנו את תגית A, עם לינקים לדפים אחרים בעזרת ה-attribute של href, hyper-reference לדף אחר, ובעצם זה מה שמייחד אותנו באינטרנט, זה הרכיבים שמעניינים אותנו כדי לעשות קרולינג, אותם רפרנסים, אותם לינקים לדפים אחרים, אז יש לנו דף אחד, אנחנו מורידים ממנו את המידע שמעניין אותנו, ואנחנו פונים לדפים אחרים, עוד לא נרחיב עכשיו על כל מיני תגיות נוספות, ב-html, ובסוף אנחנו כמובן גם רוצים ליצור איזשהו inverted index, המידע כמובן עצמו לא מחויב המציאות שנשמע אותו במחשב, מי שמכיר קצת, מי שיצא לו לעשות קצת, בניות של אפליקציות, של web search, אז לפעמים אנחנו לא סומכים על זה שהמידע לא ישתנה, ולכן לפעמים כן מורידים אפילו את כל ה-html, כדי שיהיה לנו לאיקס זמן שאנחנו רוצים את אותו מידע שהוא מעניין אותנו, אז לפעמים כן מורידים אותם בתור קובץ html, גם פה יש את כל המנגנונים של הפרי-פרוססינג שאנחנו עושים במערכת IR-קלאסית, אבל יש לנו עוד רכיבים נוספים, והרכיבים הנוספים כמו שאמרנו זה בעצם מציאה של כל מיני סידים כאלה באינטרנט, מקומות שבהם אנחנו רוצים להתחיל לחפש, ולאט לאט קרולינג ומציאה של עוד ועוד מידעים, עוד ועוד דפים מעניינים שמפנים אותנו בעצמם בתורם לעוד דפים, ושמירה של האינפורמציה כדי שנוכל לאנדקס אותה, אז הרכיבים האלה של הקרולינג ושל הסקרייפינג הם כמובן במערכת IR רגילה, שאם נרצה יש לנו יותר פרוססינג ויש לנו עוד שלב של דיסקאברי, שכולל גם לפחות הסיד הנכונים גם למצוא את הדפים, אז יש לנו פה צ'אלנג' גדול, דיברנו טיפה גם על התיצוגה של הדברים האלו בגרף, הנה יש לנו פה בעצם אם נרצה לופ שחוזר וחוזר, הוספנו פה עוד עניין של הרשאה לאנדקס את המידע עם רובוטs txt, אגב אם אתם עושים הרבה פעמים עבודות אקודמיות, אז יש לנו איזושהי הבחנה בין מידע שהוא באמת נגיד מפחדים שתגנבו אותו בצורך מסחרי, לבין מידע שהוא רק לצרכים אקדמיים, מי שיצא לו במקרה לעשות את המטלה שכולל את הרובוטs txt, אז אולי ראיתם כל מיני אג'נטים, רואים שם לפעמים באחד מהדוגמאות נגיד אג'נטים של אופן AI נגיד, או של ג'מיני וכאלו שהם רוצים שהם יחסמו, כי הפחד היום הוא בעיקר מהם, אז ממש רואים את השמות שלהם של האג'נטים, יותר פוחדים מאשר מאיתנו, ויש לנו רצון לעשות פרוססינג נוסף של למשל להוריד כפילויות, שזה נגיד אם הרגענו את המידע שהיה לנו בצורה טובה במחשב הפרטי או בארגון, אז אנחנו לא צופים לכפילויות, כאן באופן מובנה אנחנו צופים לכפילויות, אם בגלל שהתהליך נפל והרמנו אותו עוד פעם ואז יש לנו כפילות, ואם בגלל שהגענו לאותם דפים מכמה מקורות שונים.

אוקיי, אז הרעיון הזה של די-דופליקיישן הוא חלק מובנה גם מהתהליך, וגם הרנקינג כמו שהזכרנו הוא גם תהליך שונה, יש פה קצת הסבר על מוטיבציות נוספות של התקפות של הרובוטס TXT, מה הקשר בין זה לבין די-דוס ודוס, התקפים שבעצם, התקפות שבעצם תוקעות את האתר, אוקיי, אז יש לנו פה, אני מחפש את הרנקינג, יש לנו פה כמה המלצות לגבי הסידים, איך כדאי לנו לגשת למקומות התחלתיים טובים, ואת האסטרטגיות של החיפוש שלנו, כבעצם אלגוריתמים שונים של חיפושים בגרפים, שאני מניח שלמדתם בקורסי אלגוריתמים, אם זה BFS, ובהמשך דיברנו על דברים כמו Dijkstra, בגלל המשקולות שבעצם יש לנו שקשורים ללינקים, אוקיי, אז המלצות נוספות זה די-דופליקיישן, גם בקונטנט שהוא לא זה ב-100%, אבל הוא זהה מבחינת תוכן מעל רמה מסוימת, ובסוף דיברנו על שניים או שלושה אלגוריתמים, שמדברים על החשיבות שיש לנו לדפים, כמו PageRank, שזה אלגוריתם שהכניס את גוגל למפה בזמנו כמנוח חיפוש מוביל, שבעצם גם PageRank וגם האלגוריתם של Heats, שנוזכר פה בהמשך, נותן איזושהי עדיפות גבוהה לדפים, שיש להם אחוז גבוה של הפניות מאשר לדפים פחות פופולריים, בהנחה שהפופולריות היא איזושהי גם מדעת לאיכות, האלגוריתם של Heats ספסיפי מדבר לא סתם על הרבה הפניות, אלא מחלק את ה-Websites ל-Websites טובים מבחינת Authorities ו-Hubs, כאשר Hubs הוא בעצם ליסט מהימן ואיכותי של דפים איכותיים, Authorities זה הדפים האיכותיים בעצמם, וחלק מזה שהם הופכים אותם ל-Authorities, שהרבה Hubs מפנים עליהם, כלומר ההגדרה של Authorities ו-Hubs היא קצת מעגלית, זה הרעיון של האלגוריתם של Heats, ויש לנו את העדכניות של המידע, וניסיון נוסף שצריך להתמודד איתו כל הזמן בבניית מנוע חיפוש באינטרנט, זה התמודדות עם ספאם, אוקיי? אז דיברנו, אם נסכם עד כה, דיברנו על ההתמודדות בכלל עם ה-URL, להבין את הרכיבים השונים, לקלף את מה שלא צריך, לשמור את מה שצריך, כן, אם יש לנו כתובת גדולה מאוד, שהיא מי שיצא לו, נגיד, להזין לאיזה וידאו ביוטיוב, יודע שלפעמים אפילו התחלה אחרי שלוש שניות, נגיד, זה גם יכול להיות פרמטר של פרמטר ו-Value, אז אנחנו רוצים לקלף את מה שלא רלוונטי, להשאיר את מה שלא רלוונטי, להבין איפה אנחנו, את מה אנחנו מאנדקסים, אנחנו רוצים לעשות Discovery שכולל את ה-crawling ואת הסקרייפינג, דיברנו גם על דפים סטטיים, כמה כלים בפייטון גם איך להשתמש בהם, וגם על כלים דינמיים שאנחנו זקוקים, תוכן דינמי שאנחנו זקוקים לכלים כמו סילניום בשביל לטפל בהם. אוקיי, אז אנחנו רוצים גם להילחם במפרסמים, בוא נגיד בעיקר, שמציפים את האינטרנט במידע שהוא לא באמת רלוונטי עבורנו, נגיד שהם רוצים שמישהו שחיפש משהו יגיע לדף, שרוצים לפרסם, יש תוכן מוכבה למשל, אחת מהטכניקות, שלא רואים אותו בכלל, אבל הוא נמצא בטקסט ופונט שלא רואים, והתפקיד של מי שעושה, למשל, בוני מנוע חיפוש באינטרנט, זה לגלות את אותו ספאם ולהתעלם ולא לאנדקס אותו. להחשיב את המידע כמידע לא איכותי, אז זה בערך הדברים שדיברנו עליהם בשבוע שעבר.

בינתיים יש לנו שאלות על הדברים האלו, משהו שלא היה ברור משבוע שעבר. אוקיי, אז... שנייה, אני אפתח את הצגת של השבוע. אוקיי, בסוף אנחנו נגיע לאלה להם, יש לנו כמה שלבים בדרך, ובתור שלב ראשון, אני רוצה לעשות פה כל מיני תזכורות, לדברים שאנחנו נשתמש בהם בהמשך.

אז הראשונה, אולי רלוונטית במיוחד למי שפחות בא מרקע של מידת נכונה, אז יש לנו פה כל מיני... כל אחד מאלה, אי אפשר להכין שקפים, אבל אני הכנתי רק על... לקחתי רק על חלקם. נגע... אוקיי, אז בואו נראה רגע מה יש לנו, ואם יש דברים שאנחנו לא מכירים, או שאתם צריכים לזכור אותם, אז נזכיר. אז מה שיש לנו בצד... יש לנו פה מודלים שונים ללמידה מונחת, למידה לא מונחת.

אוקיי, יש לנו למידת רגרסיה, יש לנו למידת סיווג, יש לנו פלסטרינג, וגם יש לנו הורדת מימדים. אוקיי, אז מה שיש לנו פה בצד ימין למעלה, זה בעצם... בואו נראה כאילו מה אנחנו זוכרים. אלגוריתם הסתברותי גנרטיבי.

נזכיר משהו למישהו? זה רק לפי הציור. זה חוק פייס, זה בעצם אלגוריתם נאיב בייס. אוקיי, אז הרעיון של אלגוריתם נאיב בייס, לא חושב שהקדשתי לו פה שקופית.

הרעיון של נאיב בייס, נכון צדקים? זה בעצם מכונה... נכתוב את זה כאן. זה מכונה... ראשי תיבות של מקסימום, אפוסטריו, אסטמיישן או פרובוביליטי, נגיד בדרך כלל ככה. מקסימום, אפוסטריו, פרובוביליטי.

אוקיי, יש לנו פה כמה עניינים שזה אלגוריתם מאוד מאוד פשוט, אבל יש לו באמת הרבה יתרונות. קודם כל הוא מאוד מהיר ומאוד קל להתכון. חוק בייס זה אחד מהדברים שמאוד מייצג אותו.

ההסתברות הזאת בצד... בואו נניח שה-A שלנו זה הקטגוריה, וה-B שלנו זה הדאטה, נגיד, הפיצ'ר וקטור. כלומר האובייקט. שאותו אנחנו רוצים לחזות, מהי הקטגוריה שלו.

אוקיי, אז בעצם ההסתברות מהסוג הזה נקראת אפוסטריו פרובוביליטי, בעצם ההסתברות של הקטגוריה, בינתן שיש לנו איזשהו ידע שאספנו. אוקיי, ויש לנו בצד... לפי חוק בייס, החלק הזה נקרא Likelihood Probability, זה בעצם ההסתברות של הדאטה בינתן המחלקה. כפול הקלאס פרייר פרובוביליטי, זה הסתברות שאנחנו מסתובבים בעולם ונראה אובייקט מהמחלקה שאנחנו בודקים עכשיו.

אוקיי, עושים את זה לכל קטגוריה אפשרית, נגיד, שאנחנו רוצים לזהות שפות שאנחנו לא מכירים בטיול באיזשהו מקום מסוים, ואז שואלים את עצמנו, בלי לדעת מה השפה, מה הסיכוי שעכשיו אנחנו מסתובבים בעיר מסוימת שידברו בשפה מסוימת. זה הקלאס פרייר פרובוביליטי, כלומר, בלי שיש לנו שום ידע נוסף, רק שומעים מישהו מדבר, נגיד, זה שאנחנו רואים את הבן אדם לא נותן לנו שום ידע נוסף, זה נקרא קלאס פרייר פרובוביליטי, כלומר, פשוט ההסתברות הראשונית שלא תלויה בשום דבר, ובאותו אופן, ה-B זה בעצם האובייקט הספציפי הזה עם הפיצ'רים שיש לו, מה הסיכוי שסתם נסתובב ונתקלים באובייקט ונראה שהוא בעל אותם פיצ'רים כמו אלה שאנחנו רואים עכשיו. אוקיי, אז לפי חוק בייס, הסתברות האפוסטריאורית הזאת של מחלקה מסוימת, נגיד, אנחנו הולכים בסין, ויש שם, נגיד, האפשרויות שלנו זה מנדרינית וקנטונזית, ונגיד, קוריאנית ויפנית, סתם אני אומר.

כאשר מנדרינית בזמנו היה השפה רק של העצולה, וקנטונזית זה השפה שפשוטי עם בעיקר דיברו בה, ואולי יש תיירים יפנים או קוריאנים, אז בגלל שזה במזרח אסיה, אז נניח שזה אחד מהארבע עובציות האלו. אוקיי, אז אנחנו שואלים, מה ההסתברות שעכשיו אנחנו הולכים, ודווקא מישהו ידבר קנטונזית. נגיד, כשערכנו באמת סטטיסטיקה, לא סטטיסטיקה, אנחנו ממש עשינו בדיקה הסתברותית של מה אחוז האנשים באותה עיר שביקרנו, שמדברים קנטונזית.

A בהינתן B, זה בהינתן שיש לנו אינפורמציה, מידע מסוים, נגיד, צורת הלבוש, או זה שאנחנו בכלל נמצאים בעיר מסוימת ולא בכפר, אז מה הסיכוי שאז זה יהיה קנטונזית? ואז, היתרונות של אותה הסתברות זה קודם כל גנרטיביות, אנחנו בעצם רוצים לדעת שייכות למחלקה, אוקיי, זה אלגוריתם אולי גנרטיבי הבסיסי ביותר, וגם שיש לנו פה הסתברות, שזה רוב הגוריתמים לא נותנים לנו. אוקיי, אז בואו נסתכל רגע עכשיו על הגרף הזה, מה זה מבטא? נראה לכם? מה? עץ החלטה, נכון? אוקיי, אז יש לנו פה עץ החלטה שבמקור במקור הוא גוריתם ID3 שמאוד לא אינטואיטיבי, הוא נבנה בשביל בעיקר פיצ'רים בדידים וקטגוריאלים, אוקיי, אבל שכלולים שלו בעצם אפשרו לרוץ גם על פיצ'רים רציפים, ואפילו גם לחזות ערכים של רגרסיה. אוקיי, האלגוריתם הבסיסי הזה שהתחיל מאלגוריתם שמדבר על מה שנקרא מדד לחוסר החידות או אימפיוריטי אוקיי, הוא נהיה מדד מאוד חשוב והרבה אלגוריתמים שהם state-of-the-art היום משתמשים פה בתור רכיב פנימי, מי שמתעסק בלימדת מכונה אולי שמע על XGBoost או על Gradient Boosting שמשתמשים פה פנימית.

בעצם האלגוריתם הזה של XGBoost אפשר להגיד כבר סוג של כמו בשולחן העגול, מי שמכיר שהיו הבירים, מי שזוכר פניח שלמדתם כולם הבירים, כל אחד יש לו קול אז מה שאני מתכוון זה אלגוריתם שנקרא Ensembl אנחנו מדברים על אספה כזאת של כל מיני מודלים ואיזושהי החלטה משותפת שמקבלים בעזרת החלטות הפנימיות של כולם יש לנו כל מיני סוגים של אנסמבלים אז דווקא XGBoost הוא אחד מהם אוקיי, מה זה האלגוריתם הזה אני אראה לכם בעין רגרסיה ליניארית שהיא אחד מהאלגוריתמים שמתאים רוב האלגוריתמים פה כנראה מתאימים לי יותר מדבר אחד זה מתאים רק לבעיות הרגרסיה על אחוזות מספר הציף והוא מחפש יחס ליניארי אז זה האלגוריתם הבסיסי ביותר שמלמדים עבור למידת הרגרסיה ונגיד נאיב בייס וגם האלגוריתם הזה מזהים הזה שני משמאל למעלה זה לסיווג נכון? מה אבל איך קוראים לזה? זוכרים אולי? אוקיי, אז זה רגרסיה לוגיסטית אוקיי, יש לנו פה חוץ מזה זה אני מניח שמזהים למטה, שני מימין למטה מה זה? כן נכון, רשת מירוני מלאכותית זה ינן מה זה צד שמאל למטה? svm נכון זה יכול להיות כמה דברים מה נראה לכם שזה מבטא? אז האמת שזה גם svm זה svm עם קרנל, לא יודע אם למדתם את זה או לא יש איזושהי פונקציה שנקראת פונקציית פי ברמה עקרונית, לפני הרעיון של הקרנל שבעצם svm דומה לרגרסיה לוגיסטית במובן הזה שהוא מנסה למצוא הפרדה ליניארית במרחב זה ענחה מובנית פנימית אבל אם יש לנו מידע שמתנהג בצורה לא ליניארית כמו שיש לנו כאן אז מטילים אותו לאיזשהו מישור גבוה עם איזושהי פונקציה מיוחדת של התמרה עושים איזה projection כזה ושם פותרים את זה בצורה ליניארית זה נראה אומנם לא ליניארי, זה המרחב המקורי אבל בעזרת התמרה אפשר לפתור את זה בצורה ליניארית זה הדרך שהייתה פעם נהוגה לפתור לכל אלגוריתמים שמצטטים למרחב ליניארי אז גם נגיד עבור קלאסטרינג היו עושים קרנל קלאסטרינג ואז פותרים את אותה בעיה בדרך הזאת אז מה זה כאן למטה? זה k-means k-means algorithm clustering כולם זוכרים מה זה? זה יכול להיות כל מיני דברים זה נראה לי... מה? אז זה יכול להיות random forest זה כנראה random forest שזה גם סוג של ensemble אז בגדול יש פה כמה אופיות נוספות אני לא סתם מציג אותם ה-decision trees זה הדרך להתמודד עם חיצ'רים רציפים ו-SVM שדיברנו עליו לפני ויים נדמה לי support vectors שנמצאים על הגבול זה בעצם מה שאנחנו רוצים למצוא ב-SVM להפריד ככל הניתן בין המחלקות השונות זה אמור להיות בנפרד רגע... נתקן את זה זה אמור להיות בצד הרעיון פה חשוב זה אחד מהדברים שנשתמש בזה בהמשך אז זה כתוב לנו כאן אז לא צריך לנחש שזה k-NN זוכרים את השיטה של k-NN? יכול להיות שחלקכם לא מכירים זה כבר מתקרב לדברים שלמדנו אז k-NN יש פה שני דברים שהם נגיד מעניינים גם אותנו משתמשים פה באמת רחובית בהמון המון דברים כלומר, הוא מראש מובנה גם לסיווג וגם לרגרסיה אבל משתמשים בו גם כאיזשהו רכיב בקלאסטרינג ועוד בכל מיני מקומות אחרים מי שמכיר אלגוריתם של קלאסטרינג שנקרא dbscan אז הוא משתמש ב-k-NN פנימה, אולי אתם מכירים בכל מקרה, הרעיון הוא שהוא מחפש מרחק או דמיון וקטורי בגלל זה התעסקנו הרבה בדברים האלה עד כה בגלל זה אני מזכיר אותו אולי אפילו נגיע להקשר של k-NN עוד היום אם מספיק בכל מקרה, איך הוא מטפל בבעיות רגרסיה? הוא פשוט לוקח את ה-k וקטורים הקרובים ביותר ועושה ביניהם ממוצע יש כמה ורסיות של איך לעשות את המוצע הזה בצורה משוכללת או לא וכמו שאנחנו כבר מכירים, לווקטורים שלנו יכול להיות שאנחנו נרצה להשתמש ב-cos similarity ולא ב-euclidean distance אבל זה מאוד תלוי בדומיין אז זה הרעיון, הכוכבית מייצג את הוקטור החדש רוצים לדעת איך להתייחס אליו בהתאם לווקטורים הקרובים מה הקשר בין זה לעניינים? אנחנו כבר יכולים לחשוב על שימושים שונים שדיברנו עליהם אז אם קודם דיברנו על נגיד קשר בין קווירי לבין מסמך שקרוב אליו אז אפשר לחשוב על כל מיני מניפולציות שאפשר לעשות בעזרת כן אם אנחנו נדבר על עוד שימושים דומים לשימוש הזה גם בהמשך אז כל האלגוריתמים שלפעמים כשלומדים ללמידת מכונה אומרים וואו, זה אלגוריתמים ישנים שכבר אין לנו כל כך שימוש בהם אז הם עושים רטרו עוד פעם ועוד פעם בכל מיני מקומות סתם דוגמה על עוד אלגוריתם של אנסמבל פרינציבל קומפוינט אנליסיס שגם דיברנו עליו הוא מייצג אבל בעצם אפשר להגיד שהוא מבוא גם למה שאנחנו מדברים עליו היום והרבה פעמים לוקחים אותו לא בשביל ויזואליזציות שזה שימוש חשוב אלא גם בשביל בכל זאת להוריד את המימד ולנסות משהו הכי פשוט כי הוא מחפש שהוא קשר לניארי אגב דיברנו על קרנלים יש את זה גם בפרינציבל קומפוינט אנליסיס שילוב של קרנלים אז לא נדבר כרגע על איך אוקיי, אז עכשיו ככה מבוא למה שאנחנו רוצים להתעסק איתו עכשיו אחרי החזרה הזאת על כל מיני דברים אלגוריתמים שקשורים ללמידת מכונה אז נעשה רק סיכום, יש לנו בעצם היה לנו את זה לדעתי כשלמדנו פעם הקודמת כמה מאלגוריתמים יש לנו, אפשר לעשות טבלה של 2 על 2 שכאן אנחנו נשים נגיד מידע בדיד או קטגוריאלי ונגיד שכאן נשים מידע רציף וכאן נכתוב למידה מונחת ולא מונחת אז כמובן שאנחנו מדברים על מידע בדיד קטגוריאלי בלמידה מונחת אנחנו קוראים לזה למידת מה? פתאום לראות שאנחנו שולטים בזה, זה באמת רק תזכורת איך נקרא סוג הזה של בעיות כאלו, הלמידה הזאת? מה לומדים פה? כשיש לנו מידע קטגוריאלי או בדיד אם תרצו כתגוריות סופרוויזלרמינג, קטגוריות מה? מה אתה אומר? אם יודעים, זה סיבוב סיבוב, קטגוריזציה, ספציפיקיישן, כל אלה אותו דבר אם זה רציף, נכון, אז אם זה למידה לא מונחת אז יש לנו כאן כן קלאסטרים או קלאסטר אנליסס לפעמים זה נקרא אגב, הזיהוי, אני מזכיר של סימנים לפעמים אם אתם רואים משהו שיש לו כזה רואים לו כובע לפעמים תמצאו יחד עם זה לפעמים את המילה אנליסס אז יחד עם זה יש לנו מידה רציף, שזה בעצם שמנו פה את ה-dimensionality reduction בסדר? כי כל אחד מהפיצ'רים הוא בעצם לומד גם איזשהו הורודת מימדיות למרות שהוא לומד וקטור בסוף אז אולי זה לא בדיוק אותו דבר אז זה בגדול, ולפעמים אנחנו עושים מעברים כאלה אם אתם שמים לב, GMM אם אתם זוכרים למשל GMM התחיל כאלגוריתם של cluster analysis אבל משתמשים בו גם בסיבוב ויש מעברים כאלה לפעמים מצד לצד, גם לפעמים בגלל אילוצים ולפעמים בגלל ביצועים אז כל הדבר הזה זה הקדמה שאנחנו משתמש לפחות בחלקה ואנחנו רוצים לדעת בעצם מה זה language model אז בעצם אנחנו מדברים על מודל שיפיק בעצם מילים בהסתברות מסוימת מודל להפקה של מילים בעצם אם נרצה אז נוכל אחרי זה גם לקחת את זה לinformation retrieval, ליחזור מידע ולשאול על קשר הסתברותי בין השאילתה לבין מילים שאנחנו מפיקים ולהסתכל על זה בתור language model אז הצורה הכי בסיסית היא די דומה בוא נגיד לצורה גנרלטיבית שראינו בנייב בייס בנייב בייס אנחנו מכירים ובאמת השימוש של מודלים דומים לנייב בייס אחד מהמודלים האלה גם דיברנו עליו בזמנו בהקשר של זיהוי דיבור hidden Markovian model HMM אם אתם זוכרים וב-HMM אמרנו שיש לנו הנחות מרקוביות מסדרים שונים שבעצם שואלים שאלות של בכמה מילים לצורך העניין אם זה בהקשר הזה אנחנו תלויים אחורה אם זה הנחה מרקובית מסדר 0 זה דומה קצת להנחת חוסר התלות של נאיב בייס שאמרת שאנחנו תלויים רק בעצמנו זה נחה מרקובית מסדר 0 נחה מרקובית מסדר 1 תגיד לנו לגבי מילים אני מדבר על HMM רק בהקשר של מילים תגיד לנו שוונסדיי תלוי בעצמו אבל גם תלוי במילה הקודמת ומסדר 2 כמובן תלוי בשתי מילים אחורה אז הרבה זמן לפני שהדיפ לרנינג נכנס אז גם מערכות מתקדמות של NLP עבדו עם מודלים כאלה מודלים גנרטיביים שבעצם מסתכלים על רצפים של מילים יש לנו שימושים לטקסט קטגוריזיישן לאינפורמיישן רטריבל שימושים שונים שאנחנו מכירים ואנחנו מדברים על מודלים שעושים מקסימיזציה מקסימיזציה זה אומר שאנחנו שואלים מבקשים לדעת מה הרצף של המילים המסתבר ביותר בהינתן הנחות מסוימות בדרך ראינו מודל נאיבי של bag of words דיברנו על tf-idf על cos similarity שגם משתלבים פה נגיד בפלוא הזה ויש לנו כאן את המודל ההסתברותי שאנחנו מכירים שהכניסו לחיינו כל מיני מתמטיקאים וסטטיסטיקאים כמו שנון שעושים את אותו argmax שדיברנו עליו האם אתם מזהים? זה נאיב בייס נאיב בייס לא מחלקים בדרך כלל לא מפעילים את חוק בייסד הסוף כי מעניין אותנו רק ארגמקס אומנם כאן צריך לחלק גם בp של y אבל אנחנו לא עושים את זה כי בכל מקרה יש לנו תלות בp של y הנה דוגמאות לשימושים שונים שמשתמשים במודל שפה מסוג הזה אז בעצם מה שאנחנו עשינו זה הרבה פעמים אנחנו רוצים למשל לא ראינו את זה ב-HMM אבל בואו נגיד שאנחנו רוצים לזהות איזה שהוא ביטוי שמני או נאון פרייז יש לנו נגיד ביטוי כמו resident of the או בקיצור of the United States אז בעצם יש לנו מצבים שונים של beginning of a phrase יש לנו in phrase לפעמים גם אולי נסמן את הסוף של הפריז יש לנו פה רצף של מילים שאנחנו נהפוך בעצם ליחידה אחת ובבלשנות יש לנו מתוך אוסף של מילים יש לנו את המילה שהיא נחשבת בתפקיד של head מה היא המילה המרכזית שנמצאת בתוך הhead? בתוך הביטוי שהיא המילה שכולם פונים אליה למשל אם יש לנו נגיד שר החינוך כיש לא יודע מאיזה דמות הכי רלוונטי כאן לחינוך הגבוה שלנו מה? הולכים לעצור אותו? הולכים לעצור אותו אני לא רוצה לשאול כבר מה הוא עשה כי יש לנו איך קוראים לזה posterior probability שפוליטיקאי הוא כנראה מושחת יותר ממישהו לא פוליטיקאי בהינתן שאנחנו יודעים שהוא פוליטיקאי לצערנו כן אבל שאלה היא אם היה פה איזה נתיב שאחד גרם לשני או שמראש זה מלא את הסיכויים של בן אדם כלשהו להיות פוליטיקאי לא יודע לא אמרתי כלום כמובן אז מכירים את probabilities השונים האלו בוא נגיד שצהוב אדום זה משחק בנייב בייסים צהוב אדום וכחול זה הקטגוריות שלנו או שכאן אני לא בטוח מה זה המחלקה ומה זה הקטגוריה אבל לא משנה בוא נגיד שהצבע זה המחלקה והצורה זה הfeature אז איך לקרוא מתוך הדבר הזה את ההסתברויות בעצם כשאנחנו שואלים מסתכלים על מאפיין מסוים אנחנו עושים גם סדר במרחב כאילו מיינו את זה לצבעים אם אתם שמים לב הדבר הזה שווה לדבר הזה ההסתברויות נותנות כולכם שולטים אני מאמין אחרי עבודה קשה של קורסים והסתברות אז גם מי שאין לו רקע במימת נכונה אני מאמין זה קטן על כולם אז לא נחזור על זה אוקיי אז נדלג על הקדמות האלו אוקיי אז עכשיו מה קורה כשאנחנו עוברים מאולי מלאכה מרקובית מסדר 0 לתלות אולי מוחלטת במילים הקודמות בעצם אנחנו שואלים מה ההסתברות בהינתן רצף מסוים של מילים שכבר ראינו לראות מילה חדשה אוקיי אתם מכירים את נוסחת ההסתברות השלמה מכירים את הדבר הזה יש לנו הסתברות של כאן זה מבוטא כמילים w1 עד wn אי שווה אפשר לבחור מילה כלשהי אבל הכי נוח לעשות את זה בצורה כזאת של סדר זה מה שאנחנו עושים פה המילה הראשונה כאילו לא תלויה באף מילה קודמת המילה השנייה תלויה במילה האחרונה מבחינה הסתברותית כשלמדתם את זה תבחרו משתנה אחרי כלשהו ואז תכפילו את ההסתברות שלו בהסתברות של משתנה כלשהו נוסף בהינתן המשתנה הראשון וכן הלאה אבל כאן פשוט עושים את זה לפי סדר המילים w1, w2, w3 זה סדר המילים זה כלל השרשרת שיש לנו כאן וכמובן החישובים של הדברים האלה מאוד יקרים ולכן ההנחה המקלה של הדבר הזה זה לעבור למודל של יוניגרם שאנחנו מכירים מהbug of words שאנחנו אומרים שאין לנו תלות בין המילים הנחה מקובית מסדר 0 המודל הזה הוא כמובן לא מספיק טוב בשביל לחזות את המילה הבאה אז אם אנחנו מפעילים הנחה מקובית מסדר 1 אז אנחנו יכולים לקחת מודל של בייגרם מה המודל של בייגרם אומר? כל מילה תלויה במילה הקודמת ואז המילה הראשונה לא תלויה בכלום זה היוצא דופן שלנו וכל מילה תלויה בדיוק במילה אחת הקודמת כמובן גם בעצמה ואז זה נראה ממש כמו naive Bayes אפשר גם מפה בקלות ללכת להנחה מקובית מסדר 2 ואז המודלים שעובדים טוב עם הדבר הזה דוגמת Maximum Entropy שהם ממקסימים באמת את השילובים השונים בשביל למצוא את ההסתברות הגבוהה ביותר אז כל הדבר הזה הוא השיטה עובדת כמו שאמרתי שעבדה למשך זמן ארוך ואנחנו כבר מכירים את היתרונות ואת החסרונות שלה זה מודל יוניגרם אנחנו אומרים כמה המילה מסתברת ולמדתם איזה משהו שקשור למספרים עקריים מכירים פונקציות של רנדום בטח אז אתם בטח יודעים שפונקציה של רנדום בעצם אומרת תגרי לנו זה יכול להיות ערך, זה יכול להיות אינדקס במערך אבל זה מלווה באיזשהו probability function באיזשהו התפלגות probability function או distribution ואז בעצם יש להניח שגם אם תרצו או שזה תמיד החשד שגם מכונות המזל הזה עובדות עם איזה distribution function לא אחיד אז לצורך העניין זה מה שאנחנו רואים כאן שמקנה לנו מודל bag of words יוניגרם, מה הסיכוי שנראה מילה מסוימת או הבוק אז דיברנו כבר על tokenization איך פורסים את זה, אנחנו נדבר על הדברים האלה עוד כשאנחנו קצת נסתבך אז מה לגבי language model שמכיל יותר מילה אחת אז אנחנו צריכים לפעול לפי ההסתברויות המותנות יש לנו פה מודלים של יוניגרם, בייגרם וטרייגרם אוקיי אני למה יש לי פה חזרה אוקיי אז זה ממש למי שמכיר דומה לנייב בייס אנחנו עושים maximum post-reprobability זה המודל הבייסיאני אבל אם אנחנו עובדים עם n-גרמים אז אנחנו תולים את זה לא רק במילה האחרונה זה אותו רעיון, אנחנו עושים את זה עם הנחה של כמה מילים אחורה וההסתברות נורוות בהנחה שלנו כמובן מה-training set שלנו מההסתברות של המילים שמופיעים ומפה אנחנו יכולים להתמודד עם כל מיני בעיות שיש לנו במילים למשל בעיה אחת שיש לנו זה מה קורה כשיש לנו רצף מסוים של מילים שלא נראה אותם ביחד אז איך אנחנו פותרים בעיה של שכיחות אפס במקרה הזה אז יש כל מיני טכניקות איך להתגבר על הדבר הזה שעוללות ביניהם כל מיני טכניקות של החלקה שמטפלות בבעיה של השכיחות אפס של הסתברויות מותנות איך אנחנו מטופלים בזה למי שלמד נאיב בייס אז אני מניח שזה מוכר אז אחת מהברכים הטריוויאליות ביותר להתעסק בזה זה הנחה שאנחנו שוטלים שכיחות פקטיבית או הופעה פקטיבית משותפת של מילים כמו שעושים הופעה משותפת בין מחלקה לבין feature בנאיב בייס אז כאן הופעה משותפת של מילים שבאיזושהי הסתברות הגיונית ואז ההסתברות אולי תהיה נמוכה ולא תהיה הסתברות אפס מה שיאפשר לנו בעצם לתמוך ככה במודל n-grams נדלג על הדבר הזה על smoothing זה ככה בנוסחה איך אנחנו מטפלים בsmoothing אז אנחנו אומרים בצורה הזאת כמה מילים יש לנו באותו מסמך נגיד שהיחידות הבסיסיות שלנו זה מילים כמה מילים יש לנו במסמך וכאן האמת שזה החלקה אפילו מודל unigram אז נגיד שהמילה לא מופיעה במסמך אז אנחנו נוסיף איזושהי הסתברות שכטיבית זה נקרא לפלאנס סמוזינג בשביל שיהיה סיכוי שהמסמך הזה יכיל איזושהי מילה למה אנחנו עושים את כל הדבר הזה? כי בסוף אנחנו רוצים הסתברות שלו אפס לשאילתה מסוימת כל הדבר הזה אנחנו משתמשים בו ב-IR בצורה הזאת שעלו מילה w1, w2 וw3 אבל נגיד שw2 לא מופיע בכלל במסמך אז אנחנו לא יכולים פשוט להגיד שכל המסמכים ברמה שווה ולמרות שאף אחד מהם לא קיבל התאמה בגלל שההסתברות פה היא הסתברות 0 ואנחנו מביאים כפל של הסתברויות בגלל איזושהי הנחת חוסר תלות במודל יוניגרם אז אנחנו נביא גם פה הסתברות של 0 בגלל שיתכן שמסמך מספר 5 נגיד יש לו את ההסתברות הגבוהה יותר כי הוא כן מכיל את w1 וw3 אז הבעיה של ההחלקה באה לפתור בעיות מהסוג הזה אז אנחנו שוטלים מסתברות פקטיבית של אותה מילה שכן מופיעה בקורפוס אבל לא מופיעה במסמך ואז תיתן איזשהו סיכוי גם למסמך הזה להיות מועמד שיחזור לשאילתה ספציפית ששאל אוקיי? אז יש לנו פה השוואה בין לפני ואחרי פעולות של החלקה אוקיי, אז ככה נראה מודל n-grams אוקיי, אז ההחלקה כמו שאמרנו היא לא רק להופעה במסמך אלא להופעה של איזשהו מילה n-ית יחד עם n-1 מילים אחרות ואם עכשיו המילה הזאת לא מופיעה אז אנחנו משתמשים באותה פתרון של החלקה אוקיי, אנחנו שוטלים מילים פקטיביות שלא הופיעו בשביל להעלות קצת את ההסתברות של אותו n-gram של מילים ואנחנו בעצם אומרים שיש לנו עוד במקרה הזה Delta מילים שלא הופיעו ולא יוצגו במסמך והמסמך שלנו או המסמכים שיש לנו המייצגים אומנם אבל הם לא מייצגים בצורה נגיד אופטימלית את הכל הם מייצגים רק בצורה טובה אז נניח שהלנגוויג' מודל שלנו ניתן לו איזשהו מקדם Delta ולכל שאר המילים הפקטיביות ניתן מקדם של 1 מינוס Delta ואז אנחנו מחברים ככה ויוצרים מודל מאוחד שיודע להתמודד גם במילים שלא מופיעים ברצף n-gram שלנו זה הסמוזד מודל עבור n-1 מילים אבל שהוספנו לו את המילה הנית בצורה פקטיבית אז לא נעבור עכשיו על כל מיני שיטות של סמוזינג ונגיד שלמודלי שפה זה מדד שמשתמשים בו גם ב-Large Language Models אני מניח שחלקכם מכיר את השם הזה גם בגלל שזה הפך לשם של LLM Complexity אבל השם התחיל כמדד להצלחה של מודל אז אנחנו בעצם מדגימים את זה כאן למודל n-gram אבל זה נכון עבור מודלים שונים אנחנו בעצם שואלים מה ההסתברות או מה הציון פרפלקסטי כשאנחנו נראה את הרצף הזה של המילים אנחנו בהמשך נשאל מה הסיכוי שנראה מילה מסוימת בהינתן רצף של מילים אם אתם מכירים המטרה הראשונית של LLM אם היה פשוט לחזות את ה-next word אז הדבר הזה נותן לנו כאן את ההסתברות לראות את המילה בהינתן מילים הפרסידינג שלה וסך הכל אנחנו עובדים על כל אי אפשרי מכפילים את הדבר הזה אחד חלקי הדבר הזה ניתן לנו את הציון פרפלקסטי ואנחנו כמובן רוצים למקסם את הדבר הזה אוקיי אז מן הסתם שיהיה לנו פרפלקסטי יותר גבוה אז יש כאלה שעושים את זה בתור פונקציית קוסט בצורה ישירה אבל המודלים של פעם לא עשו את זה בסורה הזאת אני לא חושב שאנחנו נכנס לסקופ של למקסם פונקציית פרפלקסטית אבל יש כאלה שעושים את זה בצורה ישירה אוקיי את המילים וההסתברות שלה זה לא מודל אחוז גם מודל הסתברותי נחשב מודל לא, בסדר, אבל אני לא יכול לשקר את הפרפלקסטי שלה זה אין פעם נמדוד אותה זה לא הסתברות של הפורפוס שלה זה לא מודל, זה פרפלקסטי מה עושה את זה? בסוף אתה משתמש בזה בשביל לענות על שייכות של מסמכים אבל לא בצורה של וקטור ספייס אלא בצורה של לאנגווידג' מודל אז זה הרמה שאנחנו לא נכנס כרגע מעבר לדבר הזה כל הדבר הזה אנחנו מביאים בעיקר כהבנה של ההיסטוריה לאן הגענו עם הלאנגווידג' מודל כרגע מבחינת מערכת העייר הקלאסית אני לא חושב שאנחנו נעשה חישובים עם הדבר הזה אבל כן להבין בכלליות מודל חלופי למודל של הווקטור ספייס ואיך הסתכלנו על חיזוי של מילים ועל לענות למסמכים רלוונטיים במערכת רק מבוססת הסתברותית לעומת המודלים היותר חדשים שיש לנו עכשיו והיום נגיע אחרי חזרה על מה שעשינו עם הלאנגווידג' מודל והוורד אמבדינגס קודם בסוף בסוף לקונטקציול לאנגווידג' מודל אז זה יהיה מודל של יוניגרם למשל התצורה הזאת הנחה של חוסר תלות וזה מייצג לנו אינגרם שתלוי כמובן בכמה אינגרמים שונים הלכנו וכמובן שאנחנו יכולים לעשות איזשהו טרייד אוף כחלק מהשאלה ששאלת זה לשחק עם פרי פרוססינג לשחק עם ה-n ב-n גרמז של כמה אנחנו רוצים בשביל לקבל ציונים יותר גבוהים ולאזן את כל הדברים האלו ביחד ולראות מה נותן לנו תוצאות גבוהות יותר בסדר? אז פעם היה משחק פחות מתוחכם והרבה יותר פיצ'ר אינג'נירינג מאשר משהו שנראה כמו למודל למידת מכונה זה פשוט היה פחות אוקיי אז אנחנו נוותר פה על כל הסמוזינג למיניהם אוקיי, אז שוב זה קשר שלנו למודלי שפה אגב, כשמם כן הם חוץ מכמובן שימוש איך השתמשנו בהם ל-IR יכול להיות שדווקא יותר נוח לנו להגיד מה הקשר בינם לבין מודלי תרגום אגב, Machine Translation משתמשים ב-Language Model אבל גם בעצם אנחנו נשאל מה ההסתברות זאת אומרת, חשבתי שאני כותב מה ההסתברות של דוקיומנט בעינתן קווירי אוקיי, זה גם יהיה סוג של אם אני רוצה לתרגם את השאית על המסמך אוקיי, אז אנחנו בעצם ככה אנחנו ניגשים לבעיה של IR בדיוק כמו שאנחנו אומרים לתרגם משפט מאנגלית לצרפתית נגיד אז זה אותו סוג של Translation Model זה לא רק סטטיסטיקות באותה שפה אלא טרנספורמציה כשיש לנו זוגות במקרה הזה ואפשר לשפר אותה גם מלימוד מהיסטוריה על קוויריס אז כאן למשל יש לנו את ההסתברות לראות סופר מסוים בעינתן סטרינג מסוים אוקיי, זה לאו דווקא אפילו מסמך הנה המסמכים שלנו שדיברנו עליהם קודם ההסתברות לקווירי בעינתן דוקיומנט מסוים אוקיי, אז כל הדברים האלה אנחנו משתמשים בעזרת Language Models רציתי להראות לכם פה עוד דבר אחד אוקיי, יש לנו את Cross Entropy שגם משתמשים בו ב-Language Models כולם מכירים, שאני אחזור קצת על אנטרופיה אנטרופיה מותנית, Cross Entropy אני אדלג על זה, לא נראה לי שלכולם יש את הרקע וניצר פה סתם פער מיותר אז אני אחזור את חלק מהשקופיות האלו עכשיו, אחרי שראינו את הגלגולים הקודמים של Language Models ודיברנו על שימוש שלהם ל-IR קצת אוקיי, קצת על ההבדלים בעצם מה שהיינו רק להשתכלל זה לדבר על N-Gram אוקיי, ואנחנו קצת מכירים ממה שלמדנו בעבר על פרוססינג שונה ועל זה שאפשר לעשות N-Gram גם ברמה של מילים גם ברמה של רצפי אותיות וכל מיני מניפולציות שדיברנו בהרה אז בעצם, אם אנחנו עכשיו רוצים להציג Feature Vectors לעשות וקטוריזציה של המילים להפוך אותם לווקטורים אז לבוא ולדבר על כמו שבוודאי חלקכם למדתם על איך להפוך משתנה קטגוריאלי שאין בסדר לערכים נומריים אוקיי, לעשות כזאת קונספורמציה אז אם יש לנו מילים אולי אפילו נתחיל עם נגיד שיש לנו צבעים כמו Blue וBlack וRed אז זה לא יהיה נכון פשוט לתת להם מספרים לפי הסדר נגיד שראינו אותם או לוגיקות כאילו ואחרות כי הדבר הזה בסוף יפגע בתהליך של הלמידה אוקיי, אז מה שאנחנו עושים בתור צעד ראשון להתמודד עם הדבר הזה זה מה שנקרא One-Hot Encoding מה One-Hot Encoding נותן לנו? אז נגיד שיש לנו אם אנחנו לוקחים למשל חושבים טיפה קדימה על השיעור נחשוב על GPT נגיד במקרה הזה שיש לנו פה ווקאבלרי של ארבעה מילים אוקיי, אז מה שאנחנו נעשה זה ניתן אחד במיקום של אותה מילה ואפס בכל שאר המיקומים אוקיי, אז זה דרך שכן תייצג את המילים ותעזור למודל לא להתבלבל אבל כמו שכבר דיברנו על הבעיות שהדבר הזה יוצר זה נגיד שתי בעיות מרכזיות קודם כל יש לנו מרחב מאוד ספרסי כן, מאוד דליל ובעיה השנייה זה שיש לנו בעיה שאנחנו לא יודעים בעצם לתפוס את ההבנה של המילה את המשמעות הסמנטית של המילה ולא יודעים להגיד ששתי מילים שאין להם אותו ייצוג בשפה הם דומות אחת לשנייה נתנו לזה דוגמאות שונות אוקיי, אז יש לנו כאן מילים שהיינו רוצים מילים כמו kitchen וtable ו... sink כן, כל המילים האלה יש להם קונטקסט דומה, משמעות דומה אבל one hot encoding לא תופס אותה זה מה שהיינו רוצים לתפוס והפתרון שעליו דיברנו בשלב הבא דיברנו על זה גם עם שיטות כמו LSA או אפילו PCA וגם עם שיטות כמו root-to-vec או GloVe ב-root-to-vec דיברנו גם על SIBO continuous bag of words וגם על skip-grams אוקיי, זה שיטות בעצם שיודעות לתפוס מילים שהן דומות איך אנחנו משתמשים בהם? לצורך העניין אנחנו משתמשים בהם בצורה הזאת נגיד שיש לנו קונטקסט של שלוש מילים כן, יש לנו ווקבלר של שלוש מילים ו... עכשיו... יש לנו וקטורים של שלוש מילים שמייצגים אותן בצורה הזאת עכשיו אנחנו רוצים שיטה וקטורית ליצג את המילה אז מה ה-Word Embedding נותן לנו כאן? אנחנו רוצים להקפיל את ה-Word Embedding באיזה שהוא One Hot Encoder כלומר שיהיה אחד במקום המתאים של המילה ואז אנחנו מקבלים באוטפוט וקטור שמייצג את המילה זה מה שאנחנו רואים פה אם עכשיו ניתן One Hot Encoding של מילה אחרת אנחנו נקבל וקטור אחר מתאים איך עושים את זה? פשוט אנחנו מכפילים את ה-Vector של ה-One Hot Encoder ב-Word Embedding ומקבלים ככה את הייצוג ה-Vectors אוקיי? אז כמובן שהדבר הזה עובד לנו בסדר ב-Vectors של מילים דומה לנגיד Word2Vec ודומיהם זו השיטה שבה אנחנו מייצגים עכשיו מילים ועכשיו יש קצת בעיה בשיטה הזאת כי מה שאנחנו רוצים זה כמה דברים שאנחנו לא יודעים לטפל בהם קודם כל, בעיה אחת שיש לנו כאן ה-Marmor שלאותה מילה, נגיד מילה כמו Like או מילה כמו Cuts היא תיוצג בדרך כלשהי, אבל אם יהיה לה קונטקסט אחר היא לא תשתנה בכלל כלומר בעצם ה-Word Embedding שאנחנו רואים פה הם סטטיים אוקיי? אז איך אנחנו פותרים את הבעיה הזאת? לזה אנחנו נצטרך בעצם פתרון אחר ממה שהיה לנו עד כה בפתרונות כמו Word2Vec מה כן היינו רוצים לשמר מהפתרון הזה אוקיי? יש לנו ניסיון לדמיין בעצם מה הדברים האלה נותנים לנו עם שכבות שונות של דאטה כמו זה שלמשל הם יכולים לדבר על טנס של מילה או על מילים שהם פוגרסיב, כל טוב שם? לעומת מילים שהם Present Simple כאן אוקיי? אז יש לנו פה דמיון ממש בייצוג של המילה ואנחנו יכולים אפילו לעשות להוריד מממד מסוים ולקבל נגיד ג'נדר אחר אם אנחנו נוסיף חלק מהפיצ'רים או לקבל טנס אחר או לקבל פורמט אחר של פועל אוקיי? כל הדברים האלה היינו רוצים לקבל בעזרת איזה שהוא Word Embedding אוקיי? אז איך נעשה את הדבר הזה? אז אנחנו ראינו פתרונות פשוטים שיכולים לייצג תמונות בתמונות הדבר הזה הוא די קל אם אנחנו מדברים למשל על תמונות של גווני אפור אוקיי? דיברנו גם על תמונות של צבע אבל לצורך העין בגווני אפור יש לנו איזה שהוא ערך בין 0 ל-1 של אותו גווני אפור הדבר הזה מציג לנו מטריצה ואז מה שאנחנו עושים זה משרשרים שורה אחרי שורה ויוצרים Feature Vector ועכשיו כשיש לנו Feature Vector אנחנו יכולים לתת את זה לאיזשהו Neural Network והוא יכול לתת לנו איזשהו סיבוב של הספרה שאולי אפילו נכתבה בכתב יד לאחת מהספרות המדוברות אז זה משהו שאנחנו יודעים להתמודד איתו ה-One Hot Encoder כמובן זה ייצוג לא טוב מה שאנחנו היינו רוצים זה להפוך את ה-One Hot Encoder לאיזשהו ייצוג של המילה איך אנחנו עושים את הדבר הזה? אז נגיד שיש לנו ווקאבלרי בגודל מסוים זה באמת בנפנופי ידיים מאוד בגדול להסביר על מה World Embedding עושה אז האינפוט שלנו זה וקטורים של One Hot בגודל של הווקאבלרי אז זה מה שיש לנו באינפוט ומה שאנחנו היינו רוצים זה ללמוד את המילה בצורה מסוימת שאנחנו קוראים לה בצורה Latent או Dense והיא נתן שיש לנו את הצורה הזאת לדעת להגיד באיזה מילה מדובר אז בעצם ה-Output Layer זה חוזר להיות בממדיות של הווקאבלרי המקורית ואומרת לנו באיזה מילה מדוברת כדי שנוכל לעשות את הדבר הזה אם אנחנו מחריכים את המודל לעבור פה בממד מאוד נמוך ביחס לגודל הווקאבלרי אז בעצם מחריכים אותו ללמוד את הדברים החשובים בוא נגיד שהאינפורמציה החשובה שיש לנו מהמילים ועל ידי כך בעצם לייצג את המילים בעזרת אותו World Vector שאנחנו קוראים להם World Embeddings אז זה בעצם בגדול מה שאנחנו עושים עם שפה זה היה ה-One Hot Encoder שלנו שלא נתנו פתרון לא ברמת המסמך ולא ברמת המילים גם ייצוג של מילה עם מילה אחרת אז מה שאנחנו היינו רוצים בעזרת ה-World Embeddings זה כאן יש לנו נגיד World Embeddings בגודל שלוש שלכל מילה יהיה לנו איזשהו וקטור שונה שייצג אותה אז את זה אנחנו רוצים בעצם ללמוד ככה שאנחנו מוכל בסוף אחרי שבנינו את אותו World Embeddings על ידי איזשהו כפל וקטורי במטריצה לקבל בעצם את הייצוג של ה-Dense של כל אחד מהמילים ככה אנחנו נשתמש בעצם ב-World Embeddings אז אחרי שבנינו את אותו מנגנון של ה-World Embeddings אז בעצם הייתה הבנה שאנחנו רוצים את המנגנון הזה עבור כל דבר, לאו דווקא עבור מילים אנחנו רוצים אותו ודיברנו על זה קצת נדבר על זה עוד קצת בהמשך גם עבור סיגנל של Voice גם עבור תמונות וגם עבור מילה אבל לא רק ברמת המילה אנחנו רוצים עכשיו World Embeddings של פסקה שלמה אולי לא רק של מילה רוצים לייצג את כל הפסקה איך עושים את הדבר הזה? אז זה אנחנו עוד נדבר אבל בשביל כל הדברים האלו אנחנו רוצים קונטקסט של World Embeddings אז זה בעצם מה שנדבר עליו אחרי הפסקה עכשיו ככה משכתי אותכם קצת עוד 25 דקות זה מספיק לנו? או כמה אנחנו רוצים? חצי שעה? אתם תגידו התחלנו טיפה בייחוד אז שאלה אם 25 דקות יספיקו לנו נסתדר עם זה? סבבה טוב, אני רוצה לחזור על איזה נקודה שאני גם מספיק התגשתי אני אסביר לזה קצת יותר ברור רואים את ה... אוקיי אז בעצם איך נתמודד עם מודלי שפה בעזרת איך הם התמודדו עם החזור מידע אז יש לנו בעצם הישויות שיש לנו כאן זה השאלתה שמורכבת ממילים ויש לנו את המסמכים השונים d1, d2 וכו אוקיי אז אנחנו שואלים שאלה בסוג הזה איך אנחנו נוכל בעצם לדעת מה המודל מה התאמה של מסמך איך נוכל לעשות רנקינג במסמכים ולהגיד המסמך הזה יותר טוב מהמסמך השני אז אנחנו יכולים לעשות את זה משני הכיוונים או לשאול מה ההסתברות של לראות את השאלתה הזאת שהמודל של המסמך יייצר את אותה שאלתה בהינתן המודל של המסמך מה הסיכוי שנראה את השאלתה אז זה אופציה אחת אופציה שנייה זה להפוך את הכיוון ולהגיד מה הסיכוי שנראה דווקא את המסמך בהינתן השאלתה נבנה מודל השאלתה מה הסיכוי שנראה דווקא את המסמך הזה בהינתן השאלתה ויש גישה שלישית שבאה ואומרת בואו ניקח את שני המודלים גם את המודל למסמך וגם את המודל לשאלתה ונעשה איזשהו איחוד בין שניהם מה שאנחנו כותבים פה זה ההסתברות לראות את המסמך את המילים השונות במסמך בהינתן המודל של השאלתה כפול איזשהו סיכוי קטן יותר לראות את המילים בהינתן המודל של המסמך זה לא סיכוי קטן יותר פשוט לוג של ההסתברות אז זה קורס אנתרופי בעצם הדבר הזה ואנחנו לוקחים את שתי המודלים בעצם בתור מה שישלנו רנקינג של המסמכים גם את המודל של השאלתה וגם את המודל של השפה ובעצם אנחנו מגיעים פה לסוג של תרגום בין שפות רק בין המסמך לבין השאלתה במקום בין שפה X לשפה Y זה קוראים Translation Model זה ככה הדרך להשתמש באותו מודל שפה כשאנחנו מדברים על מסמך מצד אחד ועל שאלתה מהצד השני חזרה למודלים של הדנס שהשתמשנו בהם בסוף מהOne Hot Encoder לWorld Embedding הגענו לWorld2Vec למשהו כזה שזה ברמה עקרונית מאפשר לנו ללמוד כל מיני יחסים יש בזה הרבה יתרונות כאן אנחנו רואים למשל עיר בירה לעומת עיר אז אם אנחנו לומדים את הרלציה הזאת את היחס הזה אז אנחנו יכולים לצורך העניין לקחת נגיד את ספרד ולהגיד שנראה ככה כמו אתגר כזה של פסיכומטרי שאומרים בו אולי ספרד למדריד זה כמו איטליה למה אז זה כמו איטליה לרומא אז את זה אנחנו בעצם יכולים לעשות על ידי חיבור וחיסור או להגיד שאנחנו יכולים ממלך להוריד את הג'נדר של הזכר ולהוסיף את הג'נדר של המקווה ולכבוד שנקבל מלכה אז מניסויים שונים דווקא יחסים כמו דוד ודודה עבדו יותר טוב מאשר מלך ומלכה אבל זה רעיון שאנחנו יכולים על ידי המימדים השונים בניסיון אולי לדמיין שהיינו רוצים שזה יהיה ככה שהם יתפסו כל מיני יחסים מתוחכמים מבחינה סמנטית שייצגו ככה את המילים אוקיי אז במקרים מסוימים זה אולי עובד ממש בצורה הזאת ואמרנו ככה לפני ההפסקה אמרנו ככה לפני ההפסקה שמה שהיה מועיל כמו בוורט ובק עבור מילים אז היינו רוצים את זה גם בכל מיני סוגים של אינפוטים שעל הדברים האלו אנחנו כמובן נדבר על הדברים האלו בהמשך אז יש לנו פה איזה שהוא וקטור מייצג לתמונה וקטור מייצג לסיגנל של שמה אודיו וגם אם אנחנו מדברים על שפה זה לא חייב להיות מילה אחת אלא משפט או פסקה יחידה גדולה יותר של מילים אז זה בעצם השאיפה שאנחנו רוצים להגיע אליה אבל לפני שנדבר על הדבר הזה מה הבעיה שיש לנו בעצם עם המודלים כמו וורט ובק ומה הם בעייתיים אוקיי אם אתם זוכרים את המוטיבציה הראשונה שאנחנו דיברנו עליה עוד כש דיברנו על הניסיון לפתור את הדמיון הסמנטי בין מילים כמו הוטל ומוטל או אפילו בהתחלה כשדיברנו על שפה אם אתם זוכרים אז נתנו דוגמה קלאסית של בנק לעומת האם בנק בהקשר של ערוץ נחל כן או בהקשר של כסף אוקיי ובעצם בשיטות שונות אם בגלל שכיחות שזה מופעים משותפים עם מילים אחרות ואם בגלל ניסיון למקסם כל מיני פונקציות בטקסטים מסוימים אז אין לנו מצאנו שיטות שהן די טובות בשביל לקודר את המידע הזה של מימדים שונים שמייצגים את המילים אבל אין לנו אחרי הלמידה הזאת אפשרות להגיד שבהקשר מסוים אנחנו רוצים שזה יתפוס משמעות מסוימת אחת ובהקשר אחר זה יתפוס משמעות מסוימת אחרת אוקיי בעצם בשביל הדבר הזה זה נתן לי את המוטיבציה למה שנקרא contextualized world embeddings ובעצם מה שאנחנו נדבר עכשיו קצת זה על המנגנון בכלליות שאנחנו בעצם משתמשים פה בworld embeddings ועל אחד אנחנו נתייחס בעיקר לברד ואולי טיפה לGPT זה שני המודלים הראשונים המפורסמים שהשתמשו במנגנון שעד היום הוא המנגנון המוביל שנקרא transformer ללימוד contextualized world embeddings וכאן אנחנו נתייחס למנגנון שמשתמשים בו שאמור לפתור את הבעיה הזאת שנקרא self-attention אוקיי אז מה הרעיון הרעיון הוא כן ככה זה נראה השכבות השונות שאנחנו לומדים זה בעיקרון מה שנקרא multi-head attention יש לנו הרבה שכבות זה נראה מפלצת אחת גדולה שמאוד מאוד מפחידה קודם כל זה באמת מאוד מסובך אבל ננסה קצת לפשט את זה וקצת להוריד את זה לקרקע אוקיי אז כמו שאמרנו המודלים האלו של טרנספורמרים כן אז ראשי תיבות גם של GPT וגם של ברד שהתייחסתי אליהם אההה נכנס לאחור וגם ב-GPT ה-T שלנו בשניהם זה טרנספורמר אז ב-GPT זה ננסה זה יבוא ליד רגע ג'נרטיב זה ה-G בבלט זה בי-דירקשינל אנקודינג אנקודר ובשניהם דגש מסוים או על מה שנקרא אנקודר או על מה שנקרא דקודר ב-GPT יש יותר דגש של דקודר וב-BERT יש יותר דגש לאנקודר אבל בואו ננסה להבין את המנגון הפנימי של שניהם של הטרנספורמר אז בעצם אנחנו מדברים על מטריקות של מטריצות שמתייחסות לכמה היבטים עבור כל מילה שמכנים אותם ברשי תיבות של Q, K ו-V Q זה query, עוד רגע נסביר קצת על מה הרכיבים האלו בעצם עושים ו-K זה key ו-V זה value ובעצם אנחנו מתחילים מאיזשהו embedding סמנטי שכן תופס את המשמעות אבל הוא לא